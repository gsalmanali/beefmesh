{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to BeefMesh Project Direct any issues or questions to alisalm1 AT msu DOT edu The 'BeefMesh' framework brings together participants in the \"Beef Supply Chain\" in a distributed, yet secure and highly connected manner. This enables trust and transparency-oriented collaborative applications, such as managing traceability, measuring supply chain emissions, optimizing resource consumption, and leveraging shared data pipelines for secure federated machine learning applications. This repository is meant to provide some of the basic microservices required for the BeefMesh framework that can be easily reconfigured and adapated to run other functions. A basic starting point for any collaboration application is a connected group utilizing distributed resources such as databases, internet of things and connectivity channels such as the blockchain channels. To get started, download the repository material to your local machine under a folder named BeefMesh, which will serve as the root directory of the project. The framework can be run locally for testing as well as on multiple physical hosts. A single host setup is intended for testing purposes, where all containers run on one physical machine and use a bridge network. The multi-host setup is intended for use on different physical machines with unique IP addresses that are reachable on the Internet. The multi-host network uses Docker Swarm to create an overlay network. The application has been tested in a linux environment (Ubuntu 22.04). Before getting started, download, install, and configure the following tools so that they are available in the Linux PATH. Git Curl Touch Docker-compose Go (version 1.17.13) Jq Awk Openssl GlusterFS-server GlusterFS-client Python3 For local testing, the main Docker network is the beef_supply (bridge) network. For distributed testing, the main Docker network is the beef_supply network set up as an overlay using Docker Swarm. Several other bridge or overlay networks are also created depending on the scenario. To start using the framework, run and connect individual components by navigating into each micro-services application folder intended for a collaboration group . The details of the micro-services are summarized in following document sections. Before any tests or examples can be run, the network components need to be up and running, which includes the blockchain, databases, IPFS, timestamp authority, and Internet of Things. More details on using single host vs multi-host setup for each sub-application are provided as well.","title":"Home"},{"location":"#welcome-to-beefmesh-project","text":"Direct any issues or questions to alisalm1 AT msu DOT edu The 'BeefMesh' framework brings together participants in the \"Beef Supply Chain\" in a distributed, yet secure and highly connected manner. This enables trust and transparency-oriented collaborative applications, such as managing traceability, measuring supply chain emissions, optimizing resource consumption, and leveraging shared data pipelines for secure federated machine learning applications. This repository is meant to provide some of the basic microservices required for the BeefMesh framework that can be easily reconfigured and adapated to run other functions. A basic starting point for any collaboration application is a connected group utilizing distributed resources such as databases, internet of things and connectivity channels such as the blockchain channels. To get started, download the repository material to your local machine under a folder named BeefMesh, which will serve as the root directory of the project. The framework can be run locally for testing as well as on multiple physical hosts. A single host setup is intended for testing purposes, where all containers run on one physical machine and use a bridge network. The multi-host setup is intended for use on different physical machines with unique IP addresses that are reachable on the Internet. The multi-host network uses Docker Swarm to create an overlay network. The application has been tested in a linux environment (Ubuntu 22.04). Before getting started, download, install, and configure the following tools so that they are available in the Linux PATH. Git Curl Touch Docker-compose Go (version 1.17.13) Jq Awk Openssl GlusterFS-server GlusterFS-client Python3 For local testing, the main Docker network is the beef_supply (bridge) network. For distributed testing, the main Docker network is the beef_supply network set up as an overlay using Docker Swarm. Several other bridge or overlay networks are also created depending on the scenario. To start using the framework, run and connect individual components by navigating into each micro-services application folder intended for a collaboration group . The details of the micro-services are summarized in following document sections. Before any tests or examples can be run, the network components need to be up and running, which includes the blockchain, databases, IPFS, timestamp authority, and Internet of Things. More details on using single host vs multi-host setup for each sub-application are provided as well.","title":"Welcome to BeefMesh Project"},{"location":"blockchain/","text":"Blockchain Network The submodule files located under /blockchain directory are meant to start a hybrid permissioned blockchain network that can be extended to form different groups for collaboration. The blockchain framework apart from running containers, requires hyperledger fabric version 2.5 related binary files located under BeefMesh/fabric-samples/bin folder. Downloading the hyperledger fabric within the BeefMesh folder will create the fabric-samples/bin automatically. To download the fabric 2.5 binary files within the BeefMesh folder, call the blockchain-net.sh file in /blockchain folder. # Note that this only downloads binary files and container images! ./blockchain/blockchain-net.sh downloadfabric This will download required docker images and binary files in a /bin folder inside BeefMesh/blockchain directory. The rest of the tutorial is divided into two sections. One for testing locally on a single host and one for testing on a multi host setup. A Note on Environment Variables Environmnet variables are automatically picked up from configuration files and stored under blockchain/blockchainEnvVar for different organizations, peers and orderers. The format of the folders inside blockchain/blockchainEnvVar is of the form organizationX_Y, where X is the organziation number and Y is the peer number. This allows storing multiple organziations of the same type, e.g. first breeder org with one peer node can be stored as breeder0_0 and when another peer gets added, it can be stored as breeder0_1. Or or separate new breeder organizations can be added as breeder1_0, breeder2_0, ... and so on. Next, blockchain/OrgConfigurations folder allows storing network configuration files so that different set of organzations can be configured for a particualr channel using a particular orderer. Use the chaincode_path.sh, channel_path.sh and configtx_path.sh files to target different organization configurations. For example, if all the settings are set for 'main' configuration, then different files inside blockchain/OrgConfigurations/main folder can be used to spin up different set of blockchain confgurations when creating channels, installing chaincode etc. SingleHost Setup Once the binary are downloaded in the BeefMesh/blockchain/bin folder, start the minimim setup of manager and regulator for the main server. ./blockchain/blockchain-net.sh blockchainup tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh blockchainup <starting_channel_name> The initial blockchain framework uses network configuration files in blockchain/configtx folder to start up a blockchain network of manager and regulator that are connected with a channel (tracechannel) and to a docker bridge named 'beef_supply'. Here is the basic overview of the sequence of files/functions called. call the network.sh file in BeefMesh folder pick up configuration files in BeefMesh/blockchain/configtx folder pick up binary files in BeefMesh/blockchain/bin folder start certificate authority (CA) containers for manager and regulator with a single orderer using docker compose files in blockchain/compose/docker/ folder call the enrollment functions for manager, regulator and orderer in organizations/fabric-ca/registerEnroll.sh that creates certificates and stores them for each organziation under BeefMesh/organizations/peerOrganizations/ and BeefMesh/organizations/ordererOrganizations/ start the database and base containers for orderer and peer using the docker compose files in blockchain/compose/docker/ folder call the blockchain/scripts/createChannel.sh to start the initial channel which creates the genesis block (tracechannel.block) in blockchain/channel-artifacts folder along with setting ports for anchor peers. This setup can be brought down any time ./blockchain/blockchain-net.sh blockchaindown tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh blockchaindown <starting_channel_name> The blockchain network comprising of manager and regulator consits of a peer node in each organization and an orderer node. The orderer node is required to arrange the blockchain files after consensus when they are stored. Each organization can have their on multiple orderers and peers. The tutorial below describes how to add a new custom organization (rancher), a new peer (for regulator) and a new orderer (for racher) along with generic organization, peer and orderer with user configured settings. Managing a new organization Expand the network and add a new organization (rancher) to the network. Call the addRancher.sh file in the BeefMesh/addrancher/ folder ./blockchain/blockchain-net.sh addrancher tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addrancher <starting_channel_name> This adds a new organization to the already running network of blockchain and joins it to the channel tracechannel. Note an error of the type \"Error: proposal failed (err: bad proposal response 500: cannot create ledger from genesis block: ledger [<channel_name>] already exists with state [ACTIVE])\" means that volumes from previous configurations are still being reported by the system. This can be fixed by removing the organizations and unused docker volumes completely. ` To remove the organization from the network and from the channel, run ./blockchain/blockchain-net.sh removerancher tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removerancher <starting_channel_name> This removes the organziation completely from the network and from the channel while reconfiguring the network. Managing new peers Add a new peer for the regulator organization ./blockchain/blockchain-net.sh addregulatorpeer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addregulatorpeer <starting_channel_name> Remove the extra peer from regulator organization ./blockchain/blockchain-net.sh removeregulatorpeer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removeregulatorpeer <starting_channel_name> Since manager and regulator setup is assumed to be running on the same host along with an orderer node, the new organization running on some other host (multi-host scenario) might not be able to directly access the single running orderer. Start a new orderer for the rancher organization Managing new orderer Add a new orderer for the regulator and manager organization ./blockchain/blockchain-net.sh addrancherorderer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addrancherorderer <starting_channel_name> Remove the extra orderer just added above for the rancher organization ./blockchain/blockchain-net.sh removerancherorderer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removerancherorderer <starting_channel_name> Custom MultiHost Setup Note: A list of ports and addresses used by blockchain containers is maintained under blockchain/blockchain_ports_master.txt and would be helpful to maintain when running all containers locally or in multihost setup! When adding more organziations, just increase port number and make sure it is not used with any other container The scripts under this section can be run in single host and multi-host environment and are meant to be flexible enough to allow adpating quickly for new organizations, peers and orderers. Before using for multihost setup, make sure all networks are using 'overlay' option instead of 'bridge' in the docker-compose files. A difference between running scripts locally and multihost setup is summarized below. For multihost setup, configure the PeerOrgEnv.sh environment files with the ip address of the peer node instead of using localhost. Ports can remain the same. In a multihost setup, each organziation should setup their own orderer since it is not advisable to share configurations with other nodes running orderer to update changes to common channel. Initially, the orderers are added by sharing configurations in a group that other organziations can use to make udpates to channel. Once the changes are made, each organziation can continue with their own orderer to make more channels and make changes to it. Due to limitation of space, combined scripts are provided that can be split into more than one script that requires utilizing an orderer owned by other organization. This requires sharing files using collaboration server to allow required changes to server. The main changes that are required to use other organizations orderers are when a channel genesis file is required or when the organziaton is not on the channel but wants to join. Custom Organization To bring up a custom organization to an already existing network started by manager and regulator, use the addCustom.sh file in /addustomorg folder. First change the environment variables (non-conflicting org name, ports, addresses in the addcustomorg/env/env.sh file. Then export the variables in the terminal source addcustomorg/env/env.sh Note that folder name can be changed when resuing the template. Also change the folder name path variable in the env/env.sh file Call the the setparamcustomorg method to set configuration for the new organization ./blockchain/blockchain-net.sh setparamcustomorg Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustomorg tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustomorg tracechannel Custom Peer Note that extra peer uses the same ports for certificate authority for the organization to which it is added to To bring up a custom peer to the already existing network started by manager and regulator, use the addCustom.sh file in /addustompeer folder. First change the envrionemnt variables (non-conflicting org name, ports, addresses in the addustompeer/env/env.sh file. Then export the variables in the terminal source addcustompeer/env/env.sh Call the the setparamcustompeer method to set configuration for the new peer ./blockchain/blockchain-net.sh setparamcustompeer Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustompeer tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustompeer tracechannel Custom Orderer To bring up a custom orderer to the already existing network started by manager and regulator, use the addCustom.sh file in /addustomorderer folder. First change the envrionemnt variables (non-conflicting org name, ports, addresses in the addustomorderer/env/env.sh file. Then export the variables in the terminal source addcustomorderer/env/env.sh Call the the setparamcustompeer method to set configuration for the new peer ./blockchain/blockchain-net.sh setparamcustomorderer Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustomorderer tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustomorderer tracechannel Manage Blockchain Channels To create new channels for use after an organzation has been setup, use the porvided scripts as follows ./blockchain/blockchain-net.sh createchannel <channel_name> ./blockchain/blockchain-net.sh createchannel cedric Note that, before calling the script to use, make changes to environment variables under blockchain/OrgConfigurations and particularly createChannelOrgsCustom.txt file with first line being the channel configuration to be picked up from configtx file, the second line being the orderer to use, the third being the main organization which will start the process and fourth line being the list of organizations that will be part of the channel. Deploy Chaincode to Channel Once channels have been created, smart contracts need to be installed on the channels to use them. Same version of the program should be pakcaged and installed on all peers using the same channel to be able to make changes to the same data when they have the rights to do so. A sample Beef Chain specific program is provided under examples/emissionschaincode written in GO language. To install the program, use the custom scripts provided Note that while environment variables are automatically set for rest of the application, a record needs to be manually made before installing chaincode for the peer connection paramameters in the file blockchain/scripts/envVar.sh and in the function parsePeerConnectionParameters() ./blockchain/blockchain-net.sh installcontract <channel_name> <program_name> <program_location> <program_language> <collaboration_orgs> ./blockchain/blockchain-net.sh installcontract tracechannel traceexample ./examples/emissionschaincode/ go \"OR('ManagerMSP.peer','RegulatorMSP.peer')\" Not that this again makes use of target envrionment files in blockchain/OrgConfigurations and particularly installChaincodeOrgsCustom.txt file. The smartcontract can be reused by copying the program under different folder and installing with different program name. Deploy Chaincode to Channel Once channels have been created, smart contracts need to be installed on the channels to use them. Same version of the program should be pakcaged and installed on all peers using the same channel to be able to make changes to the same data when they have the rights to do so. A sample Beef Chain specific program is provided under examples/emissionschaincode written in GO language. To install the program, use the custom scripts provided ./blockchain/blockchain-net.sh installcontract <channel_name> <program_name> <program_location> <program_language> <collaboration_orgs> ./blockchain/blockchain-net.sh installcontract tracechannel traceexample ./examples/emissionschaincode/ go \"OR('ManagerMSP.peer','RegulatorMSP.peer')\" Not that this again makes use of target envrionment files in blockchain/OrgConfigurations and particularly installChaincodeOrgsCustom.txt file. The smartcontract can be reused by copying the program under different folder and installing with different program name. Writing and Reading Data from Chaincode Once program is installed, a helper function file is provided to read and write data into the channel using the installed program. To write a new animal record to the program, ./blockchain/blockchain-writer.sh createanimal <main_org> <orderer_to_use> <channel_name> <program_name> <animal_properties_file> <some_info> ./blockchain/blockchain-writer.sh createanimal manager0_0 orderer tracechannel traceexample animal1.sh NewAnimal Note that the record above uses an animal properties file located under examples/ folder and writes the animal_id that is generated by the blockchain under examples/animalrecords. To read back the data recorded on the blockchain, ./blockchain/blockchain-writer.sh readanimal <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh readanimal manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 Note that, we use the animal_id (example here) that was generated from the first stage of recording the animal on blockchain to target later blockchain changes. To read private data of animal from blockchain by the owner who created the data, ./blockchain/blockchain-writer.sh readprivate <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh readprivate manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 To make changes to the data stored on the blockchain, particularly the description of data, ./blockchain/blockchain-writer.sh changedescription <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <description> ./blockchain/blockchain-writer.sh changedescription manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 AnimalForSale To record data from traceability channels such as the IPFS or some other general data, ./blockchain/blockchain-writer.sh recorddata <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <data> <target_function> ./blockchain/blockchain-writer.sh recorddata manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 SomeIPFScid ChangeFarmerData Note that we used ChangeFarmerData target function above as it makes changes to the FarmerData variable. To make changes to other varaibles, some useful target functions are given below: ChangeFarmerData/ChangeFarmerTrace for managing data on FarmerData/FarmerTrace ChangeBreederData/ChangeBreederTrace for managing data on BreederData/BreederTrace ChangeDistributorData/ChangeBreederTrace for managing data on DistributorData/DistributorTrace ChangeRetailerData/ChangeBreederTrace for managing data on RetailerData/RetailerTrace ChangeRegulatorData/ChangeRegulatorTrace for managing data on RegulatorData/RegulatorTrace ChangeConsumerData/ChangeConsumerTrace for managing data on ConsumerData/ConsumerTrace ChangeProcessorData/ChangeProcessorTrace for managing data on ProcessorData/ProcessorTrace To move private animal record to other organization, officially start a selling process, ./blockchain/blockchain-writer.sh agreetosell <seller_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_trade_id>> <price> ./blockchain/blockchain-writer.sh agreetosell manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 109f4b3c50d7b0df729d299bc6f8e9ef9066971f 2500 Check the sale price that was just changed, ./blockchain/blockchain-writer.sh checksaleprice <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh checksaleprice manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 The other orgaznation interested in taking in the animal, puts on their bid price, ./blockchain/blockchain-writer.sh agreetobuy <buyer_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_properties> <price> ./blockchain/blockchain-writer.sh agreetobuy regulator0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 animal1.sh 2500 Note, that the other organization needs to knwow the properties data of the animal under negotiation Check the bid price that was just put on the blockchain, ./blockchain/blockchain-writer.sh checkbidprice <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh checkbidprice regulator0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 If the prices match and the seller agrees to hand over the animals, the seller can tranfer the animal to the other organization by using the other organizations credentails ./blockchain/blockchain-writer.sh transferanimal <seller_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_trade_id> <buyer_org> <price_settled> ./blockchain/blockchain-writer.sh transferanimal manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 109f4b3c50d7b0df729d299bc6f8e9ef9066971f regulator0_0 2500 Check Block Details To check the details of a particular transaction in a block, download and build the application provided here: https://github.com/Deeptiman/blockreader Optionally, use the blockchain explorer application to visualize the block transactions. Remove any existing organizations from the folder fabricexplorer and copy organizations data to the fabricexplorer directory . rm -d -r fabricexplorer/organizations cp -R organizations fabricexplorer/ chmod -R a+rwx fabricexplorer Use jq to export and replace keys in bulk for the organizations that are required to be viewed. For example, if there is just manager org, key_value=$(ls organizations/peerOrganizations/manager.beefsupply.com/users/User1@manager.beefsupply.com/msp/keystore/) export key_value Spin up the fabric explorer container, cd fabricexplorer && docker-compose up -d Blockchain explorer will run on interface localhost:8090 with username: exploreradmin and pass: exploreradminpw Bring down the container manually, docker stop explorerdb.beefnetwork.com docker rm explorerdb.beefnetwork.com docker stop explorer.beefnetwork.com docker rm explorer.beefnetwork.com rm -d -r fabricexplorer/organizations fuser -k 8090/tcp","title":"Blockchain Network"},{"location":"blockchain/#blockchain-network","text":"The submodule files located under /blockchain directory are meant to start a hybrid permissioned blockchain network that can be extended to form different groups for collaboration. The blockchain framework apart from running containers, requires hyperledger fabric version 2.5 related binary files located under BeefMesh/fabric-samples/bin folder. Downloading the hyperledger fabric within the BeefMesh folder will create the fabric-samples/bin automatically. To download the fabric 2.5 binary files within the BeefMesh folder, call the blockchain-net.sh file in /blockchain folder. # Note that this only downloads binary files and container images! ./blockchain/blockchain-net.sh downloadfabric This will download required docker images and binary files in a /bin folder inside BeefMesh/blockchain directory. The rest of the tutorial is divided into two sections. One for testing locally on a single host and one for testing on a multi host setup.","title":"Blockchain Network"},{"location":"blockchain/#a-note-on-environment-variables","text":"Environmnet variables are automatically picked up from configuration files and stored under blockchain/blockchainEnvVar for different organizations, peers and orderers. The format of the folders inside blockchain/blockchainEnvVar is of the form organizationX_Y, where X is the organziation number and Y is the peer number. This allows storing multiple organziations of the same type, e.g. first breeder org with one peer node can be stored as breeder0_0 and when another peer gets added, it can be stored as breeder0_1. Or or separate new breeder organizations can be added as breeder1_0, breeder2_0, ... and so on. Next, blockchain/OrgConfigurations folder allows storing network configuration files so that different set of organzations can be configured for a particualr channel using a particular orderer. Use the chaincode_path.sh, channel_path.sh and configtx_path.sh files to target different organization configurations. For example, if all the settings are set for 'main' configuration, then different files inside blockchain/OrgConfigurations/main folder can be used to spin up different set of blockchain confgurations when creating channels, installing chaincode etc.","title":"A Note on Environment Variables"},{"location":"blockchain/#singlehost-setup","text":"Once the binary are downloaded in the BeefMesh/blockchain/bin folder, start the minimim setup of manager and regulator for the main server. ./blockchain/blockchain-net.sh blockchainup tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh blockchainup <starting_channel_name> The initial blockchain framework uses network configuration files in blockchain/configtx folder to start up a blockchain network of manager and regulator that are connected with a channel (tracechannel) and to a docker bridge named 'beef_supply'. Here is the basic overview of the sequence of files/functions called. call the network.sh file in BeefMesh folder pick up configuration files in BeefMesh/blockchain/configtx folder pick up binary files in BeefMesh/blockchain/bin folder start certificate authority (CA) containers for manager and regulator with a single orderer using docker compose files in blockchain/compose/docker/ folder call the enrollment functions for manager, regulator and orderer in organizations/fabric-ca/registerEnroll.sh that creates certificates and stores them for each organziation under BeefMesh/organizations/peerOrganizations/ and BeefMesh/organizations/ordererOrganizations/ start the database and base containers for orderer and peer using the docker compose files in blockchain/compose/docker/ folder call the blockchain/scripts/createChannel.sh to start the initial channel which creates the genesis block (tracechannel.block) in blockchain/channel-artifacts folder along with setting ports for anchor peers. This setup can be brought down any time ./blockchain/blockchain-net.sh blockchaindown tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh blockchaindown <starting_channel_name> The blockchain network comprising of manager and regulator consits of a peer node in each organization and an orderer node. The orderer node is required to arrange the blockchain files after consensus when they are stored. Each organization can have their on multiple orderers and peers. The tutorial below describes how to add a new custom organization (rancher), a new peer (for regulator) and a new orderer (for racher) along with generic organization, peer and orderer with user configured settings.","title":"SingleHost Setup"},{"location":"blockchain/#managing-a-new-organization","text":"Expand the network and add a new organization (rancher) to the network. Call the addRancher.sh file in the BeefMesh/addrancher/ folder ./blockchain/blockchain-net.sh addrancher tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addrancher <starting_channel_name> This adds a new organization to the already running network of blockchain and joins it to the channel tracechannel. Note an error of the type \"Error: proposal failed (err: bad proposal response 500: cannot create ledger from genesis block: ledger [<channel_name>] already exists with state [ACTIVE])\" means that volumes from previous configurations are still being reported by the system. This can be fixed by removing the organizations and unused docker volumes completely. ` To remove the organization from the network and from the channel, run ./blockchain/blockchain-net.sh removerancher tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removerancher <starting_channel_name> This removes the organziation completely from the network and from the channel while reconfiguring the network.","title":"Managing a new organization"},{"location":"blockchain/#managing-new-peers","text":"Add a new peer for the regulator organization ./blockchain/blockchain-net.sh addregulatorpeer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addregulatorpeer <starting_channel_name> Remove the extra peer from regulator organization ./blockchain/blockchain-net.sh removeregulatorpeer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removeregulatorpeer <starting_channel_name> Since manager and regulator setup is assumed to be running on the same host along with an orderer node, the new organization running on some other host (multi-host scenario) might not be able to directly access the single running orderer. Start a new orderer for the rancher organization","title":"Managing new peers"},{"location":"blockchain/#managing-new-orderer","text":"Add a new orderer for the regulator and manager organization ./blockchain/blockchain-net.sh addrancherorderer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh addrancherorderer <starting_channel_name> Remove the extra orderer just added above for the rancher organization ./blockchain/blockchain-net.sh removerancherorderer tracechannel # generic call (replace the starting channel name variable) ./blockchain/blockchain-net.sh removerancherorderer <starting_channel_name>","title":"Managing new orderer"},{"location":"blockchain/#custom-multihost-setup","text":"Note: A list of ports and addresses used by blockchain containers is maintained under blockchain/blockchain_ports_master.txt and would be helpful to maintain when running all containers locally or in multihost setup! When adding more organziations, just increase port number and make sure it is not used with any other container The scripts under this section can be run in single host and multi-host environment and are meant to be flexible enough to allow adpating quickly for new organizations, peers and orderers. Before using for multihost setup, make sure all networks are using 'overlay' option instead of 'bridge' in the docker-compose files. A difference between running scripts locally and multihost setup is summarized below. For multihost setup, configure the PeerOrgEnv.sh environment files with the ip address of the peer node instead of using localhost. Ports can remain the same. In a multihost setup, each organziation should setup their own orderer since it is not advisable to share configurations with other nodes running orderer to update changes to common channel. Initially, the orderers are added by sharing configurations in a group that other organziations can use to make udpates to channel. Once the changes are made, each organziation can continue with their own orderer to make more channels and make changes to it. Due to limitation of space, combined scripts are provided that can be split into more than one script that requires utilizing an orderer owned by other organization. This requires sharing files using collaboration server to allow required changes to server. The main changes that are required to use other organizations orderers are when a channel genesis file is required or when the organziaton is not on the channel but wants to join.","title":"Custom MultiHost Setup"},{"location":"blockchain/#custom-organization","text":"To bring up a custom organization to an already existing network started by manager and regulator, use the addCustom.sh file in /addustomorg folder. First change the environment variables (non-conflicting org name, ports, addresses in the addcustomorg/env/env.sh file. Then export the variables in the terminal source addcustomorg/env/env.sh Note that folder name can be changed when resuing the template. Also change the folder name path variable in the env/env.sh file Call the the setparamcustomorg method to set configuration for the new organization ./blockchain/blockchain-net.sh setparamcustomorg Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustomorg tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustomorg tracechannel","title":"Custom Organization"},{"location":"blockchain/#custom-peer","text":"Note that extra peer uses the same ports for certificate authority for the organization to which it is added to To bring up a custom peer to the already existing network started by manager and regulator, use the addCustom.sh file in /addustompeer folder. First change the envrionemnt variables (non-conflicting org name, ports, addresses in the addustompeer/env/env.sh file. Then export the variables in the terminal source addcustompeer/env/env.sh Call the the setparamcustompeer method to set configuration for the new peer ./blockchain/blockchain-net.sh setparamcustompeer Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustompeer tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustompeer tracechannel","title":"Custom Peer"},{"location":"blockchain/#custom-orderer","text":"To bring up a custom orderer to the already existing network started by manager and regulator, use the addCustom.sh file in /addustomorderer folder. First change the envrionemnt variables (non-conflicting org name, ports, addresses in the addustomorderer/env/env.sh file. Then export the variables in the terminal source addcustomorderer/env/env.sh Call the the setparamcustompeer method to set configuration for the new peer ./blockchain/blockchain-net.sh setparamcustomorderer Now, bring up or bring down the new organization # bring organziation up ./blockchain/blockchain-net.sh addcustomorderer tracechannel # bring organziation down ./blockchain/blockchain-net.sh removecustomorderer tracechannel","title":"Custom Orderer"},{"location":"blockchain/#manage-blockchain-channels","text":"To create new channels for use after an organzation has been setup, use the porvided scripts as follows ./blockchain/blockchain-net.sh createchannel <channel_name> ./blockchain/blockchain-net.sh createchannel cedric Note that, before calling the script to use, make changes to environment variables under blockchain/OrgConfigurations and particularly createChannelOrgsCustom.txt file with first line being the channel configuration to be picked up from configtx file, the second line being the orderer to use, the third being the main organization which will start the process and fourth line being the list of organizations that will be part of the channel.","title":"Manage Blockchain Channels"},{"location":"blockchain/#deploy-chaincode-to-channel","text":"Once channels have been created, smart contracts need to be installed on the channels to use them. Same version of the program should be pakcaged and installed on all peers using the same channel to be able to make changes to the same data when they have the rights to do so. A sample Beef Chain specific program is provided under examples/emissionschaincode written in GO language. To install the program, use the custom scripts provided Note that while environment variables are automatically set for rest of the application, a record needs to be manually made before installing chaincode for the peer connection paramameters in the file blockchain/scripts/envVar.sh and in the function parsePeerConnectionParameters() ./blockchain/blockchain-net.sh installcontract <channel_name> <program_name> <program_location> <program_language> <collaboration_orgs> ./blockchain/blockchain-net.sh installcontract tracechannel traceexample ./examples/emissionschaincode/ go \"OR('ManagerMSP.peer','RegulatorMSP.peer')\" Not that this again makes use of target envrionment files in blockchain/OrgConfigurations and particularly installChaincodeOrgsCustom.txt file. The smartcontract can be reused by copying the program under different folder and installing with different program name.","title":"Deploy Chaincode to Channel"},{"location":"blockchain/#deploy-chaincode-to-channel_1","text":"Once channels have been created, smart contracts need to be installed on the channels to use them. Same version of the program should be pakcaged and installed on all peers using the same channel to be able to make changes to the same data when they have the rights to do so. A sample Beef Chain specific program is provided under examples/emissionschaincode written in GO language. To install the program, use the custom scripts provided ./blockchain/blockchain-net.sh installcontract <channel_name> <program_name> <program_location> <program_language> <collaboration_orgs> ./blockchain/blockchain-net.sh installcontract tracechannel traceexample ./examples/emissionschaincode/ go \"OR('ManagerMSP.peer','RegulatorMSP.peer')\" Not that this again makes use of target envrionment files in blockchain/OrgConfigurations and particularly installChaincodeOrgsCustom.txt file. The smartcontract can be reused by copying the program under different folder and installing with different program name.","title":"Deploy Chaincode to Channel"},{"location":"blockchain/#writing-and-reading-data-from-chaincode","text":"Once program is installed, a helper function file is provided to read and write data into the channel using the installed program. To write a new animal record to the program, ./blockchain/blockchain-writer.sh createanimal <main_org> <orderer_to_use> <channel_name> <program_name> <animal_properties_file> <some_info> ./blockchain/blockchain-writer.sh createanimal manager0_0 orderer tracechannel traceexample animal1.sh NewAnimal Note that the record above uses an animal properties file located under examples/ folder and writes the animal_id that is generated by the blockchain under examples/animalrecords. To read back the data recorded on the blockchain, ./blockchain/blockchain-writer.sh readanimal <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh readanimal manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 Note that, we use the animal_id (example here) that was generated from the first stage of recording the animal on blockchain to target later blockchain changes. To read private data of animal from blockchain by the owner who created the data, ./blockchain/blockchain-writer.sh readprivate <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh readprivate manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 To make changes to the data stored on the blockchain, particularly the description of data, ./blockchain/blockchain-writer.sh changedescription <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <description> ./blockchain/blockchain-writer.sh changedescription manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 AnimalForSale To record data from traceability channels such as the IPFS or some other general data, ./blockchain/blockchain-writer.sh recorddata <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <data> <target_function> ./blockchain/blockchain-writer.sh recorddata manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 SomeIPFScid ChangeFarmerData Note that we used ChangeFarmerData target function above as it makes changes to the FarmerData variable. To make changes to other varaibles, some useful target functions are given below: ChangeFarmerData/ChangeFarmerTrace for managing data on FarmerData/FarmerTrace ChangeBreederData/ChangeBreederTrace for managing data on BreederData/BreederTrace ChangeDistributorData/ChangeBreederTrace for managing data on DistributorData/DistributorTrace ChangeRetailerData/ChangeBreederTrace for managing data on RetailerData/RetailerTrace ChangeRegulatorData/ChangeRegulatorTrace for managing data on RegulatorData/RegulatorTrace ChangeConsumerData/ChangeConsumerTrace for managing data on ConsumerData/ConsumerTrace ChangeProcessorData/ChangeProcessorTrace for managing data on ProcessorData/ProcessorTrace To move private animal record to other organization, officially start a selling process, ./blockchain/blockchain-writer.sh agreetosell <seller_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_trade_id>> <price> ./blockchain/blockchain-writer.sh agreetosell manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 109f4b3c50d7b0df729d299bc6f8e9ef9066971f 2500 Check the sale price that was just changed, ./blockchain/blockchain-writer.sh checksaleprice <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh checksaleprice manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 The other orgaznation interested in taking in the animal, puts on their bid price, ./blockchain/blockchain-writer.sh agreetobuy <buyer_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_properties> <price> ./blockchain/blockchain-writer.sh agreetobuy regulator0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 animal1.sh 2500 Note, that the other organization needs to knwow the properties data of the animal under negotiation Check the bid price that was just put on the blockchain, ./blockchain/blockchain-writer.sh checkbidprice <main_org> <orderer_to_use> <channel_name> <program_name> <animal_id> ./blockchain/blockchain-writer.sh checkbidprice regulator0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 If the prices match and the seller agrees to hand over the animals, the seller can tranfer the animal to the other organization by using the other organizations credentails ./blockchain/blockchain-writer.sh transferanimal <seller_org> <orderer_to_use> <channel_name> <program_name> <animal_id> <animal_trade_id> <buyer_org> <price_settled> ./blockchain/blockchain-writer.sh transferanimal manager0_0 orderer tracechannel traceexample c9561e1bfd916da55119de6eae55cda85ac0aa5c89ed606721cd53d25d4bddf6 109f4b3c50d7b0df729d299bc6f8e9ef9066971f regulator0_0 2500","title":"Writing and Reading Data from Chaincode"},{"location":"blockchain/#check-block-details","text":"To check the details of a particular transaction in a block, download and build the application provided here: https://github.com/Deeptiman/blockreader Optionally, use the blockchain explorer application to visualize the block transactions. Remove any existing organizations from the folder fabricexplorer and copy organizations data to the fabricexplorer directory . rm -d -r fabricexplorer/organizations cp -R organizations fabricexplorer/ chmod -R a+rwx fabricexplorer Use jq to export and replace keys in bulk for the organizations that are required to be viewed. For example, if there is just manager org, key_value=$(ls organizations/peerOrganizations/manager.beefsupply.com/users/User1@manager.beefsupply.com/msp/keystore/) export key_value Spin up the fabric explorer container, cd fabricexplorer && docker-compose up -d Blockchain explorer will run on interface localhost:8090 with username: exploreradmin and pass: exploreradminpw Bring down the container manually, docker stop explorerdb.beefnetwork.com docker rm explorerdb.beefnetwork.com docker stop explorer.beefnetwork.com docker rm explorer.beefnetwork.com rm -d -r fabricexplorer/organizations fuser -k 8090/tcp","title":"Check Block Details"},{"location":"databases/","text":"Databases A number of databases are setup to allow organizations to consume different types of data from events and processes occuring within the facility from which useful information can be extracted and shared. The databases can be individually spun up using docker-compose file or as part of the main setup for BeefMesh. Note that the databases are setup individually for different organizations using internal network and does not use the global beef_suppy bridge or overlay network. This is because databases consume internal organizational data which should not be exposed to external components. Examples databases are setup for 'farmer' organization using 'farmer' network under /databases/farmer which can be extended to other organizations by reusing the same files. Specific .sql formatted tables with beef chain specific parameters are available under /databases/beefchainparameters and can be used with postgres, mariadb or mysql. The tables can be split or combined depending upon the supply chain organziation from where the specific parameters are avaialble for consumption . MariaDB MariaDB is an open-source relational database management system (RDBMS) and one of the most popular alternatives to MySQL. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/mariadb directory. docker-compose up -d Once up and running, there will be two containers that manage MariaDB database. Mariadb image spins up with the name 'farmermdb' and accompanying phpmyadmin contianer to manage SQL datbases runs up with the name 'phpmyadmin'. Mariadb container runs on port \"3307:3306\" and phpadmin container runs on port \"8082:8080\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for mariadb and mysql have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent database is attached to the container at runtime 'farmerdb_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .sql formatted queries in 'init' folder. Once the container is up and running, pass any sql query from files to the container, for example docker exec -i farmermdb mysql -uroot -pfarmermdb < farmer.sql To pass any query from bash, docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"show databases\" docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"use cattle_db; show tables\" docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"use cattle_db; show tables; select * from cattle\" The mariadb cli interface can also be used to do the same. For example, docker exec -it farmermdb mariadb --user root -pfarmermdb -e \"show databases\" To directly access mariadb cli interface within the container, docker exec -it farmermdb mariadb --user root -pfarmermdb Once inside the container, directly use sql queries at the prompt. In addition, it can be useful to directly access the phpmyadmin page running at 'http://localhost:8082' using 'farmermdb' for username and password. In addition, sotrage engine can be specified during the creation of database to allow utilizing the full potential for parallel or scalable application types. See more details here: https://mariadb.com/kb/en/storage-engines/ To bring down the Mariadb containers docker stop farmerpma && docker stop farmermdb && docker rm farmerpma && docker rm farmermdb # make backup before removing docker volume docker volume rm mariadb_farmerdb_data && docker volume prune MongoDB MongoDB is a popular open-source, NoSQL database program. It falls under the category of document-oriented databases, which means it stores data in flexible, JSON-like documents. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/mongodb directory. docker-compose up -d Once up and running, there will be two containers that manage MongoDB database. MongoDB image spins up with the name 'farmermongodb' and accompanying mongo-express contianer to manage mongo datbases runs up with the name 'farmermongoexpress'. Mongodb container runs on port \"27018:27017\" and mongo-express container runs on port \"8089:8081\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for mongoDB and mongo-express have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent database is attached to the container at runtime 'farmermongo_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .json formatted files in 'init' folder. Uncomment relevant lines in docker-compose script to allow using database initialization and custom configuration at container runtime. Once the container is up and running, directly access the continer, docker exec -it farmermongodb bash Once inside the container, access mongo cli interace, mongo mongodb://localhost:27017 -u farmermdb -p farmermdb More details on Mongodb shell commands can be found at: https://www.mongodb.com/docs/mongodb-shell/run-commands/ You can also directly access the shell in one go, docker exec -it farmermongodb mongo mongodb://localhost:27017 -u farmermdb -p farmermdb Or pass any command to mongo shell from outside the container using bash. For example, create a new user credential and a database named 'farmer', docker exec -it farmermongodb mongo mongodb://localhost:27017 -u farmermdb -p farmermdb --eval \"db.createUser({user: 'farmer', pwd: 'farmer', roles: [{role: 'readWrite', db: 'farmer'}]})\" With the flexiblity of mongodb database to store massive collections of data in json format, it is an ideal database for consuming beef supply chain data, storing documents and sensor data. To store a collection of data in .json format, lets first copy the data file (cattle_data.json) into the container in /tmp folder. docker cp databases/farmer/mongodb/cattle_data.json farmermongodb:/tmp/cattle_data.json Next, login into the container and use 'mongoimport' tool to put the example file in a database, docker exec -it farmermongodb bash # Inside the container run, mongoimport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --file tmp/cattle_data.json --jsonArray You can also run the command directly from outside the container using bash once you have copied the file into the container, docker exec -it farmermongodb mongoimport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --file tmp/cattle_data.json --jsonArray To retireve a collection of data from a database, docker exec -it farmermongodb mongoexport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --out exported_data.json This retrieves the data collection and saves into the output file exported_data.json. You can copy the file to host machine from the container, docker cp farmermongodb:exported_data.json exported_data.json Or you can directly retreive data from the database collection and store it in the host machine using bash, docker exec -it farmermongodb mongoexport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb > exported_data_direct.json The data will be stored in a file 'exported_data_direct.json' in the host machine. In addition, it can be useful to diretly access the mongo-express page running at 'http://127.0.0.1:8089' or 'localhost:8089' using 'farmermdb' for username and password. Mongoexpress allows a graphical interface to manage databases stored in mongodb. To bring down the MongoDB containers docker stop farmermongoexpress && docker stop farmermongodb && docker rm farmermongoexpress && docker rm farmermongodb # make backup before removing docker volume docker volume rm mongodb_farmermongo_data && docker volume prune PostgreSQL PostgreSQL, which is a powerful open-source relational database management system (RDBMS). For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/postgresql directory. docker-compose up -d Once up and running, there will be two containers that manage MongoDB database. PostgreSQL image spins up with the name 'farmerpostgres' and accompanying pgadmin contianer to manage potgres datbases runs up with the name 'padmin_farmer'. Postgres container runs on port \"5431:5432\" and pgadmin container runs on port \"8887:80\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for postgres and pgadmin have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent volume is attached to the container at runtime 'farmerpostgres' including a drive 'farmerpostgres_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .sql formatted files in 'init' folder. Uncomment relevant lines in docker-compose script to allow using database initialization and custom configuration at container runtime. Once the container is up and running, directly access the continer, docker exec -it farmerpostgres sh Once inside the container, access psql cli interace and enter password 'farmermdb', psql -h localhost -p 5432 -U farmermdb -W More details on psql shell commands can be found at: https://www.postgresql.org/docs/current/tutorial.html To retrieve (import) an already existing database 'farmermdb' created at runtime, into the host machine using 'pg_dump' from bash terminal, docker exec -i farmerpostgres /bin/bash -c \"PGPASSWORD=farmermdb pg_dump --username farmermdb farmermdb\" > retrieved_data.sql To upload or export data with creation of a new database, first copy .sql file into the container under /tmp folder, docker cp farmerpostgres_data/cattle.sql farmerpostgres:/tmp/cattle.sql Next sh into the container and use 'psql' cli interface to upload data, docker exec -it farmerpostgres sh # Type inside the container terminal, psql -U farmermdb farmermdb -f /tmp/cattle.sql You can also upload data into the database without going inside the container and just using the host terminal and using the file in /tmp folder, docker exec -i farmerpostgres psql -U farmermdb farmermdb -f /tmp/cattle.sql Now to retrieve the same uploaded data in the database 'cattle_db', docker exec -i farmerpostgres /bin/bash -c \"PGPASSWORD=farmermdb pg_dump --username farmermdb cattle_db\" > retrieved_cattle_db.sql In addition, it can be useful to diretly access the pgadmin page running at 'http://127.0.0.1:8887' or 'http://localhost:8887' using 'farmermdb' for username and password. Pgadmin allows a graphical interface to manage databases stored in postgres. Go to 'servers' tab on the left side, create new server connection using localhost as the 'host', port 5431, database 'farmermdb', username and password 'farmermdb'. To bring down the Postgres containers docker stop farmerpostgres && docker stop padmin_farmer && docker rm farmerpostgres && docker rm padmin_farmer # make backup before removing docker volume docker volume rm postgresdb_farmerpostgres && docker volume rm postgresdb_padmin_farmer && docker volume prune Cassandra Cassandra is NoSQL database particularly well-suited for use cases where fast reads and writes are essential, and where data needs to be distributed across multiple locations. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/cassandradb directory. docker-compose up -d Once up and running, there will be a container running Cassandra database. Cassandra image spins up with the name 'farmercassdb'. The container runs on port \"9034:9042\" in addition to ports 7000, 7001, 7199 and 9160 exposed for various functions associated with cassandra. For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' have been set to 'cassandra' for testing. A consistent volume is attached to the container at runtime 'farmercassdb_data' to keep data from being lost. Configuration for the contianers can be done using .yaml files present in the /databases/cassandra/etc folder and including it in the docker-compose file at runtime. A sample docker-compose-cluster.yaml file is also provided if a cluster of cassandra databases is requuired with different configurations for each database inherited from files in /etc folder. Once the container is up and running, check the status of the node (or cluster) docker exec -it farmercassdb nodetool status Directly access the cqlsh cli interace in container, docker exec -it farmercassdb sh cqlsh # Alernate one liner docker exec -it farmercassdb cqlsh More details on cqlsh shell commands can be found at: https://cassandra.apache.org/doc/stable/cassandra/tools/cqlsh.html Once, inside the container, create a keyspace (a namespace used for replicating data on multiple nodes) and use it as a test database, CREATE KEYSPACE my_cattle WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1}; use my_cattle Once the keyspace has been selected for use, create a sample table, CREATE TABLE IF NOT EXISTS cattle_data ( id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT ); Now, insert sample data into the table, INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Daisy', 'Angus', 3, 1100.2); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Spot', 'Hereford', 5, 1300.8); Retrieve, data from table, SELECT * FROM cattle_data; You can also directly pass commands from host terminal without logging into the container, docker exec -it farmercassdb cqlsh -e \"CREATE KEYSPACE my_cattle WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};\" docker exec -it farmercassdb cqlsh -e \"use my_cattle; CREATE TABLE IF NOT EXISTS cattle_data (id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); SELECT * FROM cattle_data;\" # Or dump data directly to a file (data_out.txt) in host machine docker exec -it farmercassdb cqlsh -e \"use my_cattle; CREATE TABLE IF NOT EXISTS cattle_data (id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); SELECT * FROM cattle_data;\" > data_out.txt It is also possible to create tables and upload data from .cql files. Copy sample .cql file in the cassandra folder into the container, docker cp cattle.cql farmercassdb:/cattle.cql Access the docker container cqlsh shell and source the file, docker exec -it farmercassdb cqlsh source '/cattle.cql'; You can also directly pass the commands from host bash terminal after copying the file to the container, docker exec -it farmercassdb cqlsh -e \"source '/cattle.cql';\" To use cassandra cluster, use the docker-compose-cluster.yaml file after configurig required changes. This may generate a lot of commit log files locally in commitlog/ folder, hence make sure there is tons space before running cluster. In addition, it can be useful to directly access the cassandradb-web page running at 'http://127.0.0.1:3003' or 'http://localhost:3003' to manage existing clusters or to create new cassandradb clusters. The cassandradb-web username and password are set to \"cassandra\". To bring down the cassandra containers and remove commit log files docker stop farmercassdb && docker rm farmercassdb && docker stop cassandra-web && docker rm cassandra-web # make backup before removing docker volume docker volume rm cassandradb_farmercassdb_data && docker volume prune # remove commitlog files rm commitlog/cassandra1/* && rm commitlog/cassandra2/* && rm commitlog/cassandra3/* # remove cluster data files after backing up rm data/cassandra1/* && rm data/cassandra2/* && rm data/cassandra3/* Multihost Setup The databases are supposed to be run within each organization and should not be accessible from outside, hence the docker shell can be directly accessed from an administrator to upload or retrieve data. The containers connect to 'farmer' network as a bridge which can be changed to other networks or connections depending upon the applciation scenario using the scripts in /utilities folder. To allow other users with limited rights to view or upload data, create new user accounts directly for the databases and allow users to access the web management portal for the databases by updating and sourcing environment varibles for the deployed url of web tools. # from BeefMesh main directory source environment/service_ip_addresses.sh # access phpmyadmin page for mariadb http://$mariadb_server_address:8082 # access mongo-express page for mongodb http://$mongodb_server_address:8089 # access pgadmin page for postres http://$postgres_server_address:8087 # access cassandra-web page for mariadb http://$cassandra_server_address:3003 Other options could include resuing the flask-based applications for emissions and sessions sub-module to render specific CRUD api for different stored schemas","title":"Databases"},{"location":"databases/#databases","text":"A number of databases are setup to allow organizations to consume different types of data from events and processes occuring within the facility from which useful information can be extracted and shared. The databases can be individually spun up using docker-compose file or as part of the main setup for BeefMesh. Note that the databases are setup individually for different organizations using internal network and does not use the global beef_suppy bridge or overlay network. This is because databases consume internal organizational data which should not be exposed to external components. Examples databases are setup for 'farmer' organization using 'farmer' network under /databases/farmer which can be extended to other organizations by reusing the same files. Specific .sql formatted tables with beef chain specific parameters are available under /databases/beefchainparameters and can be used with postgres, mariadb or mysql. The tables can be split or combined depending upon the supply chain organziation from where the specific parameters are avaialble for consumption .","title":"Databases"},{"location":"databases/#mariadb","text":"MariaDB is an open-source relational database management system (RDBMS) and one of the most popular alternatives to MySQL. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/mariadb directory. docker-compose up -d Once up and running, there will be two containers that manage MariaDB database. Mariadb image spins up with the name 'farmermdb' and accompanying phpmyadmin contianer to manage SQL datbases runs up with the name 'phpmyadmin'. Mariadb container runs on port \"3307:3306\" and phpadmin container runs on port \"8082:8080\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for mariadb and mysql have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent database is attached to the container at runtime 'farmerdb_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .sql formatted queries in 'init' folder. Once the container is up and running, pass any sql query from files to the container, for example docker exec -i farmermdb mysql -uroot -pfarmermdb < farmer.sql To pass any query from bash, docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"show databases\" docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"use cattle_db; show tables\" docker exec -i farmermdb mysql -uroot -pfarmermdb -e \"use cattle_db; show tables; select * from cattle\" The mariadb cli interface can also be used to do the same. For example, docker exec -it farmermdb mariadb --user root -pfarmermdb -e \"show databases\" To directly access mariadb cli interface within the container, docker exec -it farmermdb mariadb --user root -pfarmermdb Once inside the container, directly use sql queries at the prompt. In addition, it can be useful to directly access the phpmyadmin page running at 'http://localhost:8082' using 'farmermdb' for username and password. In addition, sotrage engine can be specified during the creation of database to allow utilizing the full potential for parallel or scalable application types. See more details here: https://mariadb.com/kb/en/storage-engines/ To bring down the Mariadb containers docker stop farmerpma && docker stop farmermdb && docker rm farmerpma && docker rm farmermdb # make backup before removing docker volume docker volume rm mariadb_farmerdb_data && docker volume prune","title":"MariaDB"},{"location":"databases/#mongodb","text":"MongoDB is a popular open-source, NoSQL database program. It falls under the category of document-oriented databases, which means it stores data in flexible, JSON-like documents. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/mongodb directory. docker-compose up -d Once up and running, there will be two containers that manage MongoDB database. MongoDB image spins up with the name 'farmermongodb' and accompanying mongo-express contianer to manage mongo datbases runs up with the name 'farmermongoexpress'. Mongodb container runs on port \"27018:27017\" and mongo-express container runs on port \"8089:8081\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for mongoDB and mongo-express have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent database is attached to the container at runtime 'farmermongo_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .json formatted files in 'init' folder. Uncomment relevant lines in docker-compose script to allow using database initialization and custom configuration at container runtime. Once the container is up and running, directly access the continer, docker exec -it farmermongodb bash Once inside the container, access mongo cli interace, mongo mongodb://localhost:27017 -u farmermdb -p farmermdb More details on Mongodb shell commands can be found at: https://www.mongodb.com/docs/mongodb-shell/run-commands/ You can also directly access the shell in one go, docker exec -it farmermongodb mongo mongodb://localhost:27017 -u farmermdb -p farmermdb Or pass any command to mongo shell from outside the container using bash. For example, create a new user credential and a database named 'farmer', docker exec -it farmermongodb mongo mongodb://localhost:27017 -u farmermdb -p farmermdb --eval \"db.createUser({user: 'farmer', pwd: 'farmer', roles: [{role: 'readWrite', db: 'farmer'}]})\" With the flexiblity of mongodb database to store massive collections of data in json format, it is an ideal database for consuming beef supply chain data, storing documents and sensor data. To store a collection of data in .json format, lets first copy the data file (cattle_data.json) into the container in /tmp folder. docker cp databases/farmer/mongodb/cattle_data.json farmermongodb:/tmp/cattle_data.json Next, login into the container and use 'mongoimport' tool to put the example file in a database, docker exec -it farmermongodb bash # Inside the container run, mongoimport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --file tmp/cattle_data.json --jsonArray You can also run the command directly from outside the container using bash once you have copied the file into the container, docker exec -it farmermongodb mongoimport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --file tmp/cattle_data.json --jsonArray To retireve a collection of data from a database, docker exec -it farmermongodb mongoexport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb --out exported_data.json This retrieves the data collection and saves into the output file exported_data.json. You can copy the file to host machine from the container, docker cp farmermongodb:exported_data.json exported_data.json Or you can directly retreive data from the database collection and store it in the host machine using bash, docker exec -it farmermongodb mongoexport --host localhost --port 27017 --username farmermdb --password farmermdb --authenticationDatabase admin --db farmermdb --collection farmermdb > exported_data_direct.json The data will be stored in a file 'exported_data_direct.json' in the host machine. In addition, it can be useful to diretly access the mongo-express page running at 'http://127.0.0.1:8089' or 'localhost:8089' using 'farmermdb' for username and password. Mongoexpress allows a graphical interface to manage databases stored in mongodb. To bring down the MongoDB containers docker stop farmermongoexpress && docker stop farmermongodb && docker rm farmermongoexpress && docker rm farmermongodb # make backup before removing docker volume docker volume rm mongodb_farmermongo_data && docker volume prune","title":"MongoDB"},{"location":"databases/#postgresql","text":"PostgreSQL, which is a powerful open-source relational database management system (RDBMS). For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/postgresql directory. docker-compose up -d Once up and running, there will be two containers that manage MongoDB database. PostgreSQL image spins up with the name 'farmerpostgres' and accompanying pgadmin contianer to manage potgres datbases runs up with the name 'padmin_farmer'. Postgres container runs on port \"5431:5432\" and pgadmin container runs on port \"8887:80\". For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' for postgres and pgadmin have been set to 'farmermdb' for testing including the creation of a default database 'farmermdb'. A consistent volume is attached to the container at runtime 'farmerpostgres' including a drive 'farmerpostgres_data' to keep data from being lost. Configuration for the contianers can be done using the file in 'config' folder and any number of databases and tables can be initiated using .sql formatted files in 'init' folder. Uncomment relevant lines in docker-compose script to allow using database initialization and custom configuration at container runtime. Once the container is up and running, directly access the continer, docker exec -it farmerpostgres sh Once inside the container, access psql cli interace and enter password 'farmermdb', psql -h localhost -p 5432 -U farmermdb -W More details on psql shell commands can be found at: https://www.postgresql.org/docs/current/tutorial.html To retrieve (import) an already existing database 'farmermdb' created at runtime, into the host machine using 'pg_dump' from bash terminal, docker exec -i farmerpostgres /bin/bash -c \"PGPASSWORD=farmermdb pg_dump --username farmermdb farmermdb\" > retrieved_data.sql To upload or export data with creation of a new database, first copy .sql file into the container under /tmp folder, docker cp farmerpostgres_data/cattle.sql farmerpostgres:/tmp/cattle.sql Next sh into the container and use 'psql' cli interface to upload data, docker exec -it farmerpostgres sh # Type inside the container terminal, psql -U farmermdb farmermdb -f /tmp/cattle.sql You can also upload data into the database without going inside the container and just using the host terminal and using the file in /tmp folder, docker exec -i farmerpostgres psql -U farmermdb farmermdb -f /tmp/cattle.sql Now to retrieve the same uploaded data in the database 'cattle_db', docker exec -i farmerpostgres /bin/bash -c \"PGPASSWORD=farmermdb pg_dump --username farmermdb cattle_db\" > retrieved_cattle_db.sql In addition, it can be useful to diretly access the pgadmin page running at 'http://127.0.0.1:8887' or 'http://localhost:8887' using 'farmermdb' for username and password. Pgadmin allows a graphical interface to manage databases stored in postgres. Go to 'servers' tab on the left side, create new server connection using localhost as the 'host', port 5431, database 'farmermdb', username and password 'farmermdb'. To bring down the Postgres containers docker stop farmerpostgres && docker stop padmin_farmer && docker rm farmerpostgres && docker rm padmin_farmer # make backup before removing docker volume docker volume rm postgresdb_farmerpostgres && docker volume rm postgresdb_padmin_farmer && docker volume prune","title":"PostgreSQL"},{"location":"databases/#cassandra","text":"Cassandra is NoSQL database particularly well-suited for use cases where fast reads and writes are essential, and where data needs to be distributed across multiple locations. For testing with docker as a standalone application, spin up the container by navigating into the /databases/farmer/cassandradb directory. docker-compose up -d Once up and running, there will be a container running Cassandra database. Cassandra image spins up with the name 'farmercassdb'. The container runs on port \"9034:9042\" in addition to ports 7000, 7001, 7199 and 9160 exposed for various functions associated with cassandra. For testing purposes, the environment variables are directly specified into the docker-compose file. Pass sensitive information to contianers such as credentials using 'secrets' as described in the 'Sessions' tutorial page. The credentials 'usernames' and 'passwords' have been set to 'cassandra' for testing. A consistent volume is attached to the container at runtime 'farmercassdb_data' to keep data from being lost. Configuration for the contianers can be done using .yaml files present in the /databases/cassandra/etc folder and including it in the docker-compose file at runtime. A sample docker-compose-cluster.yaml file is also provided if a cluster of cassandra databases is requuired with different configurations for each database inherited from files in /etc folder. Once the container is up and running, check the status of the node (or cluster) docker exec -it farmercassdb nodetool status Directly access the cqlsh cli interace in container, docker exec -it farmercassdb sh cqlsh # Alernate one liner docker exec -it farmercassdb cqlsh More details on cqlsh shell commands can be found at: https://cassandra.apache.org/doc/stable/cassandra/tools/cqlsh.html Once, inside the container, create a keyspace (a namespace used for replicating data on multiple nodes) and use it as a test database, CREATE KEYSPACE my_cattle WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1}; use my_cattle Once the keyspace has been selected for use, create a sample table, CREATE TABLE IF NOT EXISTS cattle_data ( id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT ); Now, insert sample data into the table, INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Daisy', 'Angus', 3, 1100.2); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Spot', 'Hereford', 5, 1300.8); Retrieve, data from table, SELECT * FROM cattle_data; You can also directly pass commands from host terminal without logging into the container, docker exec -it farmercassdb cqlsh -e \"CREATE KEYSPACE my_cattle WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};\" docker exec -it farmercassdb cqlsh -e \"use my_cattle; CREATE TABLE IF NOT EXISTS cattle_data (id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); SELECT * FROM cattle_data;\" # Or dump data directly to a file (data_out.txt) in host machine docker exec -it farmercassdb cqlsh -e \"use my_cattle; CREATE TABLE IF NOT EXISTS cattle_data (id UUID PRIMARY KEY, name TEXT, breed TEXT, age INT, weight FLOAT); INSERT INTO cattle_data (id, name, breed, age, weight) VALUES (uuid(), 'Bessie', 'Holstein', 4, 1200.5); SELECT * FROM cattle_data;\" > data_out.txt It is also possible to create tables and upload data from .cql files. Copy sample .cql file in the cassandra folder into the container, docker cp cattle.cql farmercassdb:/cattle.cql Access the docker container cqlsh shell and source the file, docker exec -it farmercassdb cqlsh source '/cattle.cql'; You can also directly pass the commands from host bash terminal after copying the file to the container, docker exec -it farmercassdb cqlsh -e \"source '/cattle.cql';\" To use cassandra cluster, use the docker-compose-cluster.yaml file after configurig required changes. This may generate a lot of commit log files locally in commitlog/ folder, hence make sure there is tons space before running cluster. In addition, it can be useful to directly access the cassandradb-web page running at 'http://127.0.0.1:3003' or 'http://localhost:3003' to manage existing clusters or to create new cassandradb clusters. The cassandradb-web username and password are set to \"cassandra\". To bring down the cassandra containers and remove commit log files docker stop farmercassdb && docker rm farmercassdb && docker stop cassandra-web && docker rm cassandra-web # make backup before removing docker volume docker volume rm cassandradb_farmercassdb_data && docker volume prune # remove commitlog files rm commitlog/cassandra1/* && rm commitlog/cassandra2/* && rm commitlog/cassandra3/* # remove cluster data files after backing up rm data/cassandra1/* && rm data/cassandra2/* && rm data/cassandra3/*","title":"Cassandra"},{"location":"databases/#multihost-setup","text":"The databases are supposed to be run within each organization and should not be accessible from outside, hence the docker shell can be directly accessed from an administrator to upload or retrieve data. The containers connect to 'farmer' network as a bridge which can be changed to other networks or connections depending upon the applciation scenario using the scripts in /utilities folder. To allow other users with limited rights to view or upload data, create new user accounts directly for the databases and allow users to access the web management portal for the databases by updating and sourcing environment varibles for the deployed url of web tools. # from BeefMesh main directory source environment/service_ip_addresses.sh # access phpmyadmin page for mariadb http://$mariadb_server_address:8082 # access mongo-express page for mongodb http://$mongodb_server_address:8089 # access pgadmin page for postres http://$postgres_server_address:8087 # access cassandra-web page for mariadb http://$cassandra_server_address:3003 Other options could include resuing the flask-based applications for emissions and sessions sub-module to render specific CRUD api for different stored schemas","title":"Multihost Setup"},{"location":"debugging/","text":"Debugging Issues Direct any issue or questions to alisalm1 AT msu DOT edu Network Shared Drive Note: This is applicable on Server and Client If gluster daemon is not running, restart it ./shared-drive.sh restartgluster If previously created shared volumes are causing issues # Remove caches and earlier volume related data (make backup before doing so!) ./shared-drive.sh fixfolderconflict # Fix permissions for gluster application ./shared-drive.sh fixpermissions Removing cache and other related files from gluster library for previously configured volumes may generate extra messages (particularly related to geographic data) such as below but does not interfere with normal operation. This was tested for glusterfs 10.1. Traceback (most recent call last): File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 325, in <module> main() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 41, in main argsupgrade.upgrade() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 85, in upgrade init_gsyncd_template_conf() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 50, in init_gsyncd_template_conf fd = os.open(path, os.O_CREAT | os.O_RDWR) FileNotFoundError: [Errno 2] No such file or directory: '/var/lib/glusterd/geo-replication/gsyncd_template.conf'","title":"Debugging Issues"},{"location":"debugging/#debugging-issues","text":"Direct any issue or questions to alisalm1 AT msu DOT edu","title":"Debugging Issues"},{"location":"debugging/#network-shared-drive","text":"Note: This is applicable on Server and Client If gluster daemon is not running, restart it ./shared-drive.sh restartgluster If previously created shared volumes are causing issues # Remove caches and earlier volume related data (make backup before doing so!) ./shared-drive.sh fixfolderconflict # Fix permissions for gluster application ./shared-drive.sh fixpermissions Removing cache and other related files from gluster library for previously configured volumes may generate extra messages (particularly related to geographic data) such as below but does not interfere with normal operation. This was tested for glusterfs 10.1. Traceback (most recent call last): File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 325, in <module> main() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 41, in main argsupgrade.upgrade() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 85, in upgrade init_gsyncd_template_conf() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 50, in init_gsyncd_template_conf fd = os.open(path, os.O_CREAT | os.O_RDWR) FileNotFoundError: [Errno 2] No such file or directory: '/var/lib/glusterd/geo-replication/gsyncd_template.conf'","title":"Network Shared Drive"},{"location":"emissions/","text":"Emissions This flask based submodule located under /emissions directroy is meant to serve as a common carbon emissions database maintained by a regulator. Emission parameters and factors are added to the database after neogtiation and voting on blockchain channels. Once a emissions factor is finalized, it is added to the database for users to consult from when calculating emissions at their end. For testing purposes as a standalone application from BeefMesh framework, create a virtual envrionment virtualenv emissions source emissions/bin/activate pip install -r requirements.txt python -m main Once up and running, use curl (https://curl.haxx.se/download.html) or httpie (https://httpie.io/) commands to interact with the API. For running the application for testing without docker may require setting credentials. These can be imported by \"$ source emissions/env/env.sh\" or by settting them as, export EMISSION_SUPERADMIN_PASSWORD=emissions export EMISSION_ADMIN_PASSWORD=emissions export EMISSION_TEST_USER_PASSWORD=emissions export EMISSION_GENERIC_USER_PASSWORD=emissions By default, the application initializes with the following credentials defined in /api/db_initializer/db_initializer.py: Super Admin Credentials: username: username_superadmin password: emissions email: email_superadmin@emissions.com Admin Credentials: username: username_admin password: emissions email: email_admin@emissions.com Test User Credentials: username: username_test password: emissions email: email_test@emissions.com Emissions User Credentials: username: username_emissions password: emissions email: email_emissions@emissions.com The database file is created and maintained under /api/db_files. In docker, the credentials are imported as secrets and kept under /emissions/secrets for testing purposes. For testing with docker as a standalone application, spin up the containerby navigating into the /emissions directory. docker-compose up -d Once up and running, different routes can be used to interact with the application. Some example routes are given below. Note: The curl commands below use 'localhost' in the url section. This would he replaced with the IP or URL mapping of the hosted application when accessing from an outside host. The port '6001' can be changed to whatever port is suitable for your system . The application consits of custom emission factors added for beef supply chain. Users can add their own paramters under generic category. Register Users curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@emissions.com\"}' https://localhost:6001/emissions/v1/auth/register Note that we use -k flag to use dummy certificates generated from openssl application or with arbitrary certificate assignment by flask. Login Users curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@emissions.com\",\"password\":\"example_password\"}' https://localhost:6001/emissions/v1/auth/login Logging in a user generates an Access Token and a Refresh Token that can be used to access resources on the server. An example of the response with tokens is given below. { \"access_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU0Mjg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20iLCJhZG1pbiI6MH0.DlJnfzALoul7CH_jh0_R6SpNF-2w5xzx7iu2ZR-AXh_ErFyfl_KMOLBLyZ8smX7gSe1sfvJzKHAgQPoqlUnR7w\", \"refresh_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU3ODg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20ifQ.ysZ8JDAj5SlPe0rbfr2DXGL0BPCxZ-TFvqRyvabcNYkyKJmWXP3WkJ8xryFtxIE12a04SR-VkFyiLLjGnJvVjw\" } The tokens will be passed whereever user is requied to be logged in on the server to access any resources. Logout Users curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:6001/emissions/v1/auth/logout Note that the above command requires passing values for token variables. Refresh Tokens curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:6001/emissions/v1/auth/refresh Reset Password curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://localhost:6001/emissions/v1/auth/password_reset Remove User curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@emissions.com\"}' https://localhost:6001/emissions/v1/auth/remove_user Note that removing a user requires super admin permissions and the super admin needs to be logged in to be able to pass on their access tokens. These requirements can be changed under relevant classes which are classed against each route. These classes can be found in UserHandlers.py file under /api/handlers Checking Permissions To check whether a user should be allowed to access any resource on the server, curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_user_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_admin_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_sadmin_permission The route will return a {\"status\": \"success\"} or {\"status\": \"failure\"} message depending upon whether the user is succesffuly logged in and using correct token. This can be used as a condition to allow access to resources. Testing Permissions Some test routes for debugging user permissions are provided below. curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_admin curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_super_admin Generic Parameters To add generic parameters for emissions calculations, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"factor\":\"0.0017\",\"link\":\"https://shorturl.at/hiBCG\",\"code\":\"plastic\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_generic A reponse could look like { \"status\": \"successfully added parameters to generic data\" } In the above call to emissions API, 'unit' is used as a measure for resource consumption input against which emissions are calculated, for example 500 kg of platic use and factor is a floating value. 'Code' can be used to refer to category. 'link' is used to refer to the source, 'region' can be used to highlight the area, the factor is specific to. Further 'info' can also be added to make use of more parameters to fine tune emissions or to get a detaled view of underlying processes. For example, when calculating emissions from a 'heating' process, the boiler efficiency percentage, the water temperature, losses in tranfer of heating can be taken into account. To get a view of all the available factors for generic category, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors A reponse could look like [ { \"name\": \"generic\", \"id\": 1, \"region\": \"michigan\", \"unit\": \"kWh\", \"factor\": 0.000433, \"link\": \"https://www.epa.gov/energy/greenhouse-gases-equivalencies-calculator-calculations-and-references\", \"code\": \"electricity\", \"info\": \"92.7% efficiency over distribution lines\" }, { \"name\": \"generic\", \"id\": 2, \"region\": \"michigan\", \"unit\": \"kg\", \"factor\": 0.0017, \"link\": \"https://shorturl.at/hiBCG\", \"code\": \"plastic\", \"info\": \"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\" } ] To get the resultant value of total emissions against a particular amount of resource consumption, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"code\":\"electricity\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_generic Here, 'value' is used as a place holder for resource consumption, for example 1000 kWh. A reponse could look like one given below. Note that the parameter 'scale' refers to the fact that the resultant value of total emissions given in 'result' is in metric ton CO2 equivalent. { \"factor\": 0.000433, \"result\": 0.433, \"info\": \"92.7% efficiency over distribution lines\", \"unit\": \"kWh\", \"scale\": \"metric ton CO2-equivalent\" } Note that the resultant values are given in CO2 equivalent by converting the input value (e.g. CO2 into CO2-equivalent) assuming that the input value is the only form of emissions available. The factors are used from online resources and not all of them take into account all of the CO2-equivalent contributing gases. If other measures of contributing gases (such as methane) are also available, add their CO2-equivalent contribution (in metric ton) to the resultant values . Retrieving all Info Note: The curl commands from here onwards have the 'Authorization' header part removed for testing purposes. Enable Authorization for the routes by including @auth.login_required and @role_required.permission(2) for the relevant classes called by routes in the file UserHandlers.py To get all info of factors available related to different categories, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors surl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors Example calls to API for inserting factors into database or retreiving resultant emissions from available factors in different custom categories are summarized below . Electricity Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"factor\":\"0.000433\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"92.7% efficiency\"}' https://localhost:6001/emissions/v1/auth/insert_data_electricity curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"50000\"}' https://localhost:6001/emissions/v1/auth/get_data_electricity Diesel Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"factor\":\"0.001219\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_diesel curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_diesel Gasoline Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"factor\":\"0.0010617\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_gasoline curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_gasoline Fossil Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"factor\":\"0.000904\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_fossil curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"4000\"}' https://localhost:6001/emissions/v1/auth/get_data_fossil Naturalgas Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"factor\":\"0.00005301\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_naturalgas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://localhost:6001/emissions/v1/auth/get_data_naturalgas Biogas Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"factor\":\"0.0000247\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"carbon reduction, (electricity*0.0571) assuming 1 cubic meterbiogas = 2kWh energy\"}' https://localhost:6001/emissions/v1/auth/insert_data_biogas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_biogas Solar Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"factor\":\"0.00005\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"solar panels offset 50 grams of CO2 for every kilowatt-hour of power produced\"}' https://localhost:6001/emissions/v1/auth/insert_data_solar curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_solar Windturbine Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"factor\":\"0.000006\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"offset 6 grams of CO2 for every kilowatt-hour of power produced\"}' https://localhost:6001/emissions/v1/auth/insert_data_windturbine curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"2000\"}' https://localhost:6001/emissions/v1/auth/get_data_windturbine Steam Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"factor\":\"0.000004\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"assuming 80% efficient boiler and converting used steam to natural gas, also assuming water is coming in at 60F (about 15C) with 28 btu\"}' https://localhost:6001/emissions/v1/auth/insert_data_steam curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://localhost:6001/emissions/v1/auth/get_data_steam Feed Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"factor\":\"0.000032\",\"link\":\"replace_url_here\",\"code\":\"organicgrass\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from 80% dry matter\"}' https://localhost:6001/emissions/v1/auth/insert_data_feed curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"code\":\"alfalfa\",\"region\":\"michigan\",\"value\":\"10000\"}' https://localhost:6001/emissions/v1/auth/get_data_feed Byproducts Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"factor\":\"0.000015\",\"link\":\"replace_url_here\",\"code\":\"discharge\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"volatile mixture from dump\"}' https://localhost:6001/emissions/v1/auth/insert_data_byproducts curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"code\":\"methane\",\"region\":\"michigan\",\"value\":\"10000\"}' https://localhost:6001/emissions/v1/auth/get_data_byproducts Packaging Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"factor\":\"0.000942\",\"link\":\"replace_url_here\",\"code\":\"bleachedpaper\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a cardboard box is 0.94 kg CO2e / kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_packaging curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"code\":\"paper\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_packaging Consumption Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"factor\":\"0.00316\",\"link\":\"replace_url_here\",\"code\":\"bake\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"using standard equipment for cooking\"}' https://localhost:6001/emissions/v1/auth/insert_data_consumption curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"code\":\"roast\",\"region\":\"michigan\",\"value\":\"5\"}' https://localhost:6001/emissions/v1/auth/get_data_consumption Water Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"factor\":\"0.00000098\",\"link\":\"replace_url_here\",\"code\":\"filtered\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from life cycle energy used for processing, reactions and and distribution/packaging\"}' https://localhost:6001/emissions/v1/auth/insert_data_water curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"code\":\"brackish\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_water Plantation Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"factor\":\"0.060\",\"link\":\"replace_url_here\",\"code\":\"palmtrees\",\"region\":\"michigan\",\"unit\":\"hactre\",\"info\":\"redcution from 100 generic trees/hactre, 0.060 metric ton CO2 per urban tree planted\"}' https://localhost:6001/emissions/v1/auth/insert_data_plantation curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"code\":\"trees\",\"region\":\"michigan\",\"value\":\"100\"}' https://localhost:6001/emissions/v1/auth/get_data_plantation Chemicals Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"factor\":\"0.00168\",\"link\":\"replace_url_here\",\"code\":\"colorants\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"Imidazole + Trizole 3.90kg CO2 per kg, Organophosphate 3.70kg CO2 per kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_chemicals curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"code\":\"fungicide\",\"region\":\"michigan\",\"value\":\"100\"}' https://localhost:6001/emissions/v1/auth/get_data_chemicals Processes Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"factor\":\"0.00019\",\"link\":\"replace_url_here\",\"code\":\"compression\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"0.19 Kg CO2 eq per kWh for HVAC\"}' https://localhost:6001/emissions/v1/auth/insert_data_processes curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"code\":\"cooling\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_processes Bring down containers To bring down the container manually, docker stop emission.beefsupply.com && docker rm emission.beefsupply.com && docker volume rm emission.beefsupply.com Note: Enable the 'restart' option in docker-compose file with appropriate options {always, on-failure, unless-stopped} for persistence and make back up of volumes before removing them # Remove directories created during building and running app rm -R mainserver && rm -R __pycache__ # Remove volumes (make back up before doing so) docker volume rm emissions_emission.beefsupply.com && docker volume prune # remove all __pycache__ files in /emissions directory generated during the application runtime find . -type d -name \"__pycache__\" -exec rm -r {} + Multi-Host Environment Containers can be accessed from a different host using the (1) hosted ip address (2) domain name or the (3) container name. The container name is set to 'emission.beefsupply.com' so any other container directly connected to the network 'beef_supply' can access the cotainer directly using the link https://emission.beefsupply.com:6001/ If the containers are hosted permanently with a domain name, edit the /etc/hosts file to point to the domain name (more details can be found at: https://man7.org/linux/man-pages/man5/hosts.5.html). We restrict the tutorial to a service hosted on a machine with a reachable ip address. Once the ip-address of the machine where the service is being hosted in confirmed (e.g. 203.0.113.5 ), change it for the variable 'emissions_address' in the BeefMesh/environment/service_ip_address.sh file and source it in the terminal source environment/service_ip_addresses.sh All of the commands above will be used with the new address $emissions_address instead of localhost. Register Users curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@emissions.com\"}' https://$emissions_address:6001/emissions/v1/auth/register Login Users curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@emissions.com\",\"password\":\"example_password\"}' https://$emissions_address:6001/emissions/v1/auth/login Logout Users curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$emissions_address:6001/emissions/v1/auth/logout Refresh Tokens curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$emissions_address:6001/emissions/v1/auth/refresh Reset Password curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://$emissions_address:6001/emissions/v1/auth/password_reset Remove User curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@emissions.com\"}' https://$emissions_address:6001/emissions/v1/auth/remove_user Checking Permissions To check generic user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_user_permission To check admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_admin_permission To check super admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_sadmin_permission Testing Permissions Test routes for debugging generic user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_user Test routes for debugging admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_admin Test routes for debugging super admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_super_admin Generic Parameters To add generic parameters for emissions calculations curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"factor\":\"0.0017\",\"link\":\"https://shorturl.at/hiBCG\",\"code\":\"plastic\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_generic To get a view of all the available factors for generic category, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors To get the resultant value of total emissions against a particular amount of resource consumption, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"code\":\"electricity\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_generic Retrieving all Info To get all info of factors available related to different categories, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors surl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors Example calls to API for inserting factors into database or retreiving resultant emissions are summarized below`. Electricity Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"factor\":\"0.000433\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"92.7% efficiency\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_electricity curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"50000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_electricity Diesel Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"factor\":\"0.001219\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_diesel curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_diesel Gasoline Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"factor\":\"0.0010617\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_gasoline curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_gasoline Fossil Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"factor\":\"0.000904\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_fossil curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"4000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_fossil Naturalgas Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"factor\":\"0.00005301\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_naturalgas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_naturalgas Biogas Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"factor\":\"0.0000247\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"carbon reduction, (electricity*0.0571) assuming 1 cubic meterbiogas = 2kWh energy\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_biogas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_biogas Solar Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"factor\":\"0.00005\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"solar panels offset 50 grams of CO2 for every kilowatt-hour of power produced\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_solar curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_solar Windturbine Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"factor\":\"0.000006\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"offset 6 grams of CO2 for every kilowatt-hour of power produced\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_windturbine curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"2000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_windturbine Steam Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"factor\":\"0.000004\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"assuming 80% efficient boiler and converting used steam to natural gas, also assuming water is coming in at 60F (about 15C) with 28 btu\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_steam curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_steam Feed Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"factor\":\"0.000032\",\"link\":\"replace_url_here\",\"code\":\"organicgrass\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from 80% dry matter\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_feed curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"code\":\"alfalfa\",\"region\":\"michigan\",\"value\":\"10000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_feed Byproducts Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"factor\":\"0.000015\",\"link\":\"replace_url_here\",\"code\":\"discharge\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"volatile mixture from dump\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_byproducts curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"code\":\"methane\",\"region\":\"michigan\",\"value\":\"10000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_byproducts Packaging Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"factor\":\"0.000942\",\"link\":\"replace_url_here\",\"code\":\"bleachedpaper\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a cardboard box is 0.94 kg CO2e / kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_packaging curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"code\":\"paper\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_packaging Consumption Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"factor\":\"0.00316\",\"link\":\"replace_url_here\",\"code\":\"bake\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"using standard equipment for cooking\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_consumption curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"code\":\"roast\",\"region\":\"michigan\",\"value\":\"5\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_consumption Water Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"factor\":\"0.00000098\",\"link\":\"replace_url_here\",\"code\":\"filtered\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from life cycle energy used for processing, reactions and and distribution/packaging\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_water curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"code\":\"brackish\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_water Plantation Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"factor\":\"0.060\",\"link\":\"replace_url_here\",\"code\":\"palmtrees\",\"region\":\"michigan\",\"unit\":\"hactre\",\"info\":\"redcution from 100 generic trees/hactre, 0.060 metric ton CO2 per urban tree planted\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_plantation curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"code\":\"trees\",\"region\":\"michigan\",\"value\":\"100\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_plantation Chemicals Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"factor\":\"0.00168\",\"link\":\"replace_url_here\",\"code\":\"colorants\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"Imidazole + Trizole 3.90kg CO2 per kg, Organophosphate 3.70kg CO2 per kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_chemicals curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"code\":\"fungicide\",\"region\":\"michigan\",\"value\":\"100\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_chemicals Processes Factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"factor\":\"0.00019\",\"link\":\"replace_url_here\",\"code\":\"compression\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"0.19 Kg CO2 eq per kWh for HVAC\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_processes curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"code\":\"cooling\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_processes","title":"Emissions"},{"location":"emissions/#emissions","text":"This flask based submodule located under /emissions directroy is meant to serve as a common carbon emissions database maintained by a regulator. Emission parameters and factors are added to the database after neogtiation and voting on blockchain channels. Once a emissions factor is finalized, it is added to the database for users to consult from when calculating emissions at their end. For testing purposes as a standalone application from BeefMesh framework, create a virtual envrionment virtualenv emissions source emissions/bin/activate pip install -r requirements.txt python -m main Once up and running, use curl (https://curl.haxx.se/download.html) or httpie (https://httpie.io/) commands to interact with the API. For running the application for testing without docker may require setting credentials. These can be imported by \"$ source emissions/env/env.sh\" or by settting them as, export EMISSION_SUPERADMIN_PASSWORD=emissions export EMISSION_ADMIN_PASSWORD=emissions export EMISSION_TEST_USER_PASSWORD=emissions export EMISSION_GENERIC_USER_PASSWORD=emissions By default, the application initializes with the following credentials defined in /api/db_initializer/db_initializer.py: Super Admin Credentials: username: username_superadmin password: emissions email: email_superadmin@emissions.com Admin Credentials: username: username_admin password: emissions email: email_admin@emissions.com Test User Credentials: username: username_test password: emissions email: email_test@emissions.com Emissions User Credentials: username: username_emissions password: emissions email: email_emissions@emissions.com The database file is created and maintained under /api/db_files. In docker, the credentials are imported as secrets and kept under /emissions/secrets for testing purposes. For testing with docker as a standalone application, spin up the containerby navigating into the /emissions directory. docker-compose up -d Once up and running, different routes can be used to interact with the application. Some example routes are given below. Note: The curl commands below use 'localhost' in the url section. This would he replaced with the IP or URL mapping of the hosted application when accessing from an outside host. The port '6001' can be changed to whatever port is suitable for your system . The application consits of custom emission factors added for beef supply chain. Users can add their own paramters under generic category.","title":"Emissions"},{"location":"emissions/#register-users","text":"curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@emissions.com\"}' https://localhost:6001/emissions/v1/auth/register Note that we use -k flag to use dummy certificates generated from openssl application or with arbitrary certificate assignment by flask.","title":"Register Users"},{"location":"emissions/#login-users","text":"curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@emissions.com\",\"password\":\"example_password\"}' https://localhost:6001/emissions/v1/auth/login Logging in a user generates an Access Token and a Refresh Token that can be used to access resources on the server. An example of the response with tokens is given below. { \"access_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU0Mjg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20iLCJhZG1pbiI6MH0.DlJnfzALoul7CH_jh0_R6SpNF-2w5xzx7iu2ZR-AXh_ErFyfl_KMOLBLyZ8smX7gSe1sfvJzKHAgQPoqlUnR7w\", \"refresh_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU3ODg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20ifQ.ysZ8JDAj5SlPe0rbfr2DXGL0BPCxZ-TFvqRyvabcNYkyKJmWXP3WkJ8xryFtxIE12a04SR-VkFyiLLjGnJvVjw\" } The tokens will be passed whereever user is requied to be logged in on the server to access any resources.","title":"Login Users"},{"location":"emissions/#logout-users","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:6001/emissions/v1/auth/logout Note that the above command requires passing values for token variables.","title":"Logout Users"},{"location":"emissions/#refresh-tokens","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:6001/emissions/v1/auth/refresh","title":"Refresh Tokens"},{"location":"emissions/#reset-password","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://localhost:6001/emissions/v1/auth/password_reset","title":"Reset Password"},{"location":"emissions/#remove-user","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@emissions.com\"}' https://localhost:6001/emissions/v1/auth/remove_user Note that removing a user requires super admin permissions and the super admin needs to be logged in to be able to pass on their access tokens. These requirements can be changed under relevant classes which are classed against each route. These classes can be found in UserHandlers.py file under /api/handlers","title":"Remove User"},{"location":"emissions/#checking-permissions","text":"To check whether a user should be allowed to access any resource on the server, curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_user_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_admin_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://localhost:6001/beefchain/v1/auth/check_sadmin_permission The route will return a {\"status\": \"success\"} or {\"status\": \"failure\"} message depending upon whether the user is succesffuly logged in and using correct token. This can be used as a condition to allow access to resources.","title":"Checking Permissions"},{"location":"emissions/#testing-permissions","text":"Some test routes for debugging user permissions are provided below. curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_admin curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:6001/emissions/v1/auth/data_super_admin","title":"Testing Permissions"},{"location":"emissions/#generic-parameters","text":"To add generic parameters for emissions calculations, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"factor\":\"0.0017\",\"link\":\"https://shorturl.at/hiBCG\",\"code\":\"plastic\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_generic A reponse could look like { \"status\": \"successfully added parameters to generic data\" } In the above call to emissions API, 'unit' is used as a measure for resource consumption input against which emissions are calculated, for example 500 kg of platic use and factor is a floating value. 'Code' can be used to refer to category. 'link' is used to refer to the source, 'region' can be used to highlight the area, the factor is specific to. Further 'info' can also be added to make use of more parameters to fine tune emissions or to get a detaled view of underlying processes. For example, when calculating emissions from a 'heating' process, the boiler efficiency percentage, the water temperature, losses in tranfer of heating can be taken into account. To get a view of all the available factors for generic category, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors A reponse could look like [ { \"name\": \"generic\", \"id\": 1, \"region\": \"michigan\", \"unit\": \"kWh\", \"factor\": 0.000433, \"link\": \"https://www.epa.gov/energy/greenhouse-gases-equivalencies-calculator-calculations-and-references\", \"code\": \"electricity\", \"info\": \"92.7% efficiency over distribution lines\" }, { \"name\": \"generic\", \"id\": 2, \"region\": \"michigan\", \"unit\": \"kg\", \"factor\": 0.0017, \"link\": \"https://shorturl.at/hiBCG\", \"code\": \"plastic\", \"info\": \"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\" } ] To get the resultant value of total emissions against a particular amount of resource consumption, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"code\":\"electricity\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_generic Here, 'value' is used as a place holder for resource consumption, for example 1000 kWh. A reponse could look like one given below. Note that the parameter 'scale' refers to the fact that the resultant value of total emissions given in 'result' is in metric ton CO2 equivalent. { \"factor\": 0.000433, \"result\": 0.433, \"info\": \"92.7% efficiency over distribution lines\", \"unit\": \"kWh\", \"scale\": \"metric ton CO2-equivalent\" } Note that the resultant values are given in CO2 equivalent by converting the input value (e.g. CO2 into CO2-equivalent) assuming that the input value is the only form of emissions available. The factors are used from online resources and not all of them take into account all of the CO2-equivalent contributing gases. If other measures of contributing gases (such as methane) are also available, add their CO2-equivalent contribution (in metric ton) to the resultant values .","title":"Generic Parameters"},{"location":"emissions/#retrieving-all-info","text":"Note: The curl commands from here onwards have the 'Authorization' header part removed for testing purposes. Enable Authorization for the routes by including @auth.login_required and @role_required.permission(2) for the relevant classes called by routes in the file UserHandlers.py To get all info of factors available related to different categories, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors surl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\"}' https://localhost:6001/emissions/v1/auth/get_all_emission_factors Example calls to API for inserting factors into database or retreiving resultant emissions from available factors in different custom categories are summarized below .","title":"Retrieving all Info"},{"location":"emissions/#electricity-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"factor\":\"0.000433\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"92.7% efficiency\"}' https://localhost:6001/emissions/v1/auth/insert_data_electricity curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"50000\"}' https://localhost:6001/emissions/v1/auth/get_data_electricity","title":"Electricity Factors"},{"location":"emissions/#diesel-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"factor\":\"0.001219\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_diesel curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_diesel","title":"Diesel Factors"},{"location":"emissions/#gasoline-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"factor\":\"0.0010617\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_gasoline curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_gasoline","title":"Gasoline Factors"},{"location":"emissions/#fossil-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"factor\":\"0.000904\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_fossil curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"4000\"}' https://localhost:6001/emissions/v1/auth/get_data_fossil","title":"Fossil Factors"},{"location":"emissions/#naturalgas-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"factor\":\"0.00005301\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"none\"}' https://localhost:6001/emissions/v1/auth/insert_data_naturalgas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://localhost:6001/emissions/v1/auth/get_data_naturalgas","title":"Naturalgas Factors"},{"location":"emissions/#biogas-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"factor\":\"0.0000247\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"carbon reduction, (electricity*0.0571) assuming 1 cubic meterbiogas = 2kWh energy\"}' https://localhost:6001/emissions/v1/auth/insert_data_biogas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://localhost:6001/emissions/v1/auth/get_data_biogas","title":"Biogas Factors"},{"location":"emissions/#solar-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"factor\":\"0.00005\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"solar panels offset 50 grams of CO2 for every kilowatt-hour of power produced\"}' https://localhost:6001/emissions/v1/auth/insert_data_solar curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_solar","title":"Solar Factors"},{"location":"emissions/#windturbine-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"factor\":\"0.000006\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"offset 6 grams of CO2 for every kilowatt-hour of power produced\"}' https://localhost:6001/emissions/v1/auth/insert_data_windturbine curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"2000\"}' https://localhost:6001/emissions/v1/auth/get_data_windturbine","title":"Windturbine Factors"},{"location":"emissions/#steam-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"factor\":\"0.000004\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"assuming 80% efficient boiler and converting used steam to natural gas, also assuming water is coming in at 60F (about 15C) with 28 btu\"}' https://localhost:6001/emissions/v1/auth/insert_data_steam curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://localhost:6001/emissions/v1/auth/get_data_steam","title":"Steam Factors"},{"location":"emissions/#feed-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"factor\":\"0.000032\",\"link\":\"replace_url_here\",\"code\":\"organicgrass\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from 80% dry matter\"}' https://localhost:6001/emissions/v1/auth/insert_data_feed curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"code\":\"alfalfa\",\"region\":\"michigan\",\"value\":\"10000\"}' https://localhost:6001/emissions/v1/auth/get_data_feed","title":"Feed Factors"},{"location":"emissions/#byproducts-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"factor\":\"0.000015\",\"link\":\"replace_url_here\",\"code\":\"discharge\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"volatile mixture from dump\"}' https://localhost:6001/emissions/v1/auth/insert_data_byproducts curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"code\":\"methane\",\"region\":\"michigan\",\"value\":\"10000\"}' https://localhost:6001/emissions/v1/auth/get_data_byproducts","title":"Byproducts Factors"},{"location":"emissions/#packaging-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"factor\":\"0.000942\",\"link\":\"replace_url_here\",\"code\":\"bleachedpaper\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a cardboard box is 0.94 kg CO2e / kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_packaging curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"code\":\"paper\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_packaging","title":"Packaging Factors"},{"location":"emissions/#consumption-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"factor\":\"0.00316\",\"link\":\"replace_url_here\",\"code\":\"bake\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"using standard equipment for cooking\"}' https://localhost:6001/emissions/v1/auth/insert_data_consumption curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"code\":\"roast\",\"region\":\"michigan\",\"value\":\"5\"}' https://localhost:6001/emissions/v1/auth/get_data_consumption","title":"Consumption Factors"},{"location":"emissions/#water-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"factor\":\"0.00000098\",\"link\":\"replace_url_here\",\"code\":\"filtered\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from life cycle energy used for processing, reactions and and distribution/packaging\"}' https://localhost:6001/emissions/v1/auth/insert_data_water curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"code\":\"brackish\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_water","title":"Water Factors"},{"location":"emissions/#plantation-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"factor\":\"0.060\",\"link\":\"replace_url_here\",\"code\":\"palmtrees\",\"region\":\"michigan\",\"unit\":\"hactre\",\"info\":\"redcution from 100 generic trees/hactre, 0.060 metric ton CO2 per urban tree planted\"}' https://localhost:6001/emissions/v1/auth/insert_data_plantation curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"code\":\"trees\",\"region\":\"michigan\",\"value\":\"100\"}' https://localhost:6001/emissions/v1/auth/get_data_plantation","title":"Plantation Factors"},{"location":"emissions/#chemicals-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"factor\":\"0.00168\",\"link\":\"replace_url_here\",\"code\":\"colorants\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"Imidazole + Trizole 3.90kg CO2 per kg, Organophosphate 3.70kg CO2 per kg\"}' https://localhost:6001/emissions/v1/auth/insert_data_chemicals curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"code\":\"fungicide\",\"region\":\"michigan\",\"value\":\"100\"}' https://localhost:6001/emissions/v1/auth/get_data_chemicals","title":"Chemicals Factors"},{"location":"emissions/#processes-factors","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"factor\":\"0.00019\",\"link\":\"replace_url_here\",\"code\":\"compression\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"0.19 Kg CO2 eq per kWh for HVAC\"}' https://localhost:6001/emissions/v1/auth/insert_data_processes curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"code\":\"cooling\",\"region\":\"michigan\",\"value\":\"1000\"}' https://localhost:6001/emissions/v1/auth/get_data_processes","title":"Processes Factors"},{"location":"emissions/#bring-down-containers","text":"To bring down the container manually, docker stop emission.beefsupply.com && docker rm emission.beefsupply.com && docker volume rm emission.beefsupply.com Note: Enable the 'restart' option in docker-compose file with appropriate options {always, on-failure, unless-stopped} for persistence and make back up of volumes before removing them # Remove directories created during building and running app rm -R mainserver && rm -R __pycache__ # Remove volumes (make back up before doing so) docker volume rm emissions_emission.beefsupply.com && docker volume prune # remove all __pycache__ files in /emissions directory generated during the application runtime find . -type d -name \"__pycache__\" -exec rm -r {} +","title":"Bring down containers"},{"location":"emissions/#multi-host-environment","text":"Containers can be accessed from a different host using the (1) hosted ip address (2) domain name or the (3) container name. The container name is set to 'emission.beefsupply.com' so any other container directly connected to the network 'beef_supply' can access the cotainer directly using the link https://emission.beefsupply.com:6001/ If the containers are hosted permanently with a domain name, edit the /etc/hosts file to point to the domain name (more details can be found at: https://man7.org/linux/man-pages/man5/hosts.5.html). We restrict the tutorial to a service hosted on a machine with a reachable ip address. Once the ip-address of the machine where the service is being hosted in confirmed (e.g. 203.0.113.5 ), change it for the variable 'emissions_address' in the BeefMesh/environment/service_ip_address.sh file and source it in the terminal source environment/service_ip_addresses.sh All of the commands above will be used with the new address $emissions_address instead of localhost.","title":"Multi-Host Environment"},{"location":"emissions/#register-users_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@emissions.com\"}' https://$emissions_address:6001/emissions/v1/auth/register","title":"Register Users"},{"location":"emissions/#login-users_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@emissions.com\",\"password\":\"example_password\"}' https://$emissions_address:6001/emissions/v1/auth/login","title":"Login Users"},{"location":"emissions/#logout-users_1","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$emissions_address:6001/emissions/v1/auth/logout","title":"Logout Users"},{"location":"emissions/#refresh-tokens_1","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$emissions_address:6001/emissions/v1/auth/refresh","title":"Refresh Tokens"},{"location":"emissions/#reset-password_1","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://$emissions_address:6001/emissions/v1/auth/password_reset","title":"Reset Password"},{"location":"emissions/#remove-user_1","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@emissions.com\"}' https://$emissions_address:6001/emissions/v1/auth/remove_user","title":"Remove User"},{"location":"emissions/#checking-permissions_1","text":"To check generic user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_user_permission To check admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_admin_permission To check super admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@emissions.com\",\"password\":\"emissions\"}' https://$emissions_address:6001/beefchain/v1/auth/check_sadmin_permission","title":"Checking Permissions"},{"location":"emissions/#testing-permissions_1","text":"Test routes for debugging generic user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_user Test routes for debugging admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_admin Test routes for debugging super admin user permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$emissions_address:6001/emissions/v1/auth/data_super_admin","title":"Testing Permissions"},{"location":"emissions/#generic-parameters_1","text":"To add generic parameters for emissions calculations curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"factor\":\"0.0017\",\"link\":\"https://shorturl.at/hiBCG\",\"code\":\"plastic\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a plastic use is 0.94 kg CO2e/kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_generic To get a view of all the available factors for generic category, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors To get the resultant value of total emissions against a particular amount of resource consumption, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\",\"code\":\"electricity\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_generic","title":"Generic Parameters"},{"location":"emissions/#retrieving-all-info_1","text":"To get all info of factors available related to different categories, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"generic\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors surl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\"}' https://$emissions_address:6001/emissions/v1/auth/get_all_emission_factors Example calls to API for inserting factors into database or retreiving resultant emissions are summarized below`.","title":"Retrieving all Info"},{"location":"emissions/#electricity-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"factor\":\"0.000433\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"92.7% efficiency\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_electricity curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"electricity\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"50000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_electricity","title":"Electricity Factors"},{"location":"emissions/#diesel-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"factor\":\"0.001219\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_diesel curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"diesel\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_diesel","title":"Diesel Factors"},{"location":"emissions/#gasoline-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"factor\":\"0.0010617\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_gasoline curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"gasoline\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_gasoline","title":"Gasoline Factors"},{"location":"emissions/#fossil-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"factor\":\"0.000904\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_fossil curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"fossil\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"4000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_fossil","title":"Fossil Factors"},{"location":"emissions/#naturalgas-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"factor\":\"0.00005301\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"none\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_naturalgas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"naturalgas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_naturalgas","title":"Naturalgas Factors"},{"location":"emissions/#biogas-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"factor\":\"0.0000247\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"feet^3\",\"info\":\"carbon reduction, (electricity*0.0571) assuming 1 cubic meterbiogas = 2kWh energy\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_biogas curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"biogas\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"5000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_biogas","title":"Biogas Factors"},{"location":"emissions/#solar-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"factor\":\"0.00005\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"solar panels offset 50 grams of CO2 for every kilowatt-hour of power produced\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_solar curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"solar\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_solar","title":"Solar Factors"},{"location":"emissions/#windturbine-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"factor\":\"0.000006\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"offset 6 grams of CO2 for every kilowatt-hour of power produced\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_windturbine curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"windturbine\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"2000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_windturbine","title":"Windturbine Factors"},{"location":"emissions/#steam-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"factor\":\"0.000004\",\"link\":\"replace_url_here\",\"code\":\"v02\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"assuming 80% efficient boiler and converting used steam to natural gas, also assuming water is coming in at 60F (about 15C) with 28 btu\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_steam curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"steam\",\"code\":\"v01\",\"region\":\"michigan\",\"value\":\"100000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_steam","title":"Steam Factors"},{"location":"emissions/#feed-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"factor\":\"0.000032\",\"link\":\"replace_url_here\",\"code\":\"organicgrass\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from 80% dry matter\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_feed curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"feed\",\"code\":\"alfalfa\",\"region\":\"michigan\",\"value\":\"10000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_feed","title":"Feed Factors"},{"location":"emissions/#byproducts-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"factor\":\"0.000015\",\"link\":\"replace_url_here\",\"code\":\"discharge\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"volatile mixture from dump\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_byproducts curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"byproducts\",\"code\":\"methane\",\"region\":\"michigan\",\"value\":\"10000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_byproducts","title":"Byproducts Factors"},{"location":"emissions/#packaging-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"factor\":\"0.000942\",\"link\":\"replace_url_here\",\"code\":\"bleachedpaper\",\"region\":\"michigan\",\"unit\":\"kg\",\"info\":\"The cradle to grave carbon footprint of a cardboard box is 0.94 kg CO2e / kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_packaging curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"packaging\",\"code\":\"paper\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_packaging","title":"Packaging Factors"},{"location":"emissions/#consumption-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"factor\":\"0.00316\",\"link\":\"replace_url_here\",\"code\":\"bake\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"using standard equipment for cooking\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_consumption curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"consumption\",\"code\":\"roast\",\"region\":\"michigan\",\"value\":\"5\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_consumption","title":"Consumption Factors"},{"location":"emissions/#water-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"factor\":\"0.00000098\",\"link\":\"replace_url_here\",\"code\":\"filtered\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"from life cycle energy used for processing, reactions and and distribution/packaging\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_water curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"water\",\"code\":\"brackish\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_water","title":"Water Factors"},{"location":"emissions/#plantation-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"factor\":\"0.060\",\"link\":\"replace_url_here\",\"code\":\"palmtrees\",\"region\":\"michigan\",\"unit\":\"hactre\",\"info\":\"redcution from 100 generic trees/hactre, 0.060 metric ton CO2 per urban tree planted\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_plantation curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"plantation\",\"code\":\"trees\",\"region\":\"michigan\",\"value\":\"100\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_plantation","title":"Plantation Factors"},{"location":"emissions/#chemicals-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"factor\":\"0.00168\",\"link\":\"replace_url_here\",\"code\":\"colorants\",\"region\":\"michigan\",\"unit\":\"lb\",\"info\":\"Imidazole + Trizole 3.90kg CO2 per kg, Organophosphate 3.70kg CO2 per kg\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_chemicals curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"chemicals\",\"code\":\"fungicide\",\"region\":\"michigan\",\"value\":\"100\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_chemicals","title":"Chemicals Factors"},{"location":"emissions/#processes-factors_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"factor\":\"0.00019\",\"link\":\"replace_url_here\",\"code\":\"compression\",\"region\":\"michigan\",\"unit\":\"kWh\",\"info\":\"0.19 Kg CO2 eq per kWh for HVAC\"}' https://$emissions_address:6001/emissions/v1/auth/insert_data_processes curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"processes\",\"code\":\"cooling\",\"region\":\"michigan\",\"value\":\"1000\"}' https://$emissions_address:6001/emissions/v1/auth/get_data_processes","title":"Processes Factors"},{"location":"examples/","text":"Examples and Applications The microservices provided can be used be used within different collaboration groups to run a number of useful applications including traceability, recording and sharing useful regulatory parameters, managing federated data, using machine learning applications with secure data pipelines, negotiating policies, making collaborative decisions, measuring and optimizing chain emissions and so much more. Though running full applications require lengthy scripts that start all of the microservices and perform detailed tasks, a few sample scripts are provided in the repository to help building an idea on how to use data once it is collected and available using the running services. For example, once data from underlying resoures using IoTs is collected and transmitted over secure blockchain channels for optimization and decision making at a federated node, linear optimization algorithms can be run over it to decide on the paths that organzations can use for routing certain commodity (beef in our case). The python script under examples/carbonOptimization/ExampleOptimizationMainServer.py can be used as a test case to get an idea on how the supply chain matrix would look like including the decison variables. An accompanying continerized application is provided under examples/containerized to run the optimization decisions serving other connected services. With secure blockchain data pipelines, machine learning models can be shared and optimized in coordination with federated databases maintained at different organizational domains. For example, examples/ProcessorFederated provides scripts with containerized nodes to process recognition of beef images at different processors and gradually improve it over time by coordinating with a federated node that globally combines the models and redistributes it over blockchain channels for keeping track of improvement over time and for cold storage of files. The containerized applications can be started in detached form by spinning docker-compose files, docker-compose up -d Different routes that can be called to start sharing machine learning model files are in the app.py file that gets imported into dockerfile during container startup. To manually bring down the application, docker stop processor1-app docker stop processor2-app docker stop processor3-app docker stop processor-agg-app docker stop processor-server-app docker rm processor1-app docker rm processor2-app docker rm processor3-app docker rm processor-agg-app docker rm processor-server-app docker volume prune docker system prune","title":"Examples and Applications"},{"location":"examples/#examples-and-applications","text":"The microservices provided can be used be used within different collaboration groups to run a number of useful applications including traceability, recording and sharing useful regulatory parameters, managing federated data, using machine learning applications with secure data pipelines, negotiating policies, making collaborative decisions, measuring and optimizing chain emissions and so much more. Though running full applications require lengthy scripts that start all of the microservices and perform detailed tasks, a few sample scripts are provided in the repository to help building an idea on how to use data once it is collected and available using the running services. For example, once data from underlying resoures using IoTs is collected and transmitted over secure blockchain channels for optimization and decision making at a federated node, linear optimization algorithms can be run over it to decide on the paths that organzations can use for routing certain commodity (beef in our case). The python script under examples/carbonOptimization/ExampleOptimizationMainServer.py can be used as a test case to get an idea on how the supply chain matrix would look like including the decison variables. An accompanying continerized application is provided under examples/containerized to run the optimization decisions serving other connected services. With secure blockchain data pipelines, machine learning models can be shared and optimized in coordination with federated databases maintained at different organizational domains. For example, examples/ProcessorFederated provides scripts with containerized nodes to process recognition of beef images at different processors and gradually improve it over time by coordinating with a federated node that globally combines the models and redistributes it over blockchain channels for keeping track of improvement over time and for cold storage of files. The containerized applications can be started in detached form by spinning docker-compose files, docker-compose up -d Different routes that can be called to start sharing machine learning model files are in the app.py file that gets imported into dockerfile during container startup. To manually bring down the application, docker stop processor1-app docker stop processor2-app docker stop processor3-app docker stop processor-agg-app docker stop processor-server-app docker rm processor1-app docker rm processor2-app docker rm processor3-app docker rm processor-agg-app docker rm processor-server-app docker volume prune docker system prune","title":"Examples and Applications"},{"location":"iots/","text":"IoT Network \"IoT network\" refers to a network infrastructure specifically designed to facilitate communication and data exchange among Internet of Things (IoT) devices. The submodule /iotnetwork directory is meant to serve as a resource for spinning up interfaces for IoTs and sensors. Environment Variables IoT network can be run for individual organizations and is meant to consume data within an independent private supply chain domain which can then be shared over other channels. The same setup can therefore be run within other organizations without major changes. Before the iot network can be run, check .env parameters to see if the ports are what is required at the organziation. gedit iotnetwork/docker/.env If the .env file is empty, copy environment parameters from iotnetwork/env/env.txt into iotnetwork/docker/.env file with required modifications. The framework picks up TLS certificates from iotnetwork/docker/ssl folder during startup. To generate your own certificates, see the certificate generation section. The underlying IoT network architecture ( https://mainflux.readthedocs.io/en/latest/) incoprorates there important entities namely User, Things and Channels. Users are individuals or entities who interact with the Mainflux platform. Things represent physical or virtual IoT devices, sensors, actuators, or any other entities that generate or consume data within the IoT ecosystem. Channels serve as communication conduits within the Mainflux platform, enabling data exchange between things and other components of the IoT system. Each entity can be mapped to other with one-to-one, one-to-many or with other multi connectivity associations. Running Application To run a standalone iotnetwork application (with a number of services) within an organizational domain, run the docker-compose file within iotnetwork/docker/ directory # Run from within the /iotnetwork directory docker-compose -f docker/docker-compose.yml up -d This will spin up a number of containers including databases that will be attached to 'beefchain-base-net' docker bridge network. All default databases have their usernames, passwords and database names set to 'mainflux' which can be changed in the .env file before spinning containers. The most useful environment parameter to begin with is the user email 'admin@example.com' and password 'mainflux' for https://localhost/users route. By default, 'users' service starts with the ability to register any user to begin using the platform. Should only an admin be allowed to access 'users' service, change variable MF_USERS_ALLOW_SELF_REGISTER in the .env file to false. Run Databases We also need to spin up databases to store data captured from IoTs and sensors. There are a number of options available for that purpose under iotnetwork/docker/addons folder. As an example, spin up mongodb databases. For starting any addon service .env parameters file should be copied to that folder. Call the iotnetwork/iot-network.sh script to copy .env files where required. From within iotnetwork directory use any of the the commands below in bash terminal, # to copy .env files to iotnetwork/docker/addons/mongodb-writer and */mongodb-reader ./iot-network.sh copyenv mongodb # to copy .env file to all foders in iotnetwork/docker/addons/* ./iot-network.sh copyenv all # to copy .env file to individual folders ./iot-network.sh copyenv mongodb ./iot-network.sh copyenv postgres ./iot-network.sh copyenv influxdb ./iot-network.sh copyenv timescale ./iot-network.sh copyenv twins ./iot-network.sh copyenv notifier ./iot-network.sh copyenv adapter ./iot-network.sh copyenv cassandra Once, .env files have been successfully copied to /mongodb-writer and /mongodb-reader in iotnetwork/docker/addons/ folder, spin up the containers (assuming user is in the main BeefChain directory) Note that any of the addon services require main services to be up and running as it looks for external 'beefchain-base-net network to connect to. We already started it using iotnetwork/docker/docker-compose.yml file! docker-compose -f docker/addons/mongodb-writer/docker-compose.yml -f docker/addons/mongodb-reader/docker-compose.yml up -d Create Users Once the cotnainers are up and running successfully, we can start creating users, things and channels. Note that all these there entities get assigned KEYS and IDs over the iot framework for accessing them. These are stored as environment variables in iotnetwork/env directory under /users, /things and /channels folder. Creat a new user with email 'farmer@beefchain.com' and password '12345678' by calling the ./iot-network.sh script in iotnetwork/ folder user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network.sh newuser $user_email $user_pass # using direct parameters ./iot-network.sh newuser farmer@beefchain.com 12345678 A user will be created if it doesn't exist and its ID will be stored with varaible name 'user_id' in iotnetwork/env/users/$user_email.sh. At any time it can be viewed in the bash terminal by ./iot-network.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh Signin Users Once user has been registered, sign in the paltform to get session tokens (JWT Tokens) that can be used to create things and channels. Call the provided script with email and password to sign in. ./iot-network.sh signin farmer@beefchain.com 12345678 A user file will be created if it doesn't exist and its assigned token will be stored with varaible name 'user_token' in iotnetwork/env/tokens/$user_email.sh. At any time it can be viewed in the bash terminal by ./iot-network.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh Tokens can get expired after sometime. To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network.sh checktoken farmer@beefchain.com 12345678 This will pick up session tokens against the provided user credential and check and return its validity. To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network.sh allusers admin@example.com 12345678 Create Sensors A registered user can create Things (sensor devices) on the platform which can be a direct interace for hardware devices to dump data. To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network.sh createsensor farmer@beefchain.com 12345678 energysensor This will register a sensor device interface with a name 'energysensor', generate a key and an id for the device. The key is stored in iotnetwork/env/keys folder with the sensor name and with variable name sensor_key. The sensor id is stored in the iotnetwork/env/things folder in a file named after the sensor name in a variable sensor_id along with the user_id that created it. To source directly the variable to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network.sh getthings farmer@beefchain.com 12345678 Once sensor devices as 'things' have been registered on the platform, communication channels need to be created to allow data to be transmitted from devices over the channels and stored in relevant databases. To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network.sh createchannel farmer@beefchain.com 12345678 energysensorchannel This will register a channel interface with a name 'energysensorchannel', and generate an id for the channel. The id is stored in iotnetwork/env/channels folder with the channel name and with variable name channel_id along with the user_id that created it. To source directly the variables to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network.sh getchannels farmer@beefchain.com 12345678 Once, sensor devices (things) and channels have been registered on the platform by an authorized user, sensors need to be connected to channels to allow sending data over it. To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel Sensors can also be disconnected from channels. To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel Transmit Data Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel Since we had already started the mongodb reader and writer containers earlier, we will use the mongodb database to dump sensor data and retrieve it. Check mongodb containers are running docker ps | grep mainflux-mongodb-reader docker ps | grep mainflux-mongodb-writer Note: Containers for other databases in the iotnetwork/addons can be started in a similar way by first copying .env variables and then calling docker-compose files! To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The above curl call picks up sensor key and channel id along with exporting row parameters from the energy.csv file located in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://localhost/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]' Read Data To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network.sh getdata <database_type> <user_email> <user_pass> <output_file_name> Bringdown Containers To bring down containers, call the cleanall function Note that cleanall deletes stored data and removes containers completeley! ./iot-network.sh cleanall Create Certificates To allow communicating with IoT network using secure Mutual TLS authentication, generate certificates of your own. From within iotnetwork/docker/ssl directory run # Generate certificate authority certificates make ca CN=localhost O=Beefchain OU=beefchain emailAddress=info@beefchain.com # Generate iotnetwork server certificates make server_cert CN=localhost O=Beefchain OU=beefchain emailAddress=info@beefchain.com # (Extra) Generate things certificate (Note that the THING_KEY varaible should be the actual key described earlier) make thing_cert THING_KEY=8f65ed04-0770-4ce4-a291-6d1bf2000f4d CRT_FILE_NAME=thing O=Beefchain OU=beefchain emailAddress=info@beefchain.com This should generate certificates in the iotnetwork/docker/ssl folder used by containers later. To use a user interface based system along with generating digital twins, spin up the docker-compose files in the iotnetwork/ui folder in a similar way after copying over the certificate files from iotnetwork/docker/ssl folder into the iotnetwork/ui/docker/ssl folder or generating new ones. Also copy the .env files before starting containers and make changes where required. Once the ui based containerized application is started, the iot ui can be accessed at http://localhost:3000/explorer/ with the default username admin@example.com and default password 12345678. The grafana setup for iot sensor data visualization can be accessed at http://localhost:3001 with a default username 'admin' and default password 'admin'. The username and password for grafana can be changed in the grafana-defaults.ini in configs folder or a new username and password can be created at the interface after the application runs. Remove configureadmin method from file ./iot-network-unsecure.sh if client using the file is generic user Multihost Unsecured To call the main server from other hosts without using certificates, first configure and import the url address of the deployed server in the variable 'iot_server_address' source environment/service_ip_addresses.sh Use the script ./iot-network-unsecure.sh to make calls to the main server from a host that can reach the server over ip Create User user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network-unsecure.sh newuser $user_email $user_pass # using direct parameters ./iot-network-unsecure.sh newuser farmer@beefchain.com 12345678 The id of created user can be viewed in the bash terminal by ./iot-network-unsecure.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh Signin Users Once user has been registered, sign in the platform to get session tokens (JWT Tokens) that can be used to create things and channels. ./iot-network-unsecure.sh signin farmer@beefchain.com 12345678 At any time user tokens can be viewed in the bash terminal by ./iot-network-unsecure.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network-unsecure.sh checktoken farmer@beefchain.com 12345678 To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network-unsecure.sh allusers admin@example.com 12345678 Create Sensors To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network-unsecure.sh createsensor farmer@beefchain.com 12345678 energysensor To source directly the sensor related id and key variables to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network-unsecure.sh getthings farmer@beefchain.com 12345678 To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network-unsecure.sh createchannel farmer@beefchain.com 12345678 energysensorchannel To source directly the channel id variable to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network-unsecure.sh getchannels farmer@beefchain.com 12345678 To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network-unsecure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network-unsecure.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network-unsecure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network-unsecure.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The file energy.csv file needs to be in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://$iot_server_address/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]' To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network-unsecure.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network-unsecure.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh getdata <database_type> <user_email> <user_pass> <output_file_name> Remove configureadmin method from file ./iot-network-secure.sh if client using the file is generic user Multihost Secured To use the multihost setup using secured connections, generate the certificates by modifying the file iotnetwork/docker/ssl/Makefile. On line 7, change variable value from 'localhost' to the actual host url, e.g. '203.0.113.45'. Then generate certificate and make sure the file iotnetwork/docker/ssl/certs/ca.crt is availabe at the client host. Configure and import the url address of the deployed server in the variable 'iot_server_address' source environment/service_ip_addresses.sh Use the script ./iot-network-secure.sh to make calls to the main server from a host that can reach the server over ip Create User user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network-secure.sh newuser $user_email $user_pass # using direct parameters ./iot-network-secure.sh newuser farmer@beefchain.com 12345678 The id of created user can be viewed in the bash terminal by ./iot-network-secure.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh Signin Users Once user has been registered, sign in the paltform to get session tokens (JWT Tokens) that can be used to create things and channels. ./iot-network-secure.sh signin farmer@beefchain.com 12345678 At any time user tokens can be viewed in the bash terminal by ./iot-network-secure.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network-secure.sh checktoken farmer@beefchain.com 12345678 To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network-secure.sh allusers admin@example.com 12345678 Create Sensors To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network-secure.sh createsensor farmer@beefchain.com 12345678 energysensor To source directly the sensor related id and key variables to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network-secure.sh getthings farmer@beefchain.com 12345678 To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network-secure.sh createchannel farmer@beefchain.com 12345678 energysensorchannel To source directly the channel id variable to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network-secure.sh getchannels farmer@beefchain.com 12345678 To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network-secure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network-secure.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network-secure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network-secure.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network-secure.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The file energy.csv file needs to be in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://$iot_server_address/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]' To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network-secure.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network-secure.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network-secure.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network-secure.sh getdata <database_type> <user_email> <user_pass> <output_file_name>","title":"IoT Network"},{"location":"iots/#iot-network","text":"\"IoT network\" refers to a network infrastructure specifically designed to facilitate communication and data exchange among Internet of Things (IoT) devices. The submodule /iotnetwork directory is meant to serve as a resource for spinning up interfaces for IoTs and sensors.","title":"IoT Network"},{"location":"iots/#environment-variables","text":"IoT network can be run for individual organizations and is meant to consume data within an independent private supply chain domain which can then be shared over other channels. The same setup can therefore be run within other organizations without major changes. Before the iot network can be run, check .env parameters to see if the ports are what is required at the organziation. gedit iotnetwork/docker/.env If the .env file is empty, copy environment parameters from iotnetwork/env/env.txt into iotnetwork/docker/.env file with required modifications. The framework picks up TLS certificates from iotnetwork/docker/ssl folder during startup. To generate your own certificates, see the certificate generation section. The underlying IoT network architecture ( https://mainflux.readthedocs.io/en/latest/) incoprorates there important entities namely User, Things and Channels. Users are individuals or entities who interact with the Mainflux platform. Things represent physical or virtual IoT devices, sensors, actuators, or any other entities that generate or consume data within the IoT ecosystem. Channels serve as communication conduits within the Mainflux platform, enabling data exchange between things and other components of the IoT system. Each entity can be mapped to other with one-to-one, one-to-many or with other multi connectivity associations.","title":"Environment Variables"},{"location":"iots/#running-application","text":"To run a standalone iotnetwork application (with a number of services) within an organizational domain, run the docker-compose file within iotnetwork/docker/ directory # Run from within the /iotnetwork directory docker-compose -f docker/docker-compose.yml up -d This will spin up a number of containers including databases that will be attached to 'beefchain-base-net' docker bridge network. All default databases have their usernames, passwords and database names set to 'mainflux' which can be changed in the .env file before spinning containers. The most useful environment parameter to begin with is the user email 'admin@example.com' and password 'mainflux' for https://localhost/users route. By default, 'users' service starts with the ability to register any user to begin using the platform. Should only an admin be allowed to access 'users' service, change variable MF_USERS_ALLOW_SELF_REGISTER in the .env file to false.","title":"Running Application"},{"location":"iots/#run-databases","text":"We also need to spin up databases to store data captured from IoTs and sensors. There are a number of options available for that purpose under iotnetwork/docker/addons folder. As an example, spin up mongodb databases. For starting any addon service .env parameters file should be copied to that folder. Call the iotnetwork/iot-network.sh script to copy .env files where required. From within iotnetwork directory use any of the the commands below in bash terminal, # to copy .env files to iotnetwork/docker/addons/mongodb-writer and */mongodb-reader ./iot-network.sh copyenv mongodb # to copy .env file to all foders in iotnetwork/docker/addons/* ./iot-network.sh copyenv all # to copy .env file to individual folders ./iot-network.sh copyenv mongodb ./iot-network.sh copyenv postgres ./iot-network.sh copyenv influxdb ./iot-network.sh copyenv timescale ./iot-network.sh copyenv twins ./iot-network.sh copyenv notifier ./iot-network.sh copyenv adapter ./iot-network.sh copyenv cassandra Once, .env files have been successfully copied to /mongodb-writer and /mongodb-reader in iotnetwork/docker/addons/ folder, spin up the containers (assuming user is in the main BeefChain directory) Note that any of the addon services require main services to be up and running as it looks for external 'beefchain-base-net network to connect to. We already started it using iotnetwork/docker/docker-compose.yml file! docker-compose -f docker/addons/mongodb-writer/docker-compose.yml -f docker/addons/mongodb-reader/docker-compose.yml up -d","title":"Run Databases"},{"location":"iots/#create-users","text":"Once the cotnainers are up and running successfully, we can start creating users, things and channels. Note that all these there entities get assigned KEYS and IDs over the iot framework for accessing them. These are stored as environment variables in iotnetwork/env directory under /users, /things and /channels folder. Creat a new user with email 'farmer@beefchain.com' and password '12345678' by calling the ./iot-network.sh script in iotnetwork/ folder user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network.sh newuser $user_email $user_pass # using direct parameters ./iot-network.sh newuser farmer@beefchain.com 12345678 A user will be created if it doesn't exist and its ID will be stored with varaible name 'user_id' in iotnetwork/env/users/$user_email.sh. At any time it can be viewed in the bash terminal by ./iot-network.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh","title":"Create Users"},{"location":"iots/#signin-users","text":"Once user has been registered, sign in the paltform to get session tokens (JWT Tokens) that can be used to create things and channels. Call the provided script with email and password to sign in. ./iot-network.sh signin farmer@beefchain.com 12345678 A user file will be created if it doesn't exist and its assigned token will be stored with varaible name 'user_token' in iotnetwork/env/tokens/$user_email.sh. At any time it can be viewed in the bash terminal by ./iot-network.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh Tokens can get expired after sometime. To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network.sh checktoken farmer@beefchain.com 12345678 This will pick up session tokens against the provided user credential and check and return its validity. To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network.sh allusers admin@example.com 12345678","title":"Signin Users"},{"location":"iots/#create-sensors","text":"A registered user can create Things (sensor devices) on the platform which can be a direct interace for hardware devices to dump data. To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network.sh createsensor farmer@beefchain.com 12345678 energysensor This will register a sensor device interface with a name 'energysensor', generate a key and an id for the device. The key is stored in iotnetwork/env/keys folder with the sensor name and with variable name sensor_key. The sensor id is stored in the iotnetwork/env/things folder in a file named after the sensor name in a variable sensor_id along with the user_id that created it. To source directly the variable to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network.sh getthings farmer@beefchain.com 12345678 Once sensor devices as 'things' have been registered on the platform, communication channels need to be created to allow data to be transmitted from devices over the channels and stored in relevant databases. To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network.sh createchannel farmer@beefchain.com 12345678 energysensorchannel This will register a channel interface with a name 'energysensorchannel', and generate an id for the channel. The id is stored in iotnetwork/env/channels folder with the channel name and with variable name channel_id along with the user_id that created it. To source directly the variables to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network.sh getchannels farmer@beefchain.com 12345678 Once, sensor devices (things) and channels have been registered on the platform by an authorized user, sensors need to be connected to channels to allow sending data over it. To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel Sensors can also be disconnected from channels. To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel","title":"Create Sensors"},{"location":"iots/#transmit-data","text":"Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel Since we had already started the mongodb reader and writer containers earlier, we will use the mongodb database to dump sensor data and retrieve it. Check mongodb containers are running docker ps | grep mainflux-mongodb-reader docker ps | grep mainflux-mongodb-writer Note: Containers for other databases in the iotnetwork/addons can be started in a similar way by first copying .env variables and then calling docker-compose files! To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The above curl call picks up sensor key and channel id along with exporting row parameters from the energy.csv file located in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://localhost/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]'","title":"Transmit Data"},{"location":"iots/#read-data","text":"To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network.sh getdata <database_type> <user_email> <user_pass> <output_file_name>","title":"Read Data"},{"location":"iots/#bringdown-containers","text":"To bring down containers, call the cleanall function Note that cleanall deletes stored data and removes containers completeley! ./iot-network.sh cleanall","title":"Bringdown Containers"},{"location":"iots/#create-certificates","text":"To allow communicating with IoT network using secure Mutual TLS authentication, generate certificates of your own. From within iotnetwork/docker/ssl directory run # Generate certificate authority certificates make ca CN=localhost O=Beefchain OU=beefchain emailAddress=info@beefchain.com # Generate iotnetwork server certificates make server_cert CN=localhost O=Beefchain OU=beefchain emailAddress=info@beefchain.com # (Extra) Generate things certificate (Note that the THING_KEY varaible should be the actual key described earlier) make thing_cert THING_KEY=8f65ed04-0770-4ce4-a291-6d1bf2000f4d CRT_FILE_NAME=thing O=Beefchain OU=beefchain emailAddress=info@beefchain.com This should generate certificates in the iotnetwork/docker/ssl folder used by containers later. To use a user interface based system along with generating digital twins, spin up the docker-compose files in the iotnetwork/ui folder in a similar way after copying over the certificate files from iotnetwork/docker/ssl folder into the iotnetwork/ui/docker/ssl folder or generating new ones. Also copy the .env files before starting containers and make changes where required. Once the ui based containerized application is started, the iot ui can be accessed at http://localhost:3000/explorer/ with the default username admin@example.com and default password 12345678. The grafana setup for iot sensor data visualization can be accessed at http://localhost:3001 with a default username 'admin' and default password 'admin'. The username and password for grafana can be changed in the grafana-defaults.ini in configs folder or a new username and password can be created at the interface after the application runs. Remove configureadmin method from file ./iot-network-unsecure.sh if client using the file is generic user","title":"Create Certificates"},{"location":"iots/#multihost-unsecured","text":"To call the main server from other hosts without using certificates, first configure and import the url address of the deployed server in the variable 'iot_server_address' source environment/service_ip_addresses.sh Use the script ./iot-network-unsecure.sh to make calls to the main server from a host that can reach the server over ip","title":"Multihost Unsecured"},{"location":"iots/#create-user","text":"user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network-unsecure.sh newuser $user_email $user_pass # using direct parameters ./iot-network-unsecure.sh newuser farmer@beefchain.com 12345678 The id of created user can be viewed in the bash terminal by ./iot-network-unsecure.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh","title":"Create User"},{"location":"iots/#signin-users_1","text":"Once user has been registered, sign in the platform to get session tokens (JWT Tokens) that can be used to create things and channels. ./iot-network-unsecure.sh signin farmer@beefchain.com 12345678 At any time user tokens can be viewed in the bash terminal by ./iot-network-unsecure.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network-unsecure.sh checktoken farmer@beefchain.com 12345678 To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network-unsecure.sh allusers admin@example.com 12345678","title":"Signin Users"},{"location":"iots/#create-sensors_1","text":"To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network-unsecure.sh createsensor farmer@beefchain.com 12345678 energysensor To source directly the sensor related id and key variables to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network-unsecure.sh getthings farmer@beefchain.com 12345678 To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network-unsecure.sh createchannel farmer@beefchain.com 12345678 energysensorchannel To source directly the channel id variable to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network-unsecure.sh getchannels farmer@beefchain.com 12345678 To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network-unsecure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network-unsecure.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network-unsecure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network-unsecure.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The file energy.csv file needs to be in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://$iot_server_address/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]' To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network-unsecure.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network-unsecure.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network-unsecure.sh getdata <database_type> <user_email> <user_pass> <output_file_name> Remove configureadmin method from file ./iot-network-secure.sh if client using the file is generic user","title":"Create Sensors"},{"location":"iots/#multihost-secured","text":"To use the multihost setup using secured connections, generate the certificates by modifying the file iotnetwork/docker/ssl/Makefile. On line 7, change variable value from 'localhost' to the actual host url, e.g. '203.0.113.45'. Then generate certificate and make sure the file iotnetwork/docker/ssl/certs/ca.crt is availabe at the client host. Configure and import the url address of the deployed server in the variable 'iot_server_address' source environment/service_ip_addresses.sh Use the script ./iot-network-secure.sh to make calls to the main server from a host that can reach the server over ip","title":"Multihost Secured"},{"location":"iots/#create-user_1","text":"user_email='farmer@beefchain.com' user_pass='12345678' # using bash variables ./iot-network-secure.sh newuser $user_email $user_pass # using direct parameters ./iot-network-secure.sh newuser farmer@beefchain.com 12345678 The id of created user can be viewed in the bash terminal by ./iot-network-secure.sh getid farmer@beefchain.com # or directly source to bash terminal in variable user_id source env/users/$user_email.sh","title":"Create User"},{"location":"iots/#signin-users_2","text":"Once user has been registered, sign in the paltform to get session tokens (JWT Tokens) that can be used to create things and channels. ./iot-network-secure.sh signin farmer@beefchain.com 12345678 At any time user tokens can be viewed in the bash terminal by ./iot-network-secure.sh gettoken farmer@beefchain.com # or directly source to bash terminal in variable user_token source env/tokens/$user_email.sh To get new tokens, simply signin again or use the existing tokens to check if they are still valid. ./iot-network-secure.sh checktoken farmer@beefchain.com 12345678 To see all users registered in the paltform at any time, call allusers method using admin username and password ./iot-network-secure.sh allusers admin@example.com 12345678","title":"Signin Users"},{"location":"iots/#create-sensors_2","text":"To create a sensor device using a registerd user, call the createsensor method with a username, password and sensor name ./iot-network-secure.sh createsensor farmer@beefchain.com 12345678 energysensor To source directly the sensor related id and key variables to bash # Assuming the sensor name is 'energysensor' source iotnetwork/env/things/energysensor.sh source iotnetwork/env/keys/energysensor.sh echo $sensor_id echo $sensor_key A registed user can list all the sensor devices they have on platform any time, call the getthings function with username and password ./iot-network-secure.sh getthings farmer@beefchain.com 12345678 To create channels, call the createchannel function with a registerd user email, password and channel name ./iot-network-secure.sh createchannel farmer@beefchain.com 12345678 energysensorchannel To source directly the channel id variable to bash # Assuming the sensor name is 'energysensorchannel' source iotnetwork/env/things/energysensorchannel.sh echo $channel_id A registed user can list all the channel interfaces they have on platform any time, call the getchannels function with username and password ./iot-network-secure.sh getchannels farmer@beefchain.com 12345678 To connect a particular sensor to a particular channel, call the connect function with users email, pass, sensor name and channel name ./iot-network-secure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To disconnect a particular sensor from a particular channel, call the disconnect function with users email, pass, sensor name and channel name ./iot-network-secure.sh disconnect farmer@beefchain.com 12345678 energysensor energysensorchannel Start sending data over a channel using a sensor. Connect the sensor and channel again, ./iot-network-secure.sh connect farmer@beefchain.com 12345678 energysensor energysensorchannel To send example data from a sensor to a connected channel, call the csv_to_payload function ./iot-network-secure.sh csv_to_payload farmer@beefchain.com 12345678 energysensor energysensorchannel energy # generic call parameters (replace with actual parameters) ./iot-network-secure.sh csv_to_payload <user_email> <user_pass> <sensor_name> <file_name> The file energy.csv file needs to be in iotenetwork/data/energy.csv. Number of curl calls generated will depend on the data rows in the csv file. In general a curl call to dump data can be configured as # grab sensor key for sensor name 'energysensor' in the variable $sensor_key source env/keys/energysensor.sh # grab channel_id for channel name 'energysensorchannel' in the variable $channel_id source env/channels/energysensorchannel.sh # make a curl call to put random energy data in mongodb database curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing $sensor_key\" http://$iot_server_address/http/channels/$channel_id/messages -d '[{\"bn\":\"urn:dev:DEVEUI:energysensor:\", \"bt\": 1.58565075E9},{\"n\": \"electricity\", \"v\": 35, \"u\": \"kWh\"},{\"n\": \"diesel\", \"v\": 50, \"u\": \"lb\"}]' To read back data from database, call the getdata function specifying the type of database (e.g. mongodb) with user credentials (email, pass), sensor name, channel name alonmg with offset and limit of data to view ./iot-network-secure.sh getdata mongodb farmer@beefchain.com 12345678 energysensor energysensorchannel 0 20 # generic call parameters (replace with actual parameters) ./iot-network-secure.sh getdata <database_type> <user_email> <user_pass> <sensor_name> <channel_name> <data_offset> <data_limit> A registered user can also dump data in a file on host machine by calling the dumpdata function ./iot-network-secure.sh dumpdata mongodb farmer@beefchain.com 12345678 farmer_data_dump # generic call parameters (replace with actual parameters) ./iot-network-secure.sh getdata <database_type> <user_email> <user_pass> <output_file_name>","title":"Create Sensors"},{"location":"ipfs/","text":"IPFS Network This submodule located under /ipfsnetwork directory is meant to serve as a resource for spinning up nodes to mainatin IPFS database. IPFS, which stands for InterPlanetary File System, is a protocol and network designed to create a peer-to-peer method of storing and sharing hypermedia in a distributed file system. It is a decentralized system for storing and accessing files, webpages, applications, and data, aiming to improve upon the traditional client-server model of the internet. For beef supply chain, it is particualry useful to maintain regulatory data in addititon to off-loading data of different formats from the blockchain ledger. SingleOrg Setup To run a standalone two node (manager+regulator) ipfs network (assuming one organizational domain) at the beef chain admin side, run the docker-compose file within /ipfsnetwork directory docker-compose -f p2p/docker-compose-single-org.yml up -d Note that for testing purposes, 'secrets' lines have been disabled in the docker-compose files. Make sure to pass sensitive information using secure measures using files or by passing parameters through environment variables (.env) This spins up two ipfs cotnainers (manager.ipfs.com and regulator.ipfs.com) and an ipfc cli (ipfscli) container attached to both for quick access to ipfs resources. The containers are connected to beefchain_ipfs network. We run the ipfs network in private setting and remove all external public nodes from our application setup. A number of preconfigurations can be done in the config/config.sh file as well which is used in the docker-compose setup. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now configure the two ipfs nodes to work in a private setting, run from bash terminal ./configure-single.sh This setsup the two nodes to use private networking by removing all public nodes, adding each others private addresses and copying swarm key. Finally a test is performed by adding a file to manager node and retreiving the file from regulator node by using the CID that was generated at the manager node during the time of file upload. You should see a result like this, added /ip4/192.168.0.3/tcp/4001/ipfs/Qmf6G9iaQNVbpYrYxLhEYcSZuyJzya7XV3xxpMLEJ3kBzc added /ip4/192.168.0.2/tcp/4001/ipfs/QmUt6WnUFWUmJdBJr4fvPL2oq7KM4gF2M7KNNVbc1veEAS Successfully copied 2.05kB to manager.ipfs.com:/var/ipfs Successfully copied 2.05kB to regulator.ipfs.com:/var/ipfs Successfully copied 2.05kB to manager.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/test_file.txt Successfully copied 2.05kB to $USER_PATH/BeefMesh/ipfsnetwork/test/retrieved_file.txt Note that removing all public addresse and adding only known addresses forces the ipfs nodes to work in private setting. For extra measures you can also force variable LIBP2P_FORCE_PNET to be true by logging into running container and setting variable 'export LIBP2P_FORCE_PNET=true' or enabling it in the docker-compose files 'environment' section To upload a file (user_file.txt) to the manager ipfs node, first copy it to the container docker cp test/user_file.txt manager.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/user_file.txt Then upload the file and retrive the CID of the uploaded file to store on blockchain or somewhere else, result=$(docker exec manager.ipfs.com ipfs add user_file.txt) CID_DATA=$(echo $result | awk '{print $2}') The contents of the variable (echo $CID_DATA) is a binary format string formed by the hash of the uploaded content (e.g. Qmc1DxF7dmEAmkwn1eynm2BeW6DnFcgvw9s5W1NDeXZc28 ). The CID is stored and kept safe for retrieveing the file later. To retrive the file from the regulator node using the same CID, docker exec regulator.ipfs.com bash -c \"ipfs cat $CID_DATA > retrieved_user_file.txt\" Now copy back the file from container to the host machine, docker cp regulator.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/retrieved_user_file.txt test/retrieved_user_file.txt To spin up a number of IPFS nodes associated with multiple organizations for local testing of BeefMesh application, first make sure the single organization containers are down. Use the provided script (ipfs-network.sh) to automate upload and download of files. To upload a file, ./ipfs-network.sh upload <container_name> <source_file_path> <container_file_path> ./ipfs-network.sh upload manager.ipfs.com test/user_file.txt /opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/user_file.txt Note that when uploading a file, a record is maintained in the folder 'ipfs_records'. To download a file using the provided script, ./ipfs-network.sh download <container_name> <cid_data> <container_file_path> ./ipfs-network.sh download manager.ipfs.com Qmc1DxF7dmEAmkwn1eynm2BeW6DnFcgvw9s5W1NDeXZc28 /opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/retrieved_user_file.txt To configure a ipfs node continer to use a specfic private address from other ipfs container, ./ipfs-network.sh configure <container_name> <container_address_to_add> <swarm_key_file_path> ./ipfs-network.sh configure manager.ipfs.com /ip4/172.18.0.3/tcp/4001/ipfs/QmPtQcKFmGWato6K5MvtAgHoVJMbermgDqdjNvVAyfozwD swarmkeygen/swarm.key To get the address of a ipfs node container for sharing with other ipfs node, ./ipfs-network.sh getaddress ./ipfs-network.sh getaddress manager.ipfs.com A custom container file is provided for instantiating at a new organization. The file is located at ipfsnetwork/p2p/docker-compose-custom.yml To bring down containers manually, docker stop manager.ipfs.com regulator.ipfs.com ipfscli ipfs-swarm-key-gen docker rm manager.ipfs.com regulator.ipfs.com ipfscli ipfs-swarm-key-gen # remove volumes (make sure to create backup before doing do!) docker volume rm p2p_manager.ipfs.com && docker volume rm p2p_regulator.ipfs.com && docker volume prune MultiOrg Setup To run multi-node ipfs network (assuming multiple organizational domain) locally, run the docker-compose file within /ipfsnetwork directory docker-compose -f p2p/docker-compose-multi-org.yml up -d Note that for testing purposes, 'secrets' lines have been disabled in the docker-compose files. Make sure to pass sensitive information using secure measures using files or by passing parameters through environment variables (.env) We run the ipfs network in private setting and remove all external public nodes from our application setup. These can be preconfigured in the config.sh file and enabled in the docker-compose file. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now configure the two ipfs nodes to work in a private setting, run from bash terminal ./configure-multi.sh This setsup the two nodes to use private networking by removing all public nodes, adding each others private addresses and copying swarm key. Finally a test is performed by adding a file to manager node and retreiving the file from regulator node by using the CID that was generated at the manager node during the time of file upload. To bring down the containers manually docker stop ipfscli manager.ipfs.com regulator.ipfs.com farmer.ipfs.com breeder.ipfs.com retailer.ipfs.com consumer.ipfs.com distributor.ipfs.com processor.ipfs.com ipfs-swarm-key-gen docker rm ipfscli manager.ipfs.com regulator.ipfs.com farmer.ipfs.com breeder.ipfs.com retailer.ipfs.com consumer.ipfs.com distributor.ipfs.com processor.ipfs.com ipfs-swarm-key-gen Cluster Setup IPFS creates a decentralized method of storing and sharing hypermedia in a distributed file system. While IPFS itself doesn't inherently involve clustering, we can certainly set up clusters of IPFS nodes to improve pinning files and improving performance, redundancy, and availability. Pinning files ensures that they are only available on certain nodes with priority adding another layer of security and reliability. First make sure there are no other IPFS containers running. Generate a cluster secret of random 32 byte hexadecimal stringusing openssl used in docker-compose file later export CLUSTER_SECRET=$(openssl rand -hex 32) # Save it to a file echo $CLUSTER_SECRET > swarmkeygen/cluster_secret.txt The cluster setup of two ipfs nodes (manager.ipfs.node and regulator.ipfs.node) and two ipfs cluster nodes (manager.ipfs.cluster and regulator.ipfs.cluster) is split up into two docker-compose files. First, we spin up the manger setup and once its up and running we spin up the regualtor setup which takes configuration settings from the already running manager setup at runtime. To start the manager setup, spin up the docker-compose file docker-compose -f p2p/docker-compose-cluster-manager.yml up -d The running containers connect to beefchain_ipfs network. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now run the first bash script to configure and extract settings from the running ipfs cluster nodes. ./configure-cluster-1.sh Next source node and cluster address settings for the manager containers, source p2p/secrets/cluster.sh Spin up the containers for regulator setup, docker-compose -f p2p/docker-compose-cluster-regulator.yml up -d Next, run the second bash script to configure the regulator setup which also tests uploading files to manager nodes and retrieving them from regulator nodes. ./configure-cluster-2.sh Finally, test cluster settings from any cluster node. Copy test files to regulator cluster docker cp test/test_file.txt regulator.ipfs.cluster:/tmp/test_file.txt Add file to cluster, file_upload_info=(docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl add tmp/test_file.txt) file_upload_cid=$(echo $file_upload_info | awk '{print $2}') Pin uploaded file to cluster, docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl pin add $file_upload_cid Check status of pinned files, cluster and peer nodes in the cluster docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl pin ls docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl status $file_upload_cid docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl peers ls To bring down the containers manually, docker stop regulator.ipfs.cluster regulator.ipfs.node manager.ipfs.cluster manager.ipfs.node docker rm regulator.ipfs.cluster regulator.ipfs.node manager.ipfs.cluster manager.ipfs.node","title":"IPFS Network"},{"location":"ipfs/#ipfs-network","text":"This submodule located under /ipfsnetwork directory is meant to serve as a resource for spinning up nodes to mainatin IPFS database. IPFS, which stands for InterPlanetary File System, is a protocol and network designed to create a peer-to-peer method of storing and sharing hypermedia in a distributed file system. It is a decentralized system for storing and accessing files, webpages, applications, and data, aiming to improve upon the traditional client-server model of the internet. For beef supply chain, it is particualry useful to maintain regulatory data in addititon to off-loading data of different formats from the blockchain ledger.","title":"IPFS Network"},{"location":"ipfs/#singleorg-setup","text":"To run a standalone two node (manager+regulator) ipfs network (assuming one organizational domain) at the beef chain admin side, run the docker-compose file within /ipfsnetwork directory docker-compose -f p2p/docker-compose-single-org.yml up -d Note that for testing purposes, 'secrets' lines have been disabled in the docker-compose files. Make sure to pass sensitive information using secure measures using files or by passing parameters through environment variables (.env) This spins up two ipfs cotnainers (manager.ipfs.com and regulator.ipfs.com) and an ipfc cli (ipfscli) container attached to both for quick access to ipfs resources. The containers are connected to beefchain_ipfs network. We run the ipfs network in private setting and remove all external public nodes from our application setup. A number of preconfigurations can be done in the config/config.sh file as well which is used in the docker-compose setup. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now configure the two ipfs nodes to work in a private setting, run from bash terminal ./configure-single.sh This setsup the two nodes to use private networking by removing all public nodes, adding each others private addresses and copying swarm key. Finally a test is performed by adding a file to manager node and retreiving the file from regulator node by using the CID that was generated at the manager node during the time of file upload. You should see a result like this, added /ip4/192.168.0.3/tcp/4001/ipfs/Qmf6G9iaQNVbpYrYxLhEYcSZuyJzya7XV3xxpMLEJ3kBzc added /ip4/192.168.0.2/tcp/4001/ipfs/QmUt6WnUFWUmJdBJr4fvPL2oq7KM4gF2M7KNNVbc1veEAS Successfully copied 2.05kB to manager.ipfs.com:/var/ipfs Successfully copied 2.05kB to regulator.ipfs.com:/var/ipfs Successfully copied 2.05kB to manager.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/test_file.txt Successfully copied 2.05kB to $USER_PATH/BeefMesh/ipfsnetwork/test/retrieved_file.txt Note that removing all public addresse and adding only known addresses forces the ipfs nodes to work in private setting. For extra measures you can also force variable LIBP2P_FORCE_PNET to be true by logging into running container and setting variable 'export LIBP2P_FORCE_PNET=true' or enabling it in the docker-compose files 'environment' section To upload a file (user_file.txt) to the manager ipfs node, first copy it to the container docker cp test/user_file.txt manager.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/user_file.txt Then upload the file and retrive the CID of the uploaded file to store on blockchain or somewhere else, result=$(docker exec manager.ipfs.com ipfs add user_file.txt) CID_DATA=$(echo $result | awk '{print $2}') The contents of the variable (echo $CID_DATA) is a binary format string formed by the hash of the uploaded content (e.g. Qmc1DxF7dmEAmkwn1eynm2BeW6DnFcgvw9s5W1NDeXZc28 ). The CID is stored and kept safe for retrieveing the file later. To retrive the file from the regulator node using the same CID, docker exec regulator.ipfs.com bash -c \"ipfs cat $CID_DATA > retrieved_user_file.txt\" Now copy back the file from container to the host machine, docker cp regulator.ipfs.com:/opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/retrieved_user_file.txt test/retrieved_user_file.txt To spin up a number of IPFS nodes associated with multiple organizations for local testing of BeefMesh application, first make sure the single organization containers are down. Use the provided script (ipfs-network.sh) to automate upload and download of files. To upload a file, ./ipfs-network.sh upload <container_name> <source_file_path> <container_file_path> ./ipfs-network.sh upload manager.ipfs.com test/user_file.txt /opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/user_file.txt Note that when uploading a file, a record is maintained in the folder 'ipfs_records'. To download a file using the provided script, ./ipfs-network.sh download <container_name> <cid_data> <container_file_path> ./ipfs-network.sh download manager.ipfs.com Qmc1DxF7dmEAmkwn1eynm2BeW6DnFcgvw9s5W1NDeXZc28 /opt/gopath/src/github.com/BeefChain/IPFS/p2p/peer/retrieved_user_file.txt To configure a ipfs node continer to use a specfic private address from other ipfs container, ./ipfs-network.sh configure <container_name> <container_address_to_add> <swarm_key_file_path> ./ipfs-network.sh configure manager.ipfs.com /ip4/172.18.0.3/tcp/4001/ipfs/QmPtQcKFmGWato6K5MvtAgHoVJMbermgDqdjNvVAyfozwD swarmkeygen/swarm.key To get the address of a ipfs node container for sharing with other ipfs node, ./ipfs-network.sh getaddress ./ipfs-network.sh getaddress manager.ipfs.com A custom container file is provided for instantiating at a new organization. The file is located at ipfsnetwork/p2p/docker-compose-custom.yml To bring down containers manually, docker stop manager.ipfs.com regulator.ipfs.com ipfscli ipfs-swarm-key-gen docker rm manager.ipfs.com regulator.ipfs.com ipfscli ipfs-swarm-key-gen # remove volumes (make sure to create backup before doing do!) docker volume rm p2p_manager.ipfs.com && docker volume rm p2p_regulator.ipfs.com && docker volume prune","title":"SingleOrg Setup"},{"location":"ipfs/#multiorg-setup","text":"To run multi-node ipfs network (assuming multiple organizational domain) locally, run the docker-compose file within /ipfsnetwork directory docker-compose -f p2p/docker-compose-multi-org.yml up -d Note that for testing purposes, 'secrets' lines have been disabled in the docker-compose files. Make sure to pass sensitive information using secure measures using files or by passing parameters through environment variables (.env) We run the ipfs network in private setting and remove all external public nodes from our application setup. These can be preconfigured in the config.sh file and enabled in the docker-compose file. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now configure the two ipfs nodes to work in a private setting, run from bash terminal ./configure-multi.sh This setsup the two nodes to use private networking by removing all public nodes, adding each others private addresses and copying swarm key. Finally a test is performed by adding a file to manager node and retreiving the file from regulator node by using the CID that was generated at the manager node during the time of file upload. To bring down the containers manually docker stop ipfscli manager.ipfs.com regulator.ipfs.com farmer.ipfs.com breeder.ipfs.com retailer.ipfs.com consumer.ipfs.com distributor.ipfs.com processor.ipfs.com ipfs-swarm-key-gen docker rm ipfscli manager.ipfs.com regulator.ipfs.com farmer.ipfs.com breeder.ipfs.com retailer.ipfs.com consumer.ipfs.com distributor.ipfs.com processor.ipfs.com ipfs-swarm-key-gen","title":"MultiOrg Setup"},{"location":"ipfs/#cluster-setup","text":"IPFS creates a decentralized method of storing and sharing hypermedia in a distributed file system. While IPFS itself doesn't inherently involve clustering, we can certainly set up clusters of IPFS nodes to improve pinning files and improving performance, redundancy, and availability. Pinning files ensures that they are only available on certain nodes with priority adding another layer of security and reliability. First make sure there are no other IPFS containers running. Generate a cluster secret of random 32 byte hexadecimal stringusing openssl used in docker-compose file later export CLUSTER_SECRET=$(openssl rand -hex 32) # Save it to a file echo $CLUSTER_SECRET > swarmkeygen/cluster_secret.txt The cluster setup of two ipfs nodes (manager.ipfs.node and regulator.ipfs.node) and two ipfs cluster nodes (manager.ipfs.cluster and regulator.ipfs.cluster) is split up into two docker-compose files. First, we spin up the manger setup and once its up and running we spin up the regualtor setup which takes configuration settings from the already running manager setup at runtime. To start the manager setup, spin up the docker-compose file docker-compose -f p2p/docker-compose-cluster-manager.yml up -d The running containers connect to beefchain_ipfs network. To setup ipfs in private settings, a swarm key can be used. Spin up swarm key generator container, docker-compose -f swarmkeygen/docker-compose-swarm.yml up -d Generate a key from the swarm key generator container and copy it to host machine, docker exec -i ipfs-swarm-key-gen piskg > swarmkeygen/swarm.key Now run the first bash script to configure and extract settings from the running ipfs cluster nodes. ./configure-cluster-1.sh Next source node and cluster address settings for the manager containers, source p2p/secrets/cluster.sh Spin up the containers for regulator setup, docker-compose -f p2p/docker-compose-cluster-regulator.yml up -d Next, run the second bash script to configure the regulator setup which also tests uploading files to manager nodes and retrieving them from regulator nodes. ./configure-cluster-2.sh Finally, test cluster settings from any cluster node. Copy test files to regulator cluster docker cp test/test_file.txt regulator.ipfs.cluster:/tmp/test_file.txt Add file to cluster, file_upload_info=(docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl add tmp/test_file.txt) file_upload_cid=$(echo $file_upload_info | awk '{print $2}') Pin uploaded file to cluster, docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl pin add $file_upload_cid Check status of pinned files, cluster and peer nodes in the cluster docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl pin ls docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl status $file_upload_cid docker exec -it regulator.ipfs.cluster ipfs-cluster-ctl peers ls To bring down the containers manually, docker stop regulator.ipfs.cluster regulator.ipfs.node manager.ipfs.cluster manager.ipfs.node docker rm regulator.ipfs.cluster regulator.ipfs.node manager.ipfs.cluster manager.ipfs.node","title":"Cluster Setup"},{"location":"layout/","text":"Layout The BeefMesh framework connects together a number of microservices to run collaborative applications, which are arranged in different sub-folders described below. More details for each sub-module can be found in later sections. sessions The /sessions folder hosts a Flask server that allows users to register to the framework, form groups, and upload or download resources related to GlusterFS, the overlay network, and IPFS addresses, in addition to other required resources to form a distributed network. The sessions application also runs as the starting collaboration point, hence is also referred to as collaborator or group initiator. emissions The /emissions folder hosts a Flask server that allows a regulator to maintain a carbon emissions database, which can be modified according to the consensus among organizations using a particular emissions factor. utsaserver The /utsaserver folder is meant to timestamp files and verify their originality. The resultant files from timestamping can be stored on the blockchain or other databases. databases The /databases folder is meant to spin up a number of databases to allow organizations to consume different types of data from events and processes occurring within the facility, from which useful information can be extracted and shared. ipfsnetwork The /ipfsnetwork folder is meant to spin up nodes for managing IPFS databases, which allow organizations to mutually maintain data in a decentralized manner with redundancy and fault tolerance, a quality particularly useful for regulatory data. overlaynetwork The /overlaynetwork folder contains resources that can be used to create a swarm-based overlay network between nodes for secure, resilient, and distributed communication between containerized applications. iotnetwork The /iotnetwork folder is meant to spin up interfaces for consuming data from IoT devices, sensors, and other types of similar gadgets so that it can be used for machine learning or knowledge transfer. netshareddrive The /netshareddrive folder is meant to establish a secure network shared drive among groups to quickly access shared or common files needed to run certain applications, e.g., blockchain services. blockchain The /blockchain folder is meant to provide resources to spin up a blockchain network to provide services like traceability, data sharing, or data recording.","title":"Layout"},{"location":"layout/#layout","text":"The BeefMesh framework connects together a number of microservices to run collaborative applications, which are arranged in different sub-folders described below. More details for each sub-module can be found in later sections.","title":"Layout"},{"location":"layout/#sessions","text":"The /sessions folder hosts a Flask server that allows users to register to the framework, form groups, and upload or download resources related to GlusterFS, the overlay network, and IPFS addresses, in addition to other required resources to form a distributed network. The sessions application also runs as the starting collaboration point, hence is also referred to as collaborator or group initiator.","title":"sessions"},{"location":"layout/#emissions","text":"The /emissions folder hosts a Flask server that allows a regulator to maintain a carbon emissions database, which can be modified according to the consensus among organizations using a particular emissions factor.","title":"emissions"},{"location":"layout/#utsaserver","text":"The /utsaserver folder is meant to timestamp files and verify their originality. The resultant files from timestamping can be stored on the blockchain or other databases.","title":"utsaserver"},{"location":"layout/#databases","text":"The /databases folder is meant to spin up a number of databases to allow organizations to consume different types of data from events and processes occurring within the facility, from which useful information can be extracted and shared.","title":"databases"},{"location":"layout/#ipfsnetwork","text":"The /ipfsnetwork folder is meant to spin up nodes for managing IPFS databases, which allow organizations to mutually maintain data in a decentralized manner with redundancy and fault tolerance, a quality particularly useful for regulatory data.","title":"ipfsnetwork"},{"location":"layout/#overlaynetwork","text":"The /overlaynetwork folder contains resources that can be used to create a swarm-based overlay network between nodes for secure, resilient, and distributed communication between containerized applications.","title":"overlaynetwork"},{"location":"layout/#iotnetwork","text":"The /iotnetwork folder is meant to spin up interfaces for consuming data from IoT devices, sensors, and other types of similar gadgets so that it can be used for machine learning or knowledge transfer.","title":"iotnetwork"},{"location":"layout/#netshareddrive","text":"The /netshareddrive folder is meant to establish a secure network shared drive among groups to quickly access shared or common files needed to run certain applications, e.g., blockchain services.","title":"netshareddrive"},{"location":"layout/#blockchain","text":"The /blockchain folder is meant to provide resources to spin up a blockchain network to provide services like traceability, data sharing, or data recording.","title":"blockchain"},{"location":"overlay/","text":"Overlay Network Overlay network allows nodes in the network to communicate with each other regardless of their physical location or the underlying network infrastructure. Overlay networks are commonly used in peer-to-peer (P2P) systems, content delivery networks (CDNs), and virtual private networks (VPNs). It can be establisehd using docker and swarm services. We use overlay network in our application for following reasons. It allows calling containerzied applciations by their hostname or container name. There is therefore no need to individually host tons of containers for a domain name. It provides a secure layer for containers to interact and isolates it from other public networks. It further makes it easier to discover services over the dedicated network. It is secure, resiliant and enables efficient communication between P2P and CDN architectures. Setting up an overlay network requires more than one physical hosts that can be reached over internet. If you are using virtual machines for testing on the same physical host, make sure they are using 'Bridged Adapter' so that each VM gets a separate network adddress. NAT will not work for creating overlay on VMs on the same host machine because this assigns them the same network address. Further, WSL machines on windows also get the same network addresses and currently (January 2024) don't support bridged networking. The scripts provided here have been tested with VirtualBox VMs by setting network to use 'Bridged Adapter'. To get started, grab the network addresses of each machine and ping other machines to check if they are reachable. Run from within overlaynetwork folder. # Run from within 'overlay' folder ./overlay-network.sh ipaddress # Replace with other hosts ip address ping <other_host_ip_address> The overlay-network.sh script saves environment variables in /overlaynetwork/env folder everytime its called. One of the host machines needs to start a docker swarm manager and others can join as 'manager' or 'workers' depending upon the requirements and hierarchy of network. On any one of the host machines, first remove any existing swarm network and create a new one. # (Optional) Remove existing overlay network ./overlay-network.sh remove # Create a new swarm overlay network ./overlay-network.sh create Successful creation of swarm overlay network stores 'manager' and 'worker' keys in /overlaynetwork/env folder (swarmmanager.txt and swarmworker.txt). These keys are needed in other host machine and should be shared and avialble in overlaynetwork/env folder before they can join the overlay network. Once the files are successfully shared and in place, run the provided script to join as 'manager' or 'worker'. # Join as manager ./overlay-network.sh joinasmanager # Join as worker ./overlay-network.sh joinasworker Ocassionally, an error shown below may pop up: Error response from daemon: manager stopped: can't initialize raft node: rpc error: code = Unknown desc = could not connect to prospective new cluster member using its advertised address: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp x.x.x.x:2377: connect: no route to host\" Overlay network works on specific ports which need to be configured to allow traffic coming and and out of the host network interface. To allow configuring the specific ports, either use ufw (Ubuntu) application or firewall-cmd (Debian, CentOS or others) application. To set network filters using ufw (note: may require raised priviliges!) # Apply ufw filters ./overlay-network.sh applyufw # Remove ufw filters ./overlay-network.sh removeufw # list applied ufw filters ./overlay-network.sh listufw # disable ufw application ./overlay-network.sh disableufw # enable ufw application ./overlay-network.sh enableufw To set network filters using firewall-cmd (note: may require raised priviliges!) # Apply firewall filters ./overlay-network.sh applyfirewall # Remove firewall filters ./overlay-network.sh removefirewall # list applied firewall filters ./overlay-network.sh listfirewall # disable firewall application ./overlay-network.sh disablefirewall # enable firewall application ./overlay-network.sh enablefirewall Note: Using both filters on a linux system will cause the same error mentioned earlier due to conflicts. For a similar error scenario, remove and disable either of the network filter application completely and apply and enable the other one! Once other hosts have joined the overlay network, create a named network that will used to connect containerized applications. (Note that this can be done on 'managers' only and since we join hosts as managers, we can create it from any host connected to the overlay network). # Create a named network to use on overlay ./overlay-network.sh createoverlay <network_name> # Remove a named network from overlay ./overlay-network.sh removeoverlay <network_name> At any time, list the networks (bridge, overlay, host, null) available to use with contaiers ./overlay-network.sh listnetworks To directly source variables stored in overlaynetwork/env folder to use in bash terminal # ipaddress=x.x.x.x source overlaynetwork/env/hostip.sh # swarm_manager_key manager_key=$(cat overlaynetwork/env/swarmmanager.txt) # swam_worker_key worker_key=$(cat overlaynetwork/env/swarmworker.txt)","title":"Overlay Network"},{"location":"overlay/#overlay-network","text":"Overlay network allows nodes in the network to communicate with each other regardless of their physical location or the underlying network infrastructure. Overlay networks are commonly used in peer-to-peer (P2P) systems, content delivery networks (CDNs), and virtual private networks (VPNs). It can be establisehd using docker and swarm services. We use overlay network in our application for following reasons. It allows calling containerzied applciations by their hostname or container name. There is therefore no need to individually host tons of containers for a domain name. It provides a secure layer for containers to interact and isolates it from other public networks. It further makes it easier to discover services over the dedicated network. It is secure, resiliant and enables efficient communication between P2P and CDN architectures. Setting up an overlay network requires more than one physical hosts that can be reached over internet. If you are using virtual machines for testing on the same physical host, make sure they are using 'Bridged Adapter' so that each VM gets a separate network adddress. NAT will not work for creating overlay on VMs on the same host machine because this assigns them the same network address. Further, WSL machines on windows also get the same network addresses and currently (January 2024) don't support bridged networking. The scripts provided here have been tested with VirtualBox VMs by setting network to use 'Bridged Adapter'. To get started, grab the network addresses of each machine and ping other machines to check if they are reachable. Run from within overlaynetwork folder. # Run from within 'overlay' folder ./overlay-network.sh ipaddress # Replace with other hosts ip address ping <other_host_ip_address> The overlay-network.sh script saves environment variables in /overlaynetwork/env folder everytime its called. One of the host machines needs to start a docker swarm manager and others can join as 'manager' or 'workers' depending upon the requirements and hierarchy of network. On any one of the host machines, first remove any existing swarm network and create a new one. # (Optional) Remove existing overlay network ./overlay-network.sh remove # Create a new swarm overlay network ./overlay-network.sh create Successful creation of swarm overlay network stores 'manager' and 'worker' keys in /overlaynetwork/env folder (swarmmanager.txt and swarmworker.txt). These keys are needed in other host machine and should be shared and avialble in overlaynetwork/env folder before they can join the overlay network. Once the files are successfully shared and in place, run the provided script to join as 'manager' or 'worker'. # Join as manager ./overlay-network.sh joinasmanager # Join as worker ./overlay-network.sh joinasworker Ocassionally, an error shown below may pop up: Error response from daemon: manager stopped: can't initialize raft node: rpc error: code = Unknown desc = could not connect to prospective new cluster member using its advertised address: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp x.x.x.x:2377: connect: no route to host\" Overlay network works on specific ports which need to be configured to allow traffic coming and and out of the host network interface. To allow configuring the specific ports, either use ufw (Ubuntu) application or firewall-cmd (Debian, CentOS or others) application. To set network filters using ufw (note: may require raised priviliges!) # Apply ufw filters ./overlay-network.sh applyufw # Remove ufw filters ./overlay-network.sh removeufw # list applied ufw filters ./overlay-network.sh listufw # disable ufw application ./overlay-network.sh disableufw # enable ufw application ./overlay-network.sh enableufw To set network filters using firewall-cmd (note: may require raised priviliges!) # Apply firewall filters ./overlay-network.sh applyfirewall # Remove firewall filters ./overlay-network.sh removefirewall # list applied firewall filters ./overlay-network.sh listfirewall # disable firewall application ./overlay-network.sh disablefirewall # enable firewall application ./overlay-network.sh enablefirewall Note: Using both filters on a linux system will cause the same error mentioned earlier due to conflicts. For a similar error scenario, remove and disable either of the network filter application completely and apply and enable the other one! Once other hosts have joined the overlay network, create a named network that will used to connect containerized applications. (Note that this can be done on 'managers' only and since we join hosts as managers, we can create it from any host connected to the overlay network). # Create a named network to use on overlay ./overlay-network.sh createoverlay <network_name> # Remove a named network from overlay ./overlay-network.sh removeoverlay <network_name> At any time, list the networks (bridge, overlay, host, null) available to use with contaiers ./overlay-network.sh listnetworks To directly source variables stored in overlaynetwork/env folder to use in bash terminal # ipaddress=x.x.x.x source overlaynetwork/env/hostip.sh # swarm_manager_key manager_key=$(cat overlaynetwork/env/swarmmanager.txt) # swam_worker_key worker_key=$(cat overlaynetwork/env/swarmworker.txt)","title":"Overlay Network"},{"location":"sessions/","text":"Sessions This flask based submodule located under /sessions directory is meant to serve as a frontend for verifying user credentials and forming collaboration groups by uploading or downloading resources and information related to swarm overlay network, shared network drive (GlusterFS), shared databases (IPFS) or literally any useful application. The sessions application also runs as the starting collaboration point, hence is also referred to as collaborator or group initiator. For testing purposes as a standalone application from BeefMesh framework, create a virtual envrionment virtualenv mainserver source mainserver/bin/activate pip install -r requirements.txt python -m main Once up and running, use curl (https://curl.haxx.se/download.html) or httpie (https://httpie.io/) commands to interact with the API. For running the application for testing without docker may require setting credentials. These can be imported by \"$ source sessions/env/env.sh\" or by settting them as, export SESSION_SUPERADMIN_PASSWORD=beefchain export SESSION_ADMIN_PASSWORD=beefchain export SESSION_TEST_USER_PASSWORD=beefchain export SESSION_GENERIC_USER_PASSWORD=beefchain In docker, the credentials are imported as secrets and kept under /sessions/secrets for testing purposes. By default, the application initializes with the following credentials defined in /api/db_initializer/db_initializer.py: Super Admin Credentials: username: username_superadmin password: beefchain email: email_superadmin@beefsupply.com Admin Credentials: username: username_admin password: beefchain email: email_admin@beefsupply.com Test User Credentials: username: username_test password: beefchain email: email_test@beefsupply.com BeefChain User Credentials: username: username_beefsupply password: beefchain email: email_beefsupply@beefsupply.com The database file is created and maintained under /api/db_files. For testing with docker as a standalone application, spin up the containerby navigating into the /sessions directory. docker-compose up -d Once up and running, different routes can be used to interact with the application. Some example routes are given below. Note: The curl commands below use 'localhost' in the url section. This would he replaced with the IP or URL mapping of the hosted application when accessing from an outside host. The port '7001' can be changed to whatever port is suitable for your system . Register Users curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@beefsupply.com\"}' https://localhost:7001/beefchain/v1/auth/register Note that we use -k flag to use dummy certificates generated from openssl application or with arbitrary certificate assignment by flask. Login Users curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@beefsupply.com\",\"password\":\"example_password\"}' https://localhost:7001/beefchain/v1/auth/login Logging in a user generates an Access Token and a Refresh Token that can be used to access resources on the server. An example of the response with tokens is given below. { \"access_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU0Mjg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20iLCJhZG1pbiI6MH0.DlJnfzALoul7CH_jh0_R6SpNF-2w5xzx7iu2ZR-AXh_ErFyfl_KMOLBLyZ8smX7gSe1sfvJzKHAgQPoqlUnR7w\", \"refresh_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU3ODg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20ifQ.ysZ8JDAj5SlPe0rbfr2DXGL0BPCxZ-TFvqRyvabcNYkyKJmWXP3WkJ8xryFtxIE12a04SR-VkFyiLLjGnJvVjw\" } The tokens will be passed whereever user is requied to be logged in on the server to access any resources. Logout Users curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:7001/beefchain/v1/auth/logout Note that the above command requires passing values for token variables. Refresh Tokens curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:7001/beefchain/v1/auth/refresh Reset Password curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://localhost:7001/beefchain/v1/auth/password_reset Remove User curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@beefsupply.com\"}' https://localhost:7001/beefchain/v1/auth/remove_user Note that removing a user requires super admin permissions and the super admin needs to be logged in to be able to pass on their access tokens. These requirements can be changed under relevant classes which are classed against each route. These classes can be found in UserHandlers.py file under /api/handlers Checking Permissions To check whether a user should be allowed to access any resource on the server, curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_user_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_admin_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_sadmin_permission The route will return a {\"status\": \"success\"} or {\"status\": \"failure\"} message depending upon whether the user is succesffuly logged in and using correct token. This can be used as a condition to allow access to resources. Testing Permissions Some test routes for debugging user permissions are provided below. curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_admin curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_super_admin Create Collaboration Groups Note: The curl commands from here onwards have the 'Authorization' header part removed for testing purposes. Enable Authorization for the routes by including @auth.login_required and @role_required.permission(2) for the relevant classes called by routes in the file UserHandlers.py A group for collaboration can be created by, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\",\"gluster\":\"192.168.1.5\",\"ipfs\":\"192.168.1.5\",\"overlay\":\"breeder_network\"}' https://localhost:7001/beefchain/v1/auth/insert_group_info The command in addition to adding information to a database, creates a folder with the same group name under /api/db_files. The folder further contains files for appending information from group users related to glusterFS, IPFS and Overlay network overtime. Information related to the group can be retrieved by, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\"}' https://localhost:7001/beefchain/v1/auth/get_group_info Upload Group Related Files Files that can be used to form collboration groups can be uploaded by, curl -k -F 'file=@/home/USER/BeefChain/sessions/beefchain.txt' https://localhost:7001/beefchain/v1/auth/upload_file To allow a file extension to be allowed or disallowed from being uploaded to the server, modify the ALLOWED_EXTENSIONS variable in the UserHandlers.py file under /api/handlers. To download a file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://localhost:7001/beefchain/v1/auth/download_file > beefchain.txt To remove a file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://localhost:7001/beefchain/v1/auth/remove_file Making Group Related Requests To submit any request related to group that can be viewed by admins or reguator, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"requestinfo\":\"create a new sub-group breeder1 for us!\"}' https://localhost:7001/beefchain/v1/auth/add_user_requests This creates a folder under /api/requests with a filename mathching the requestor name. The staus of the request is set to 'Pending' at the time of submission and changed once the request has been fulfilled relevant authority. To see the status of the request, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\"}' https://localhost:7001/beefchain/v1/auth/user_requests_status To change the status of the request as an adminstrator or regulator, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"status\":\"Completed!\"}' https://localhost:7001/beefchain/v1/auth/user_requests_update Bring Down Containers To bring down the container manually, docker stop session.beefsupply.com && docker rm session.beefsupply.com && docker volume rm sessions_session.beefsupply.com # Remove directories created during building and running app rm -R mainserver && rm -R __pycache__ # Remove volumes (make back up before doing so) docker volume rm sessions_session.beefsupply.com && docker volume prune # remove all __pycache__ files in /sessions directory generated during the application runtime find . -type d -name \"__pycache__\" -exec rm -r {} + # To remove the database files rm api/db_files/* Note: Enable the 'restart' option in docker-compose file with appropriate options {always, on-failure, unless-stopped} for persistence and make back up of volumes before removing them Multi-Host Environment Containers can be accessed from a different host using the (1) hosted ip address (2) domain name or the (3) container name. The container name is set to 'session.beefsupply.com' so any other container directly connected to the network 'beef_supply' can access the cotainer directly using the link https://session.beefsupply.com:7001/ If the containers are hosted permanently with a domain name, edit the /etc/hosts file to point to the domain name (more details can be found at: https://man7.org/linux/man-pages/man5/hosts.5.html). We restrict the tutorial to a service hosted on a machine with a reachable ip address. Once the ip-address of the machine where the service is being hosted in confirmed (e.g. 203.0.113.5 ), change it for the variable 'sessions_address' in the BeefMesh/environment/service_ip_address.sh file and source it in the terminal source environment/service_ip_addresses.sh All of the commands above will be used with the new address $sessions_address instead of localhost. Register Users curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@beefsupply.com\"}' https://$sessions_address:7001/beefchain/v1/auth/register Login Users curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@beefsupply.com\",\"password\":\"example_password\"}' https://$sessions_address:7001/beefchain/v1/auth/login Logout Users curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$sessions_address:7001/beefchain/v1/auth/logout Refresh Tokens curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$sessions_address:7001/beefchain/v1/auth/refresh Reset Password curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://$sessions_address:7001/beefchain/v1/auth/password_reset Remove User curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@beefsupply.com\"}' https://$sessions_address:7001/beefchain/v1/auth/remove_user Checking Permissions For generic user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_user_permission For admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_admin_permission For super admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_sadmin_permission Testing Permissions For generic user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_user For admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_admin For super admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_super_admin Create Collaboration Groups To create a group curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\",\"gluster\":\"192.168.1.5\",\"ipfs\":\"192.168.1.5\",\"overlay\":\"breeder_network\"}' https://$sessions_address:7001/beefchain/v1/auth/insert_group_info To retrieve information related to a group curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\"}' https://$sessions_address:7001/beefchain/v1/auth/get_group_info Upload Group Related Files Top upload files related to a group curl -k -F 'file=@/home/USER/BeefChain/sessions/beefchain.txt' https://$sessions_address:7001/beefchain/v1/auth/upload_file To download a group related file curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://$sessions_address:7001/beefchain/v1/auth/download_file > beefchain.txt To remove a group related file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://$sessions_address:7001/beefchain/v1/auth/remove_file Making Group Related Requests To submit any request related to group curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"requestinfo\":\"create a new sub-group breeder1 for us!\"}' https://$sessions_address:7001/beefchain/v1/auth/add_user_requests To see the status of the request curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\"}' https://$sessions_address:7001/beefchain/v1/auth/user_requests_status To change the status of the request as an adminstrator or regulator curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"status\":\"Completed!\"}' https://$sessions_address:7001/beefchain/v1/auth/user_requests_update","title":"Sessions"},{"location":"sessions/#sessions","text":"This flask based submodule located under /sessions directory is meant to serve as a frontend for verifying user credentials and forming collaboration groups by uploading or downloading resources and information related to swarm overlay network, shared network drive (GlusterFS), shared databases (IPFS) or literally any useful application. The sessions application also runs as the starting collaboration point, hence is also referred to as collaborator or group initiator. For testing purposes as a standalone application from BeefMesh framework, create a virtual envrionment virtualenv mainserver source mainserver/bin/activate pip install -r requirements.txt python -m main Once up and running, use curl (https://curl.haxx.se/download.html) or httpie (https://httpie.io/) commands to interact with the API. For running the application for testing without docker may require setting credentials. These can be imported by \"$ source sessions/env/env.sh\" or by settting them as, export SESSION_SUPERADMIN_PASSWORD=beefchain export SESSION_ADMIN_PASSWORD=beefchain export SESSION_TEST_USER_PASSWORD=beefchain export SESSION_GENERIC_USER_PASSWORD=beefchain In docker, the credentials are imported as secrets and kept under /sessions/secrets for testing purposes. By default, the application initializes with the following credentials defined in /api/db_initializer/db_initializer.py: Super Admin Credentials: username: username_superadmin password: beefchain email: email_superadmin@beefsupply.com Admin Credentials: username: username_admin password: beefchain email: email_admin@beefsupply.com Test User Credentials: username: username_test password: beefchain email: email_test@beefsupply.com BeefChain User Credentials: username: username_beefsupply password: beefchain email: email_beefsupply@beefsupply.com The database file is created and maintained under /api/db_files. For testing with docker as a standalone application, spin up the containerby navigating into the /sessions directory. docker-compose up -d Once up and running, different routes can be used to interact with the application. Some example routes are given below. Note: The curl commands below use 'localhost' in the url section. This would he replaced with the IP or URL mapping of the hosted application when accessing from an outside host. The port '7001' can be changed to whatever port is suitable for your system .","title":"Sessions"},{"location":"sessions/#register-users","text":"curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@beefsupply.com\"}' https://localhost:7001/beefchain/v1/auth/register Note that we use -k flag to use dummy certificates generated from openssl application or with arbitrary certificate assignment by flask.","title":"Register Users"},{"location":"sessions/#login-users","text":"curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@beefsupply.com\",\"password\":\"example_password\"}' https://localhost:7001/beefchain/v1/auth/login Logging in a user generates an Access Token and a Refresh Token that can be used to access resources on the server. An example of the response with tokens is given below. { \"access_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU0Mjg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20iLCJhZG1pbiI6MH0.DlJnfzALoul7CH_jh0_R6SpNF-2w5xzx7iu2ZR-AXh_ErFyfl_KMOLBLyZ8smX7gSe1sfvJzKHAgQPoqlUnR7w\", \"refresh_token\": \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwODY1MDY4NSwiZXhwIjoxNzA4NjU3ODg1fQ.eyJlbWFpbCI6ImV4YW1wbGVAZXhhbXBsZS5jb20ifQ.ysZ8JDAj5SlPe0rbfr2DXGL0BPCxZ-TFvqRyvabcNYkyKJmWXP3WkJ8xryFtxIE12a04SR-VkFyiLLjGnJvVjw\" } The tokens will be passed whereever user is requied to be logged in on the server to access any resources.","title":"Login Users"},{"location":"sessions/#logout-users","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:7001/beefchain/v1/auth/logout Note that the above command requires passing values for token variables.","title":"Logout Users"},{"location":"sessions/#refresh-tokens","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://localhost:7001/beefchain/v1/auth/refresh","title":"Refresh Tokens"},{"location":"sessions/#reset-password","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://localhost:7001/beefchain/v1/auth/password_reset","title":"Reset Password"},{"location":"sessions/#remove-user","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@beefsupply.com\"}' https://localhost:7001/beefchain/v1/auth/remove_user Note that removing a user requires super admin permissions and the super admin needs to be logged in to be able to pass on their access tokens. These requirements can be changed under relevant classes which are classed against each route. These classes can be found in UserHandlers.py file under /api/handlers","title":"Remove User"},{"location":"sessions/#checking-permissions","text":"To check whether a user should be allowed to access any resource on the server, curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_user_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_admin_permission curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://localhost:7001/beefchain/v1/auth/check_sadmin_permission The route will return a {\"status\": \"success\"} or {\"status\": \"failure\"} message depending upon whether the user is succesffuly logged in and using correct token. This can be used as a condition to allow access to resources.","title":"Checking Permissions"},{"location":"sessions/#testing-permissions","text":"Some test routes for debugging user permissions are provided below. curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_admin curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://localhost:7001/beefchain/v1/auth/data_super_admin","title":"Testing Permissions"},{"location":"sessions/#create-collaboration-groups","text":"Note: The curl commands from here onwards have the 'Authorization' header part removed for testing purposes. Enable Authorization for the routes by including @auth.login_required and @role_required.permission(2) for the relevant classes called by routes in the file UserHandlers.py A group for collaboration can be created by, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\",\"gluster\":\"192.168.1.5\",\"ipfs\":\"192.168.1.5\",\"overlay\":\"breeder_network\"}' https://localhost:7001/beefchain/v1/auth/insert_group_info The command in addition to adding information to a database, creates a folder with the same group name under /api/db_files. The folder further contains files for appending information from group users related to glusterFS, IPFS and Overlay network overtime. Information related to the group can be retrieved by, curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\"}' https://localhost:7001/beefchain/v1/auth/get_group_info","title":"Create Collaboration Groups"},{"location":"sessions/#upload-group-related-files","text":"Files that can be used to form collboration groups can be uploaded by, curl -k -F 'file=@/home/USER/BeefChain/sessions/beefchain.txt' https://localhost:7001/beefchain/v1/auth/upload_file To allow a file extension to be allowed or disallowed from being uploaded to the server, modify the ALLOWED_EXTENSIONS variable in the UserHandlers.py file under /api/handlers. To download a file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://localhost:7001/beefchain/v1/auth/download_file > beefchain.txt To remove a file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://localhost:7001/beefchain/v1/auth/remove_file","title":"Upload Group Related Files"},{"location":"sessions/#making-group-related-requests","text":"To submit any request related to group that can be viewed by admins or reguator, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"requestinfo\":\"create a new sub-group breeder1 for us!\"}' https://localhost:7001/beefchain/v1/auth/add_user_requests This creates a folder under /api/requests with a filename mathching the requestor name. The staus of the request is set to 'Pending' at the time of submission and changed once the request has been fulfilled relevant authority. To see the status of the request, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\"}' https://localhost:7001/beefchain/v1/auth/user_requests_status To change the status of the request as an adminstrator or regulator, curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"status\":\"Completed!\"}' https://localhost:7001/beefchain/v1/auth/user_requests_update","title":"Making Group Related Requests"},{"location":"sessions/#bring-down-containers","text":"To bring down the container manually, docker stop session.beefsupply.com && docker rm session.beefsupply.com && docker volume rm sessions_session.beefsupply.com # Remove directories created during building and running app rm -R mainserver && rm -R __pycache__ # Remove volumes (make back up before doing so) docker volume rm sessions_session.beefsupply.com && docker volume prune # remove all __pycache__ files in /sessions directory generated during the application runtime find . -type d -name \"__pycache__\" -exec rm -r {} + # To remove the database files rm api/db_files/* Note: Enable the 'restart' option in docker-compose file with appropriate options {always, on-failure, unless-stopped} for persistence and make back up of volumes before removing them","title":"Bring Down Containers"},{"location":"sessions/#multi-host-environment","text":"Containers can be accessed from a different host using the (1) hosted ip address (2) domain name or the (3) container name. The container name is set to 'session.beefsupply.com' so any other container directly connected to the network 'beef_supply' can access the cotainer directly using the link https://session.beefsupply.com:7001/ If the containers are hosted permanently with a domain name, edit the /etc/hosts file to point to the domain name (more details can be found at: https://man7.org/linux/man-pages/man5/hosts.5.html). We restrict the tutorial to a service hosted on a machine with a reachable ip address. Once the ip-address of the machine where the service is being hosted in confirmed (e.g. 203.0.113.5 ), change it for the variable 'sessions_address' in the BeefMesh/environment/service_ip_address.sh file and source it in the terminal source environment/service_ip_addresses.sh All of the commands above will be used with the new address $sessions_address instead of localhost.","title":"Multi-Host Environment"},{"location":"sessions/#register-users_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"username\":\"example_name\",\"password\":\"example_password\", \"email\":\"example@beefsupply.com\"}' https://$sessions_address:7001/beefchain/v1/auth/register","title":"Register Users"},{"location":"sessions/#login-users_1","text":"curl -k -H \"Content-Type: application/json\" --data '{\"email\":\"example@beefsupply.com\",\"password\":\"example_password\"}' https://$sessions_address:7001/beefchain/v1/auth/login","title":"Login Users"},{"location":"sessions/#logout-users_1","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$sessions_address:7001/beefchain/v1/auth/logout","title":"Logout Users"},{"location":"sessions/#refresh-tokens_1","text":"curl -k -H \"Content-Type: application/json\" -H 'Authorization: Bearer '$AccessToken --data '{\"refresh_token\":'\\\"$RefreshToken\\\"'}' https://$sessions_address:7001/beefchain/v1/auth/refresh","title":"Refresh Tokens"},{"location":"sessions/#reset-password_1","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"old_pass\":\"example_password\",\"new_pass\":\"new_password\"}' https://$sessions_address:7001/beefchain/v1/auth/password_reset","title":"Reset Password"},{"location":"sessions/#remove-user_1","text":"curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"username\":\"example_name\",\"password\":\"example_password\",\"email\":\"example@beefsupply.com\"}' https://$sessions_address:7001/beefchain/v1/auth/remove_user","title":"Remove User"},{"location":"sessions/#checking-permissions_1","text":"For generic user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_user_permission For admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_admin_permission For super admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" --data '{\"refresh_token\":'\\\"$RefreshToken\\\"',\"email\":\"email_admin@beefsupply.com\",\"password\":\"beefchain\"}' https://$sessions_address:7001/beefchain/v1/auth/check_sadmin_permission","title":"Checking Permissions"},{"location":"sessions/#testing-permissions_1","text":"For generic user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_user For admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_admin For super admin user curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer $AccessToken\" https://$sessions_address:7001/beefchain/v1/auth/data_super_admin","title":"Testing Permissions"},{"location":"sessions/#create-collaboration-groups_1","text":"To create a group curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\",\"gluster\":\"192.168.1.5\",\"ipfs\":\"192.168.1.5\",\"overlay\":\"breeder_network\"}' https://$sessions_address:7001/beefchain/v1/auth/insert_group_info To retrieve information related to a group curl -k -H \"Content-Type: application/json\" --data '{\"name\":\"breeder\",\"description\":\"BreederConsortium\"}' https://$sessions_address:7001/beefchain/v1/auth/get_group_info","title":"Create Collaboration Groups"},{"location":"sessions/#upload-group-related-files_1","text":"Top upload files related to a group curl -k -F 'file=@/home/USER/BeefChain/sessions/beefchain.txt' https://$sessions_address:7001/beefchain/v1/auth/upload_file To download a group related file curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://$sessions_address:7001/beefchain/v1/auth/download_file > beefchain.txt To remove a group related file, curl -k -H \"Content-Type: application/json\" --data '{\"filename\":\"beefchain.txt\",\"description\":\"beefchain.txt\"}' https://$sessions_address:7001/beefchain/v1/auth/remove_file","title":"Upload Group Related Files"},{"location":"sessions/#making-group-related-requests_1","text":"To submit any request related to group curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"requestinfo\":\"create a new sub-group breeder1 for us!\"}' https://$sessions_address:7001/beefchain/v1/auth/add_user_requests To see the status of the request curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\"}' https://$sessions_address:7001/beefchain/v1/auth/user_requests_status To change the status of the request as an adminstrator or regulator curl -k -H \"Content-Type: application/json\" --data '{\"requestor\":\"breeder\",\"identity\":\"SpartyBreeder\",\"status\":\"Completed!\"}' https://$sessions_address:7001/beefchain/v1/auth/user_requests_update","title":"Making Group Related Requests"},{"location":"shareddrive/","text":"Network Shared Drive Network shared drive serves as an alternative mechanism for quickly accessing shared beef chain application critical files instead of sharing them from the central beefchain application. Network shared drive applications require dedicated servers spinning up an application and a shared folder to which clients can connect and upload/download documents. We use GlusterFS (https://www.gluster.org/) in our beef chain application to spin up atleast two dedicated servers (in a collaboration group) to which client hosts can connect and start a shared folder. It is advisable to use unallocated disk partitions that are mounted on run time for shared network drives instead of a shared folder in system or root parititon to avoid data conflicts or data loss Getting Started Setting up a shared network folder requires more than one physical hosts that can be reached over internet. If you are using virtual machines for testing on the same physical host, make sure they are using 'Bridged Adapter' so that each VM gets a separate network adddress. NAT will not work for creating shared network drive/folder on VMs on the same host machine because this assigns them the same network address. Further, WSL machines on windows also get the same network addresses and currently (January 2024) don't support bridged networking. The scripts provided here have been tested with VirtualBox VMs by setting network to use 'Bridged Adapter'. Note that GlusterFs works with a minimum of two hosts nodes working as servers to which other client nodes can attach and start a shared volume. For the remainder of tutorial, We will make a distinction of whether the script function should be called on a server or client where required. Also, most of the commands below require raised priviliges so make sure to check whether its safe to run it or not before hand! To get started, grab the network addresses of each machine and ping other machines to check if they are reachable. Run from within /sharednetdrive folder. # Run from within 'overlay' folder ./shared-drive.sh ipaddress # Replace with other hosts ip address ping <other_host_ip_address> The shared-drive.sh script saves environment variables in /sharednetdrive/env folder everytime its called. Applying Net Filters Note: This is applicable on Server and Client Shared net drive works on specific ports which need to be configured to allow traffic coming and and out of the host network interface. To allow configuring the specific ports, either use ufw (Ubuntu) application or firewall-cmd (Debian, CentOS or others) application. To set network filters using ufw (note: may require raised priviliges!), run from within /sharednetdrive folder # Apply ufw filters ./shared-drive.sh applyufw # Remove ufw filters ./shared-drive.sh removeufw # list applied ufw filters ./shared-drive.sh listufw # disable ufw application ./shared-drive.sh disableufw # enable ufw application ./shared-drive.sh enableufw Note: Using both filters on a linux system will cause the same error mentioned earlier due to conflicts. For a similar error scenario, remove and disable either of the network filter application completely and apply and enable the other one! To set network filters using firewall-cmd (note: may require raised priviliges!), run from within /sharednetdrive folder # Apply firewall filters ./shared-drive.sh applyfirewall # Remove firewall filters ./shared-drive.sh removefirewall # list applied firewall filters ./shared-drive.sh listfirewall # disable firewall application ./shared-drive.sh disablefirewall # enable firewall application ./shared-drive.sh enablefirewall Removing Shared Folder Note: This is applicable on Server and Client We make a distinction between folder and volume here. A folder is a directory on host machine which when shared over network is seen as a volume with a specific name. Hence different folders on different hosts with different paths can be mapped to one volume. To remove any lingering folders and volumes from previous GlusterFS configurations and to start a new, first check for any existing volumes # If you don't know any volumes ./shared-drive.sh listvolume # If you know the volume name (e.g. beefchainvolume) and need details ./shared-drive.sh listvolume beefchainvolume Calling the 'listvolume' function stores details of volumes listed under gluster in shardnetdrive/env/volume_info.txt for future reference. Once you know the volume name (e.g. beefchainvolume), you can delete the volume (make sure to create a backup before doing so!) ./shared-drive.sh deletevolume <replace_volume_name_here> Once the volume has been deleted, you can now safely remove the folder from the host (make backup before doing so!) # To remove a shared folder from host (e.g. /tmp/shared-beefchain-folder) ./shared-drive.sh removefolder <replace_full_folder_path_here> If the volume still persists, then remove the bricks before attempting to proceed to delete volume. Bricks information can be viewed by listing volume information and brick can be removed by # Assumes volume replica count 2! Change variable 'REPLICA_COUNT' in script if its different ./shared-drive.sh removebrick <volume_name> <brick_to_remove> # Call to removebrick method with example paramaters ./shared-drive.sh removebrick beefchain 192.168.1.13:/tmp/beefchain-storage Configuring Shared Folder Note: This is applicable on Server To start configuring a shared volume, first create a folder on the host (server) machine. It is recommende that a new parition is formatted and used instead of using a folder that is created on s system (root) directory. The commands use '--force' flag to create folder or volume so make sure you are not passing a sensitive folder path. First create a shared folder that will be used for gluster volume # Create a folder to use, e.g. /tmp/beefchain-shared-folder ./shared-drive.sh makefolder <replace_full_folder_path_here> Next, connect to a second server node (note that GlusterFS require minimum two server nodes) # Make sure the other node (e.g. 192.168.1.25) can be pinged first ./shared-drive.sh connectnode <replace_node_ipaddress_here> If there are issues connecting between server node, modify iptables to allow traffic as described in section ## Modify IPtables Once the nodes are successfully connected, you can check node status ./shared-drive.sh nodestatus The above command should return a set of connected nodes, e.g. UUID Hostname State 292d2c93-e7f3-44a4-b7f2-8f1da5759a20 192.168.1.13 Connected e378b375-8106-4f42-bf89-b28db00cf186 localhost Connected Once the host server shows up as connected, configure a shared volume # Generic command (replace with exact parameters) ./shared-drive.sh configurevolume <volume_name_here> <host_address_here> <folder_path_here> # Above command with specific example parameters ./shared-drive.sh configurevolume beefchain 192.168.1.21 /tmp/beefchain-shared-folder Once the shared volume has been successfully configured, start the volume for other clients to access ./shared-drive.sh startvolume <replace_volume_name_here> Check status of volume # If you don't know any volumes ./shared-drive.sh listvolume # If you know the volume name (e.g. beefchain) and need details ./shared-drive.sh listvolume beefchain To remove a specific server node (e.g. 192.168.1.25) from the gluster server setup, first make backup of volumes shared with the server, stop the volume, delete it and then proceed to removing the node ./shared-drive.sh removenode <replace_node_ipaddress_here> Modify IPTables If there are issues connecting between servers despite being able to ping them, # To allow incoming traffic from a host server (e.g. 192.168.2.31) # Run on all hosts (client and and server) ./shared-drive.sh allowtraffic <replace_node_ipaddress_here> # To disallow traffic from a specific host for safety purposes ./shared-drive.sh disallowtraffic <replace_node_ipaddress_here> Join as Client Once a minimum of two server nodes have been configured properly, a gluster client can connect to the servers and with a local folder where all the shared content over gluster volume is synched! Before doing so make sure that the client can ping any of the server nodes, has a local folder setup with proper permissions and knows the volume name configured over the gluster servers. # Generic command (replace with exact parameters) ./shared-drive.sh joinasclient <volume_name_here> <server_address_here> <local_folder_path_here> # Above command with specific example parameters ./shared-drive.sh joinasclient beefchain 192.168.1.35 /tmp/beefchain-local-folder Testing Volume To test the configured shared network drive, simply move a file to the shared folder in any host and it should appear shortly in the shared folder on other hosts cp BeefMesh/sharednetdrive/test-file.txt /tmp/beefchain-shared-folder/test-file.txt Debugging Issues Note: This is applicable on Server and Client If gluster daemon is not running, restart it ./shared-drive.sh restartgluster If previously created shared volumes are causing issues # Remove caches and earlier volume related data (make backup before doing so!) ./shared-drive.sh fixfolderconflict # Fix permissions for gluster application ./shared-drive.sh fixpermissions Removing cache and other related files from gluster library for previously configured volumes may generate extra messages (particularly related to geographic data) such as below but does not interfere with normal operation. This was tested for glusterfs 10.1. Traceback (most recent call last): File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 325, in <module> main() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 41, in main argsupgrade.upgrade() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 85, in upgrade init_gsyncd_template_conf() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 50, in init_gsyncd_template_conf fd = os.open(path, os.O_CREAT | os.O_RDWR) FileNotFoundError: [Errno 2] No such file or directory: '/var/lib/glusterd/geo-replication/gsyncd_template.conf'","title":"Network Shared Drive"},{"location":"shareddrive/#network-shared-drive","text":"Network shared drive serves as an alternative mechanism for quickly accessing shared beef chain application critical files instead of sharing them from the central beefchain application. Network shared drive applications require dedicated servers spinning up an application and a shared folder to which clients can connect and upload/download documents. We use GlusterFS (https://www.gluster.org/) in our beef chain application to spin up atleast two dedicated servers (in a collaboration group) to which client hosts can connect and start a shared folder. It is advisable to use unallocated disk partitions that are mounted on run time for shared network drives instead of a shared folder in system or root parititon to avoid data conflicts or data loss","title":"Network Shared Drive"},{"location":"shareddrive/#getting-started","text":"Setting up a shared network folder requires more than one physical hosts that can be reached over internet. If you are using virtual machines for testing on the same physical host, make sure they are using 'Bridged Adapter' so that each VM gets a separate network adddress. NAT will not work for creating shared network drive/folder on VMs on the same host machine because this assigns them the same network address. Further, WSL machines on windows also get the same network addresses and currently (January 2024) don't support bridged networking. The scripts provided here have been tested with VirtualBox VMs by setting network to use 'Bridged Adapter'. Note that GlusterFs works with a minimum of two hosts nodes working as servers to which other client nodes can attach and start a shared volume. For the remainder of tutorial, We will make a distinction of whether the script function should be called on a server or client where required. Also, most of the commands below require raised priviliges so make sure to check whether its safe to run it or not before hand! To get started, grab the network addresses of each machine and ping other machines to check if they are reachable. Run from within /sharednetdrive folder. # Run from within 'overlay' folder ./shared-drive.sh ipaddress # Replace with other hosts ip address ping <other_host_ip_address> The shared-drive.sh script saves environment variables in /sharednetdrive/env folder everytime its called.","title":"Getting Started"},{"location":"shareddrive/#applying-net-filters","text":"Note: This is applicable on Server and Client Shared net drive works on specific ports which need to be configured to allow traffic coming and and out of the host network interface. To allow configuring the specific ports, either use ufw (Ubuntu) application or firewall-cmd (Debian, CentOS or others) application. To set network filters using ufw (note: may require raised priviliges!), run from within /sharednetdrive folder # Apply ufw filters ./shared-drive.sh applyufw # Remove ufw filters ./shared-drive.sh removeufw # list applied ufw filters ./shared-drive.sh listufw # disable ufw application ./shared-drive.sh disableufw # enable ufw application ./shared-drive.sh enableufw Note: Using both filters on a linux system will cause the same error mentioned earlier due to conflicts. For a similar error scenario, remove and disable either of the network filter application completely and apply and enable the other one! To set network filters using firewall-cmd (note: may require raised priviliges!), run from within /sharednetdrive folder # Apply firewall filters ./shared-drive.sh applyfirewall # Remove firewall filters ./shared-drive.sh removefirewall # list applied firewall filters ./shared-drive.sh listfirewall # disable firewall application ./shared-drive.sh disablefirewall # enable firewall application ./shared-drive.sh enablefirewall","title":"Applying Net Filters"},{"location":"shareddrive/#removing-shared-folder","text":"Note: This is applicable on Server and Client We make a distinction between folder and volume here. A folder is a directory on host machine which when shared over network is seen as a volume with a specific name. Hence different folders on different hosts with different paths can be mapped to one volume. To remove any lingering folders and volumes from previous GlusterFS configurations and to start a new, first check for any existing volumes # If you don't know any volumes ./shared-drive.sh listvolume # If you know the volume name (e.g. beefchainvolume) and need details ./shared-drive.sh listvolume beefchainvolume Calling the 'listvolume' function stores details of volumes listed under gluster in shardnetdrive/env/volume_info.txt for future reference. Once you know the volume name (e.g. beefchainvolume), you can delete the volume (make sure to create a backup before doing so!) ./shared-drive.sh deletevolume <replace_volume_name_here> Once the volume has been deleted, you can now safely remove the folder from the host (make backup before doing so!) # To remove a shared folder from host (e.g. /tmp/shared-beefchain-folder) ./shared-drive.sh removefolder <replace_full_folder_path_here> If the volume still persists, then remove the bricks before attempting to proceed to delete volume. Bricks information can be viewed by listing volume information and brick can be removed by # Assumes volume replica count 2! Change variable 'REPLICA_COUNT' in script if its different ./shared-drive.sh removebrick <volume_name> <brick_to_remove> # Call to removebrick method with example paramaters ./shared-drive.sh removebrick beefchain 192.168.1.13:/tmp/beefchain-storage","title":"Removing Shared Folder"},{"location":"shareddrive/#configuring-shared-folder","text":"Note: This is applicable on Server To start configuring a shared volume, first create a folder on the host (server) machine. It is recommende that a new parition is formatted and used instead of using a folder that is created on s system (root) directory. The commands use '--force' flag to create folder or volume so make sure you are not passing a sensitive folder path. First create a shared folder that will be used for gluster volume # Create a folder to use, e.g. /tmp/beefchain-shared-folder ./shared-drive.sh makefolder <replace_full_folder_path_here> Next, connect to a second server node (note that GlusterFS require minimum two server nodes) # Make sure the other node (e.g. 192.168.1.25) can be pinged first ./shared-drive.sh connectnode <replace_node_ipaddress_here> If there are issues connecting between server node, modify iptables to allow traffic as described in section ## Modify IPtables Once the nodes are successfully connected, you can check node status ./shared-drive.sh nodestatus The above command should return a set of connected nodes, e.g. UUID Hostname State 292d2c93-e7f3-44a4-b7f2-8f1da5759a20 192.168.1.13 Connected e378b375-8106-4f42-bf89-b28db00cf186 localhost Connected Once the host server shows up as connected, configure a shared volume # Generic command (replace with exact parameters) ./shared-drive.sh configurevolume <volume_name_here> <host_address_here> <folder_path_here> # Above command with specific example parameters ./shared-drive.sh configurevolume beefchain 192.168.1.21 /tmp/beefchain-shared-folder Once the shared volume has been successfully configured, start the volume for other clients to access ./shared-drive.sh startvolume <replace_volume_name_here> Check status of volume # If you don't know any volumes ./shared-drive.sh listvolume # If you know the volume name (e.g. beefchain) and need details ./shared-drive.sh listvolume beefchain To remove a specific server node (e.g. 192.168.1.25) from the gluster server setup, first make backup of volumes shared with the server, stop the volume, delete it and then proceed to removing the node ./shared-drive.sh removenode <replace_node_ipaddress_here>","title":"Configuring Shared Folder"},{"location":"shareddrive/#modify-iptables","text":"If there are issues connecting between servers despite being able to ping them, # To allow incoming traffic from a host server (e.g. 192.168.2.31) # Run on all hosts (client and and server) ./shared-drive.sh allowtraffic <replace_node_ipaddress_here> # To disallow traffic from a specific host for safety purposes ./shared-drive.sh disallowtraffic <replace_node_ipaddress_here>","title":"Modify IPTables"},{"location":"shareddrive/#join-as-client","text":"Once a minimum of two server nodes have been configured properly, a gluster client can connect to the servers and with a local folder where all the shared content over gluster volume is synched! Before doing so make sure that the client can ping any of the server nodes, has a local folder setup with proper permissions and knows the volume name configured over the gluster servers. # Generic command (replace with exact parameters) ./shared-drive.sh joinasclient <volume_name_here> <server_address_here> <local_folder_path_here> # Above command with specific example parameters ./shared-drive.sh joinasclient beefchain 192.168.1.35 /tmp/beefchain-local-folder","title":"Join as Client"},{"location":"shareddrive/#testing-volume","text":"To test the configured shared network drive, simply move a file to the shared folder in any host and it should appear shortly in the shared folder on other hosts cp BeefMesh/sharednetdrive/test-file.txt /tmp/beefchain-shared-folder/test-file.txt","title":"Testing Volume"},{"location":"shareddrive/#debugging-issues","text":"Note: This is applicable on Server and Client If gluster daemon is not running, restart it ./shared-drive.sh restartgluster If previously created shared volumes are causing issues # Remove caches and earlier volume related data (make backup before doing so!) ./shared-drive.sh fixfolderconflict # Fix permissions for gluster application ./shared-drive.sh fixpermissions Removing cache and other related files from gluster library for previously configured volumes may generate extra messages (particularly related to geographic data) such as below but does not interfere with normal operation. This was tested for glusterfs 10.1. Traceback (most recent call last): File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 325, in <module> main() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/gsyncd.py\", line 41, in main argsupgrade.upgrade() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 85, in upgrade init_gsyncd_template_conf() File \"/usr/lib/x86_64-linux-gnu/glusterfs/python/syncdaemon/argsupgrade.py\", line 50, in init_gsyncd_template_conf fd = os.open(path, os.O_CREAT | os.O_RDWR) FileNotFoundError: [Errno 2] No such file or directory: '/var/lib/glusterd/geo-replication/gsyncd_template.conf'","title":"Debugging Issues"},{"location":"utilities/","text":"Utilities and Monitoring To monitor the containerized network, use the grafana and prometheus setup provided in utilities/prometheus folder. Navigate to utilities/prometheus folder and spin the containers, docker-compose up -d This will start an application setup comprising of cadvisor, grafana and prometheus that can be used to monitor the workload of containers and the network throughout. Navigate to 127.0.0.1:9100 address to view grafana interface. Select prometheus as the data source and use any dashboard of choice from the grafana website (https://grafana.com/grafana/dashboards/) that supports cadvisor and prometheus. To bring down the containers manually, docker stop prometheus advisor grafana docker rm prometheus advisor grafana docker volume prune A script from hyperledger fabric is also reproduced here that is used when downloading fabric resources locally using custom scripts provided under blockchain/ folder. Docker container scripts under folder utilities/docker can be used as a guide to setup large blockchain consortium groups locally for testing.","title":"Utilities and Monitoring"},{"location":"utilities/#utilities-and-monitoring","text":"To monitor the containerized network, use the grafana and prometheus setup provided in utilities/prometheus folder. Navigate to utilities/prometheus folder and spin the containers, docker-compose up -d This will start an application setup comprising of cadvisor, grafana and prometheus that can be used to monitor the workload of containers and the network throughout. Navigate to 127.0.0.1:9100 address to view grafana interface. Select prometheus as the data source and use any dashboard of choice from the grafana website (https://grafana.com/grafana/dashboards/) that supports cadvisor and prometheus. To bring down the containers manually, docker stop prometheus advisor grafana docker rm prometheus advisor grafana docker volume prune A script from hyperledger fabric is also reproduced here that is used when downloading fabric resources locally using custom scripts provided under blockchain/ folder. Docker container scripts under folder utilities/docker can be used as a guide to setup large blockchain consortium groups locally for testing.","title":"Utilities and Monitoring"},{"location":"utsa/","text":"UTSAServer This submodule is located under /utsaserver directory and is meant to time stamp files using RFC 3161. The time stamp is basically a cryptographic signature with a date attached to it. Basics The bacis working of time stamp authority is as follows. Hash of the data that the client wants to time stamp, is sent to the time stamp authority. The time stamp authority then concatenates the exact time with the hash of the data. It then creates the time stamp to deliver by using a private key. The time stamped file is then returned to the requesting client application. The returned time stamped file can be used to verify the originality of the file and its creation. To verify the time stamped file, a time stamp key pair (X509 certificate) is required at the time of verification request that is issued by a certificate authority. The main use of time stamping could be to verify logs from rotation time, or to prove files upload in due time along with cross checking data context. Creating Certificates By default the application runs a 'uts-server' USER which can be changed from docker compose file or using any of the sample configuration files given in /utsaserver/cfg/ folder. Before the applciation can be started it is necessary to create certificate files that are then imported into the application when it is run. To create certificate files, first modify the configuration file CAtsa.cnf in /utsaserver/pki/ folder accordingly or use the default settings. Then call the createtsa method from BeefMesh directory to generate required files ./utsaserver/utsa-network.sh createtsa A number of certificate are created for ssl communication between client and server, and to verify files using certificate authority. The certicficates contained in the folder /utsaserver/pki are imported into the container under /opt/uts-server/tests/cfg/pki/ for use. This can be modified accordingly including the ports (2020) and addresses. Note: To modify the ports on which the server runs, also change it in the configuration file in addition to the docker compose file. Further, force the uts-server to pick the configuration file by specifying it in the docker entrypoint 'uts-server -c /etc/uts-server/uts-server.cnf -D' . Running Application For testing with docker as a standalone application, spin up the container by navigating into the /utsaserver directory. docker-compose up -d Once up and running, a file can be time stamped using the server url 'http://localhost:2020' (assuming testing locally). To timestamp a file (e.g. sample file ./utsaserver/files/test-file.txt), use the utility function provided with the application that creates a formatted curl command to send to the time stamping authority using the hash of the provided data. Required certitificates for clients should be shared and be available in the /utsaserver/pki folder for client to call the server. From the main BeefMesh directory, run ./utsaserver/utsa-network.sh timestampfile localhost test-file.txt # generic call assuming files are located in utsaserver/files/ ./utsaserver/utsa-network.sh timestampfile <server_address> <file_name_with_extension> Once the command is run successfully, an example result could look like this [INFO] Generating timestamp on file '/home/USER/BeefMesh/utsaserver/test-file.txt', to '/home/USER/BeefMesh/utsaserver/test-file.txt.tsr', using server 'http://localhost:2020' Using configuration from /usr/lib/ssl/openssl.cnf [SUCCESS] Timestamp of file '/home/USER/BeefMesh/utsaserver/test-file.txt' using server 'http://localhost:2020' succeed, ts written to '/home/USER/BeefMesh/utsaserver/test-file.txt.tsr' The timestamped file is returned and stored as a .tsr format file. The timestamped file can then be verified any time using the earlier created certificates containing public key. Verifying Files To verify the time stamp file, run ./utsaserver/utsa-network.sh verifytimestamp localhost test-file.txt.tsr test-file.txt tsaca.pem # generic call assuming files are located in utsaserver/files/ and certificate is in utsaserver/pki/ ./utsaserver/utsa-network.sh verifytimestamp <server_address> <timestamped_file> <original_file> <certificate_name> Note that the timestamped file (./utsaserver/files/test-file.txt.tsr) and the orignal file (./utsaserver/files/test-file.txt) needs to be provided along with the certificate (./utsaserver/pki/tsaca.pem). A sample response could look like this Using configuration from /usr/lib/ssl/openssl.cnf Verification: OK The details of the time stamp file can also be retreived for examination by calling timestampdetails method ./utsaserver/utsa-network.sh timestampdetails test-file.txt.tsr # generic call assuming files are located in utsaserver/files/ and certificate is in utsaserver/pki/ ./utsaserver/utsa-network.sh timestampdetails <timestamped_file> Here, we have given the .tsr file as an input. A typical response could look like this TST info: Version: 1 Policy OID: tsa_policy1 Hash Algorithm: sha256 Message data: 0000 - 1e b4 c9 63 84 6c db 8f-88 9c 63 49 41 07 a1 51 ...c.l....cIA..Q 0010 - 06 b5 9f d0 fd 49 5f 03-03 fd 20 ba 08 17 79 bd .....I_... ...y. Serial number: 0x8FD375B3007C73207DEC3B03D1164F8C8B247DCC Time stamp: Mar 26 17:21:14 2024 GMT Accuracy: 0x01 seconds, 0x01F4 millis, 0x64 micros Ordering: yes Nonce: 0x96522CA3458B1660 TSA: DirName:/C=US/ST=Michigan/L=USA/O=UTS-SERVER test/CN=TSA CERT 1 Bringdown Containers To bring down and clear containers docker stop beef.utsa.com && docker rm beef.utsa.com Multihost Setup When using from a different host, update the server address variable in the environment/service_ip_addresses.sh file and source it before calling the server. # from BeefMesh main directory source environment/service_ip_addresses.sh # call to server with the url for deployed server to timestamp file ./utsaserver/utsa-network.sh timestampfile $utsa_address test-file.txt # call to server with the url for deployed server to verify timestamped file ./utsaserver/utsa-network.sh verifytimestamp $utsa_address test-file.txt.tsr test-file.txt tsaca.pem","title":"UTSA Server"},{"location":"utsa/#utsaserver","text":"This submodule is located under /utsaserver directory and is meant to time stamp files using RFC 3161. The time stamp is basically a cryptographic signature with a date attached to it.","title":"UTSAServer"},{"location":"utsa/#basics","text":"The bacis working of time stamp authority is as follows. Hash of the data that the client wants to time stamp, is sent to the time stamp authority. The time stamp authority then concatenates the exact time with the hash of the data. It then creates the time stamp to deliver by using a private key. The time stamped file is then returned to the requesting client application. The returned time stamped file can be used to verify the originality of the file and its creation. To verify the time stamped file, a time stamp key pair (X509 certificate) is required at the time of verification request that is issued by a certificate authority. The main use of time stamping could be to verify logs from rotation time, or to prove files upload in due time along with cross checking data context.","title":"Basics"},{"location":"utsa/#creating-certificates","text":"By default the application runs a 'uts-server' USER which can be changed from docker compose file or using any of the sample configuration files given in /utsaserver/cfg/ folder. Before the applciation can be started it is necessary to create certificate files that are then imported into the application when it is run. To create certificate files, first modify the configuration file CAtsa.cnf in /utsaserver/pki/ folder accordingly or use the default settings. Then call the createtsa method from BeefMesh directory to generate required files ./utsaserver/utsa-network.sh createtsa A number of certificate are created for ssl communication between client and server, and to verify files using certificate authority. The certicficates contained in the folder /utsaserver/pki are imported into the container under /opt/uts-server/tests/cfg/pki/ for use. This can be modified accordingly including the ports (2020) and addresses. Note: To modify the ports on which the server runs, also change it in the configuration file in addition to the docker compose file. Further, force the uts-server to pick the configuration file by specifying it in the docker entrypoint 'uts-server -c /etc/uts-server/uts-server.cnf -D' .","title":"Creating Certificates"},{"location":"utsa/#running-application","text":"For testing with docker as a standalone application, spin up the container by navigating into the /utsaserver directory. docker-compose up -d Once up and running, a file can be time stamped using the server url 'http://localhost:2020' (assuming testing locally). To timestamp a file (e.g. sample file ./utsaserver/files/test-file.txt), use the utility function provided with the application that creates a formatted curl command to send to the time stamping authority using the hash of the provided data. Required certitificates for clients should be shared and be available in the /utsaserver/pki folder for client to call the server. From the main BeefMesh directory, run ./utsaserver/utsa-network.sh timestampfile localhost test-file.txt # generic call assuming files are located in utsaserver/files/ ./utsaserver/utsa-network.sh timestampfile <server_address> <file_name_with_extension> Once the command is run successfully, an example result could look like this [INFO] Generating timestamp on file '/home/USER/BeefMesh/utsaserver/test-file.txt', to '/home/USER/BeefMesh/utsaserver/test-file.txt.tsr', using server 'http://localhost:2020' Using configuration from /usr/lib/ssl/openssl.cnf [SUCCESS] Timestamp of file '/home/USER/BeefMesh/utsaserver/test-file.txt' using server 'http://localhost:2020' succeed, ts written to '/home/USER/BeefMesh/utsaserver/test-file.txt.tsr' The timestamped file is returned and stored as a .tsr format file. The timestamped file can then be verified any time using the earlier created certificates containing public key.","title":"Running Application"},{"location":"utsa/#verifying-files","text":"To verify the time stamp file, run ./utsaserver/utsa-network.sh verifytimestamp localhost test-file.txt.tsr test-file.txt tsaca.pem # generic call assuming files are located in utsaserver/files/ and certificate is in utsaserver/pki/ ./utsaserver/utsa-network.sh verifytimestamp <server_address> <timestamped_file> <original_file> <certificate_name> Note that the timestamped file (./utsaserver/files/test-file.txt.tsr) and the orignal file (./utsaserver/files/test-file.txt) needs to be provided along with the certificate (./utsaserver/pki/tsaca.pem). A sample response could look like this Using configuration from /usr/lib/ssl/openssl.cnf Verification: OK The details of the time stamp file can also be retreived for examination by calling timestampdetails method ./utsaserver/utsa-network.sh timestampdetails test-file.txt.tsr # generic call assuming files are located in utsaserver/files/ and certificate is in utsaserver/pki/ ./utsaserver/utsa-network.sh timestampdetails <timestamped_file> Here, we have given the .tsr file as an input. A typical response could look like this TST info: Version: 1 Policy OID: tsa_policy1 Hash Algorithm: sha256 Message data: 0000 - 1e b4 c9 63 84 6c db 8f-88 9c 63 49 41 07 a1 51 ...c.l....cIA..Q 0010 - 06 b5 9f d0 fd 49 5f 03-03 fd 20 ba 08 17 79 bd .....I_... ...y. Serial number: 0x8FD375B3007C73207DEC3B03D1164F8C8B247DCC Time stamp: Mar 26 17:21:14 2024 GMT Accuracy: 0x01 seconds, 0x01F4 millis, 0x64 micros Ordering: yes Nonce: 0x96522CA3458B1660 TSA: DirName:/C=US/ST=Michigan/L=USA/O=UTS-SERVER test/CN=TSA CERT 1","title":"Verifying Files"},{"location":"utsa/#bringdown-containers","text":"To bring down and clear containers docker stop beef.utsa.com && docker rm beef.utsa.com","title":"Bringdown Containers"},{"location":"utsa/#multihost-setup","text":"When using from a different host, update the server address variable in the environment/service_ip_addresses.sh file and source it before calling the server. # from BeefMesh main directory source environment/service_ip_addresses.sh # call to server with the url for deployed server to timestamp file ./utsaserver/utsa-network.sh timestampfile $utsa_address test-file.txt # call to server with the url for deployed server to verify timestamped file ./utsaserver/utsa-network.sh verifytimestamp $utsa_address test-file.txt.tsr test-file.txt tsaca.pem","title":"Multihost Setup"}]}